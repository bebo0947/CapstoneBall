{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abdelrahman Alkhawas Sprint 3\n",
    "This is my sprint two notebook where I will be Reconfiguring my data for some initial NN experimentation. This notebook assumes that the user has access to `eeaao.csv` or has run the latest iteration of `webScraping1.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu==2.10 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (24.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu==2.10) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%pip install tensorflow-gpu==2.10\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 200\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_id</th>\n",
       "      <th>Season</th>\n",
       "      <th>Player</th>\n",
       "      <th>Nation</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Squad</th>\n",
       "      <th>Comp</th>\n",
       "      <th>Age</th>\n",
       "      <th>Born</th>\n",
       "      <th>Playing Time, MP</th>\n",
       "      <th>Playing Time, Starts</th>\n",
       "      <th>Playing Time, Min</th>\n",
       "      <th>Playing Time, 90s</th>\n",
       "      <th>Performance, Gls</th>\n",
       "      <th>Performance, Ast</th>\n",
       "      <th>Performance, G+A</th>\n",
       "      <th>Performance, G-PK</th>\n",
       "      <th>Performance, PK</th>\n",
       "      <th>Performance, PKatt</th>\n",
       "      <th>Performance, CrdY</th>\n",
       "      <th>Performance, CrdR</th>\n",
       "      <th>Expected, xG</th>\n",
       "      <th>Expected, npxG</th>\n",
       "      <th>Expected, xAG</th>\n",
       "      <th>Expected, npxG+xAG</th>\n",
       "      <th>Progression, PrgC</th>\n",
       "      <th>Progression, PrgP</th>\n",
       "      <th>Progression, PrgR</th>\n",
       "      <th>Per 90 Minutes, Gls</th>\n",
       "      <th>Per 90 Minutes, Ast</th>\n",
       "      <th>Per 90 Minutes, G+A</th>\n",
       "      <th>Per 90 Minutes, G-PK</th>\n",
       "      <th>Per 90 Minutes, G+A-PK</th>\n",
       "      <th>Per 90 Minutes, xG</th>\n",
       "      <th>Per 90 Minutes, xAG</th>\n",
       "      <th>Per 90 Minutes, xG+xAG</th>\n",
       "      <th>Per 90 Minutes, npxG</th>\n",
       "      <th>Per 90 Minutes, npxG+xAG</th>\n",
       "      <th>Playing Time, MP.1</th>\n",
       "      <th>Playing Time, Min.1</th>\n",
       "      <th>Playing Time, Mn/MP</th>\n",
       "      <th>Playing Time, Min%</th>\n",
       "      <th>Playing Time, 90s.1</th>\n",
       "      <th>Starts, Starts</th>\n",
       "      <th>Starts, Mn/Start</th>\n",
       "      <th>Starts, Compl</th>\n",
       "      <th>Subs, Subs</th>\n",
       "      <th>Subs, Mn/Sub</th>\n",
       "      <th>Subs, unSub</th>\n",
       "      <th>Team Success, PPM</th>\n",
       "      <th>Team Success, onG</th>\n",
       "      <th>Team Success, onGA</th>\n",
       "      <th>Team Success, +/-</th>\n",
       "      <th>Team Success, +/-90</th>\n",
       "      <th>Team Success, On-Off</th>\n",
       "      <th>Team Success (xG), onxG</th>\n",
       "      <th>Team Success (xG), onxGA</th>\n",
       "      <th>Team Success (xG), xG+/-</th>\n",
       "      <th>Team Success (xG), xG+/-90</th>\n",
       "      <th>Team Success (xG), On-Off</th>\n",
       "      <th>Standard, Gls</th>\n",
       "      <th>Standard, Sh</th>\n",
       "      <th>Standard, SoT</th>\n",
       "      <th>Standard, SoT%</th>\n",
       "      <th>Standard, Sh/90</th>\n",
       "      <th>Standard, SoT/90</th>\n",
       "      <th>Standard, G/Sh</th>\n",
       "      <th>Standard, G/SoT</th>\n",
       "      <th>Standard, Dist</th>\n",
       "      <th>Standard, FK</th>\n",
       "      <th>Standard, PK</th>\n",
       "      <th>Standard, PKatt</th>\n",
       "      <th>Expected, xG.1</th>\n",
       "      <th>Expected, npxG.1</th>\n",
       "      <th>Expected, npxG/Sh</th>\n",
       "      <th>Expected, G-xG</th>\n",
       "      <th>Expected, np:G-xG</th>\n",
       "      <th>Total, Cmp</th>\n",
       "      <th>Total, Att</th>\n",
       "      <th>Total, Cmp%</th>\n",
       "      <th>Total, TotDist</th>\n",
       "      <th>Total, PrgDist</th>\n",
       "      <th>Short, Cmp</th>\n",
       "      <th>Short, Att</th>\n",
       "      <th>Short, Cmp%</th>\n",
       "      <th>Medium, Cmp</th>\n",
       "      <th>Medium, Att</th>\n",
       "      <th>Medium, Cmp%</th>\n",
       "      <th>Long, Cmp</th>\n",
       "      <th>Long, Att</th>\n",
       "      <th>Long, Cmp%</th>\n",
       "      <th>Ast</th>\n",
       "      <th>xAG</th>\n",
       "      <th>Expected, xA</th>\n",
       "      <th>Expected, A-xAG</th>\n",
       "      <th>KP</th>\n",
       "      <th>1/3</th>\n",
       "      <th>PPA</th>\n",
       "      <th>CrsPA</th>\n",
       "      <th>PrgP</th>\n",
       "      <th>Att</th>\n",
       "      <th>Pass Types, Live</th>\n",
       "      <th>Pass Types, Dead</th>\n",
       "      <th>Pass Types, FK</th>\n",
       "      <th>Pass Types, TB</th>\n",
       "      <th>Pass Types, Sw</th>\n",
       "      <th>Pass Types, Crs</th>\n",
       "      <th>Pass Types, TI</th>\n",
       "      <th>Pass Types, CK</th>\n",
       "      <th>Corner Kicks, In</th>\n",
       "      <th>Corner Kicks, Out</th>\n",
       "      <th>Corner Kicks, Str</th>\n",
       "      <th>Outcomes, Cmp</th>\n",
       "      <th>Outcomes, Off</th>\n",
       "      <th>Outcomes, Blocks</th>\n",
       "      <th>SCA, SCA</th>\n",
       "      <th>SCA, SCA90</th>\n",
       "      <th>SCA Types, PassLive</th>\n",
       "      <th>SCA Types, PassDead</th>\n",
       "      <th>SCA Types, TO</th>\n",
       "      <th>SCA Types, Sh</th>\n",
       "      <th>SCA Types, Fld</th>\n",
       "      <th>SCA Types, Def</th>\n",
       "      <th>GCA, GCA</th>\n",
       "      <th>GCA, GCA90</th>\n",
       "      <th>GCA Types, PassLive</th>\n",
       "      <th>GCA Types, PassDead</th>\n",
       "      <th>GCA Types, TO</th>\n",
       "      <th>GCA Types, Sh</th>\n",
       "      <th>GCA Types, Fld</th>\n",
       "      <th>GCA Types, Def</th>\n",
       "      <th>Tackles, Tkl</th>\n",
       "      <th>Tackles, TklW</th>\n",
       "      <th>Tackles, Def 3rd</th>\n",
       "      <th>Tackles, Mid 3rd</th>\n",
       "      <th>Tackles, Att 3rd</th>\n",
       "      <th>Challenges, Tkl</th>\n",
       "      <th>Challenges, Att</th>\n",
       "      <th>Challenges, Tkl%</th>\n",
       "      <th>Challenges, Lost</th>\n",
       "      <th>Blocks, Blocks</th>\n",
       "      <th>Blocks, Sh</th>\n",
       "      <th>Blocks, Pass</th>\n",
       "      <th>Int</th>\n",
       "      <th>Tkl+Int</th>\n",
       "      <th>Clr</th>\n",
       "      <th>Err</th>\n",
       "      <th>Touches, Touches</th>\n",
       "      <th>Touches, Def Pen</th>\n",
       "      <th>Touches, Def 3rd</th>\n",
       "      <th>Touches, Mid 3rd</th>\n",
       "      <th>Touches, Att 3rd</th>\n",
       "      <th>Touches, Att Pen</th>\n",
       "      <th>Touches, Live</th>\n",
       "      <th>Take-Ons, Att</th>\n",
       "      <th>Take-Ons, Succ</th>\n",
       "      <th>Take-Ons, Succ%</th>\n",
       "      <th>Take-Ons, Tkld</th>\n",
       "      <th>Take-Ons, Tkld%</th>\n",
       "      <th>Carries, Carries</th>\n",
       "      <th>Carries, TotDist</th>\n",
       "      <th>Carries, PrgDist</th>\n",
       "      <th>Carries, PrgC</th>\n",
       "      <th>Carries, 1/3</th>\n",
       "      <th>Carries, CPA</th>\n",
       "      <th>Carries, Mis</th>\n",
       "      <th>Carries, Dis</th>\n",
       "      <th>Receiving, Rec</th>\n",
       "      <th>Receiving, PrgR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patrick van Aanholt-NED-1990</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Patrick van Aanholt</td>\n",
       "      <td>nl NED</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>eng Premier League</td>\n",
       "      <td>26</td>\n",
       "      <td>1990</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>2184</td>\n",
       "      <td>24.3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>46.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>28</td>\n",
       "      <td>2184</td>\n",
       "      <td>78</td>\n",
       "      <td>63.9</td>\n",
       "      <td>24.3</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.21</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>32.2</td>\n",
       "      <td>34.7</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>5</td>\n",
       "      <td>33.0</td>\n",
       "      <td>11</td>\n",
       "      <td>33.3</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>23.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>884.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>75.2</td>\n",
       "      <td>14197.0</td>\n",
       "      <td>6422.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>74.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>48.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>897.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>47.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>83.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>717.0</td>\n",
       "      <td>3714.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rolando Aarons-ENG-1995</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Rolando Aarons</td>\n",
       "      <td>eng ENG</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Newcastle Utd</td>\n",
       "      <td>eng Premier League</td>\n",
       "      <td>21</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>35</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>65.9</td>\n",
       "      <td>376.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rolando Aarons-ENG-1995</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Rolando Aarons</td>\n",
       "      <td>eng ENG</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hellas Verona</td>\n",
       "      <td>it Serie A</td>\n",
       "      <td>21</td>\n",
       "      <td>1995</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>517</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>11</td>\n",
       "      <td>517</td>\n",
       "      <td>47</td>\n",
       "      <td>15.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-1.74</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>2.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>87.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>83.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>71.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>53.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46.7</td>\n",
       "      <td>105.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ignazio Abate-ITA-1986</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Ignazio Abate</td>\n",
       "      <td>it ITA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Milan</td>\n",
       "      <td>it Serie A</td>\n",
       "      <td>30</td>\n",
       "      <td>1986</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>1057</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>17</td>\n",
       "      <td>1057</td>\n",
       "      <td>62</td>\n",
       "      <td>30.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>1.71</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.33</td>\n",
       "      <td>13.8</td>\n",
       "      <td>11.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>17.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>625.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>10991.0</td>\n",
       "      <td>4535.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>92.9</td>\n",
       "      <td>287.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>86.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>48.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aymen Abdennour-TUN-1989</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Aymen Abdennour</td>\n",
       "      <td>tn TUN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>fr Ligue 1</td>\n",
       "      <td>27</td>\n",
       "      <td>1989</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>499</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>8</td>\n",
       "      <td>499</td>\n",
       "      <td>62</td>\n",
       "      <td>14.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18</td>\n",
       "      <td>2.38</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>310.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>93.1</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>1557.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>98.4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>95.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>70.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>1522.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           p_id     Season               Player   Nation  Pos           Squad                Comp  Age  Born  Playing Time, MP  Playing Time, Starts  Playing Time, Min  Playing Time, 90s  Performance, Gls  Performance, Ast  Performance, G+A  Performance, G-PK  Performance, PK  Performance, PKatt  Performance, CrdY  Performance, CrdR  Expected, xG  Expected, npxG  Expected, xAG  Expected, npxG+xAG  Progression, PrgC  Progression, PrgP  Progression, PrgR  Per 90 Minutes, Gls  Per 90 Minutes, Ast  Per 90 Minutes, G+A  Per 90 Minutes, G-PK  Per 90 Minutes, G+A-PK  Per 90 Minutes, xG  Per 90 Minutes, xAG  Per 90 Minutes, xG+xAG  Per 90 Minutes, npxG  Per 90 Minutes, npxG+xAG  Playing Time, MP.1  Playing Time, Min.1  Playing Time, Mn/MP  Playing Time, Min%  Playing Time, 90s.1  Starts, Starts  Starts, Mn/Start  Starts, Compl  Subs, Subs  Subs, Mn/Sub  Subs, unSub  Team Success, PPM  Team Success, onG  Team Success, onGA  Team Success, +/-  Team Success, +/-90  \\\n",
       "0  Patrick van Aanholt-NED-1990  2017-2018  Patrick van Aanholt   nl NED  1.0  Crystal Palace  eng Premier League   26  1990                28                    25               2184               24.3                 5                 1                 6                  5                0                   0                  7                  0           3.1             3.1            2.1                 5.2               46.0               92.0               86.0                 0.21                 0.04                 0.25                  0.21                    0.25                0.13                 0.09                    0.21                  0.13                      0.21                  28                 2184                   78                63.9                 24.3              25               NaN           21.0           3           NaN            8               1.21               31.0                38.0               -7.0                -0.29   \n",
       "1       Rolando Aarons-ENG-1995  2017-2018       Rolando Aarons  eng ENG  5.0   Newcastle Utd  eng Premier League   21  1995                 4                     1                139                1.5                 0                 0                 0                  0                0                   0                  0                  0           0.1             0.1            0.0                 0.1                7.0                3.0                4.0                 0.00                 0.00                 0.00                  0.00                    0.00                0.04                 0.00                    0.04                  0.04                      0.04                   4                  139                   35                 4.1                  1.5               1               NaN            0.0           3           NaN            7               0.25                1.0                 2.0               -1.0                -0.65   \n",
       "2       Rolando Aarons-ENG-1995  2017-2018       Rolando Aarons  eng ENG  5.0   Hellas Verona          it Serie A   21  1995                11                     6                517                5.7                 0                 0                 0                  0                0                   0                  0                  0           0.2             0.2            0.2                 0.3                9.0               17.0               31.0                 0.00                 0.00                 0.00                  0.00                    0.00                0.03                 0.03                    0.06                  0.03                      0.06                  11                  517                   47                15.1                  5.7               6               NaN            1.0           5           NaN            4               0.55                2.0                12.0              -10.0                -1.74   \n",
       "3        Ignazio Abate-ITA-1986  2017-2018        Ignazio Abate   it ITA  1.0           Milan          it Serie A   30  1986                17                    11               1057               11.7                 1                 0                 1                  1                0                   0                  3                  0           0.2             0.2            0.5                 0.7               20.0               81.0               65.0                 0.09                 0.00                 0.09                  0.09                    0.09                0.01                 0.04                    0.06                  0.01                      0.06                  17                 1057                   62                30.9                 11.7              11               NaN            7.0           6           NaN           15               1.71               17.0                10.0                7.0                 0.60   \n",
       "4      Aymen Abdennour-TUN-1989  2017-2018      Aymen Abdennour   tn TUN  1.0       Marseille          fr Ligue 1   27  1989                 8                     6                499                5.5                 0                 0                 0                  0                0                   0                  3                  0           0.1             0.1            0.0                 0.1                2.0               12.0                0.0                 0.00                 0.00                 0.00                  0.00                    0.00                0.02                 0.00                    0.02                  0.02                      0.02                   8                  499                   62                14.6                  5.5               6              82.0            4.0           2           3.0           18               2.38               10.0                 4.0                6.0                 1.08   \n",
       "\n",
       "   Team Success, On-Off  Team Success (xG), onxG  Team Success (xG), onxGA  Team Success (xG), xG+/-  Team Success (xG), xG+/-90  Team Success (xG), On-Off  Standard, Gls  Standard, Sh  Standard, SoT  Standard, SoT%  Standard, Sh/90  Standard, SoT/90  Standard, G/Sh  Standard, G/SoT  Standard, Dist  Standard, FK  Standard, PK  Standard, PKatt  Expected, xG.1  Expected, npxG.1  Expected, npxG/Sh  Expected, G-xG  Expected, np:G-xG  Total, Cmp  Total, Att  Total, Cmp%  Total, TotDist  Total, PrgDist  Short, Cmp  Short, Att  Short, Cmp%  Medium, Cmp  Medium, Att  Medium, Cmp%  Long, Cmp  Long, Att  Long, Cmp%  Ast  xAG  Expected, xA  Expected, A-xAG    KP   1/3   PPA  CrsPA  PrgP     Att  Pass Types, Live  Pass Types, Dead  Pass Types, FK  Pass Types, TB  Pass Types, Sw  Pass Types, Crs  Pass Types, TI  Pass Types, CK  Corner Kicks, In  Corner Kicks, Out  Corner Kicks, Str  Outcomes, Cmp  Outcomes, Off  Outcomes, Blocks  SCA, SCA  SCA, SCA90  SCA Types, PassLive  SCA Types, PassDead  \\\n",
       "0                 -0.07                     32.2                      34.7                      -2.5                       -0.10                      -0.66              5          33.0             11            33.3             1.36              0.45            0.15             0.45            23.4           4.0             0                0             3.1               3.1               0.09             1.9                1.9       884.0      1176.0         75.2         14197.0          6422.0       479.0       532.0         90.0        311.0        431.0          72.2       74.0      152.0        48.7    1  2.1           1.8             -1.1  18.0  63.0  28.0    6.0  92.0  1176.0             897.0             276.0            30.0             3.0             2.0             56.0           235.0            11.0               3.0                5.0                2.0          884.0            3.0              31.0      46.0        1.90                 24.0                 12.0   \n",
       "1                 -0.46                      0.4                       2.9                      -2.6                       -1.66                      -1.43              0           2.0              0             0.0             1.29              0.00            0.00              NaN            19.9           0.0             0                0             0.1               0.1               0.03            -0.1               -0.1        29.0        44.0         65.9           376.0            77.0        24.0        30.0         80.0          2.0          5.0          40.0        2.0        3.0        66.7    0  0.0           0.0              0.0   0.0   2.0   1.0    1.0   3.0    44.0              41.0               2.0             2.0             0.0             1.0              2.0             0.0             0.0               0.0                0.0                0.0           29.0            1.0               3.0       1.0        0.65                  0.0                  0.0   \n",
       "2                 -0.56                      2.7                      10.0                      -7.3                       -1.26                      -0.34              0           3.0              0             0.0             0.52              0.00            0.00              NaN            17.4           0.0             0                0             0.2               0.2               0.05            -0.2               -0.2        87.0       120.0         72.5          1174.0           325.0        50.0        60.0         83.3         25.0         35.0          71.4        4.0        8.0        50.0    0  0.2           0.1             -0.2   3.0   8.0   7.0    1.0  17.0   120.0             110.0              10.0             4.0             0.0             0.0             10.0             4.0             0.0               0.0                0.0                0.0           87.0            0.0               5.0       5.0        0.87                  4.0                  1.0   \n",
       "3                  0.33                     13.8                      11.4                       2.5                        0.21                      -0.24              1           4.0              2            50.0             0.34              0.17            0.25             0.50            17.1           0.0             0                0             0.2               0.2               0.04             0.8                0.8       625.0       776.0         80.5         10991.0          4535.0       273.0       294.0         92.9        287.0        333.0          86.2       58.0      120.0        48.3    0  0.5           0.8             -0.5  10.0  55.0  20.0    7.0  81.0   776.0             653.0             119.0             7.0             0.0             2.0             31.0           112.0             0.0               0.0                0.0                0.0          625.0            4.0              21.0      27.0        2.30                 24.0                  1.0   \n",
       "4                  0.25                     12.0                       4.2                       7.8                        1.41                       0.53              0           2.0              1            50.0             0.36              0.18            0.00             0.00            11.5           0.0             0                0             0.1               0.1               0.06            -0.1               -0.1       310.0       333.0         93.1          5550.0          1557.0       126.0       128.0         98.4        148.0        155.0          95.5       24.0       34.0        70.6    0  0.0           0.0              0.0   0.0   8.0   0.0    0.0  12.0   333.0             325.0               7.0             7.0             0.0             3.0              0.0             0.0             0.0               0.0                0.0                0.0          310.0            1.0               2.0       2.0        0.36                  2.0                  0.0   \n",
       "\n",
       "   SCA Types, TO  SCA Types, Sh  SCA Types, Fld  SCA Types, Def  GCA, GCA  GCA, GCA90  GCA Types, PassLive  GCA Types, PassDead  GCA Types, TO  GCA Types, Sh  GCA Types, Fld  GCA Types, Def  Tackles, Tkl  Tackles, TklW  Tackles, Def 3rd  Tackles, Mid 3rd  Tackles, Att 3rd  Challenges, Tkl  Challenges, Att  Challenges, Tkl%  Challenges, Lost  Blocks, Blocks  Blocks, Sh  Blocks, Pass   Int  Tkl+Int   Clr  Err  Touches, Touches  Touches, Def Pen  Touches, Def 3rd  Touches, Mid 3rd  Touches, Att 3rd  Touches, Att Pen  Touches, Live  Take-Ons, Att  Take-Ons, Succ  Take-Ons, Succ%  Take-Ons, Tkld  Take-Ons, Tkld%  Carries, Carries  Carries, TotDist  Carries, PrgDist  Carries, PrgC  Carries, 1/3  Carries, CPA  Carries, Mis  Carries, Dis  Receiving, Rec  Receiving, PrgR  \n",
       "0            6.0            2.0             1.0             1.0       4.0        0.16                  2.0                  0.0            2.0            0.0             0.0             0.0          47.0           32.0              29.0              15.0               3.0             16.0             34.0              47.1              18.0            24.0         5.0          19.0  47.0     94.0  64.0  2.0            1435.0              90.0             445.0             616.0             391.0              32.0         1435.0           31.0            26.0             83.9             5.0             16.1             717.0            3714.0            1966.0           46.0          33.0           7.0          37.0          18.0           711.0             86.0  \n",
       "1            0.0            1.0             0.0             0.0       0.0        0.00                  0.0                  0.0            0.0            0.0             0.0             0.0           4.0            4.0               3.0               1.0               0.0              4.0              6.0              66.7               2.0             3.0         0.0           3.0   1.0      5.0   0.0  0.0              65.0               0.0              11.0              30.0              25.0               3.0           65.0            6.0             4.0             66.7             2.0             33.3              37.0             380.0             127.0            7.0           3.0           3.0           4.0           6.0            39.0              4.0  \n",
       "2            0.0            0.0             0.0             0.0       0.0        0.00                  0.0                  0.0            0.0            0.0             0.0             0.0          13.0            8.0               4.0               4.0               5.0              1.0              7.0              14.3               6.0             3.0         1.0           2.0   2.0     15.0   0.0  0.0             166.0               3.0              18.0              71.0              80.0               7.0          166.0           15.0             8.0             53.3             7.0             46.7             105.0             712.0             352.0            9.0          11.0           2.0           9.0          13.0           125.0             31.0  \n",
       "3            0.0            2.0             0.0             0.0       4.0        0.34                  3.0                  0.0            0.0            1.0             0.0             0.0          20.0           17.0              10.0               9.0               1.0              6.0             11.0              54.5               5.0            22.0         2.0          20.0   8.0     28.0  29.0  0.0             864.0              35.0             237.0             428.0             206.0               8.0          864.0            1.0             0.0              0.0             1.0            100.0             386.0            1802.0            1124.0           20.0          18.0           0.0           7.0           6.0           563.0             65.0  \n",
       "4            0.0            0.0             0.0             0.0       0.0        0.00                  0.0                  0.0            0.0            0.0             0.0             0.0           7.0            3.0               5.0               2.0               0.0              4.0              7.0              57.1               3.0             5.0         5.0           0.0   4.0     11.0  20.0  0.0             375.0              27.0             175.0             200.0               4.0               2.0          375.0            1.0             1.0            100.0             0.0              0.0             296.0            1522.0             829.0            2.0           1.0           0.0           0.0           1.0           278.0              0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing my Data. \n",
    "df = pd.read_csv('../data/eeaao.csv')\n",
    "df = df.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns: print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some initial thoughts\n",
    "Following are some initial thoughts I was planning on tackling this problem that couldnt take off the ground, and whose EDA/planning was too messy/incoherent to make it to this notebook.\n",
    "<br><br>\n",
    "I was looking to treat each datapoint as a sentence in typical RNN fashion. This would skip the usual token vectorization since the data is already numeric. However, now comes the question of 'what is a datapoint?' for any given player, do we have 7 datapoints (one for each season) of sentence length 160 and we try to predict the entire next year, or do we have 160 datapoints (one for each feature) of size 7 and we try to predict the next element (word) of that features sequence (sentence)?\n",
    "<br>\n",
    "The issue with the first way is that sentences are extremely long and the idea of generating sentences that way can lead to 'jumbled sentenced'. The issue with the second way is that we have to teach 160 models how to interact with each other and we lose any value of using NNs due to lack of interaction of features. \n",
    "<br><br>\n",
    "Whilst the second approach makes no sense to use, the first approach has somoe promise, with some alterations to the structure of the data and the preprocessing techniques. \n",
    "\n",
    "---\n",
    "\n",
    "#### The approach\n",
    "1. We will runn a similar 'cleaning' operation as in Sprint 2 on the entire dataset to make sure each player has only one datapoint per season\n",
    "2. For our first attempt we will also filter to only players that have datapoints for every season (so we dont have to worry about putting NaNs into the model) and only try to predict our previously defined 'impact' metrics\n",
    "3. Generate the cleaned 3D matrix and to a traintest separate X_train and Y_train\n",
    "4. train a simple (not too many layers/ big layers) LTSM NN \n",
    "5. evaluate\n",
    "##### Problems with Approach 1\n",
    "- Missing important players (obv)\n",
    "- Bias towards aging players or players on teams that stay up\n",
    "##### Places to progress\n",
    "- train on all players. Will also require funky preprocessing. (update: done. we inserted 'empty' rows for players with missing seasons)\n",
    "- bigger NN (update: Turns out we are very prone to overfitting and bigger =/= better)\n",
    "- different activations (update: not really much to do here)\n",
    "- combine all the above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10178, 169)\n"
     ]
    }
   ],
   "source": [
    "# The objective of this block of code is to consolidate player datapoints such that each player for each season has 'at most' one row. \n",
    "# We are also not dealing with any funny business with transfers.\n",
    "tgb = df.groupby('p_id')  # Group by the unique players. \n",
    "\n",
    "seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024']\n",
    "\n",
    "df_by_pid = {}  # {pid: df}\n",
    "for pid in tgb.groups.keys():\n",
    "    if tgb.get_group(pid).shape[0] >= 5:\n",
    "        df_by_pid[pid] = tgb.get_group(pid)\n",
    "\n",
    "full_careers = {}  # {pid: cleaned_df}\n",
    "total = pd.DataFrame()\n",
    "n_full = 0\n",
    "\n",
    "# want to get rid of rows where trnsfer window gets in the way. This means for the same season, there are two rows\n",
    "# If for one of them has Playing Time, Min > 180, ignore this player. Otherwise, take the max. \n",
    "for pid in df_by_pid.keys():\n",
    "    # print(pid)\n",
    "    omit_player = False\n",
    "    pid_df = pd.DataFrame()\n",
    "    # gor into pdf and ensure for each \n",
    "    # go through each season JUST TO CHECK WHETHER WE SHOULD OMIT THS PLAYER\n",
    "    df_by_pid_season = df_by_pid[pid].groupby('Season')  # groupby object where keys are seasons\n",
    "    for season in df_by_pid_season.groups.keys(): \n",
    "        season_df = df_by_pid_season.get_group(season)\n",
    "        if season_df.shape[0] >= 2 and sum(season_df['Playing Time, Min'] > 270) >= 2: \n",
    "            # So we want to drop this season from this player.\n",
    "            # # We can do this by just ignoring this season and moving on to the next.\n",
    "            pass\n",
    "        elif season_df.shape[0] >= 2:\n",
    "            # This player has two rows for this season, but only one of them is keepable\n",
    "            # print(f'{pid} transferred in {season}')\n",
    "            most_mins = season_df['Playing Time, Min'].max()\n",
    "            season_df = season_df.loc[season_df['Playing Time, Min'] == most_mins]\n",
    "            pid_df = pd.concat([pid_df, season_df], axis=0)\n",
    "        else:\n",
    "            # This season is good to add to this players dataframe. \n",
    "            pid_df = pd.concat([pid_df, season_df], axis=0)\n",
    "    full_careers[pid] = pid_df\n",
    "\n",
    "    # full_careers[pid] = df_by_pid[pid].loc[df_by_pid_season['Playing Time, Min'].idxmax()]\n",
    "\n",
    "    if not omit_player and full_careers[pid].shape[0] >= 5:  # This player has at least 5 seasons\n",
    "        n_full+= 1\n",
    "        player_data = full_careers[pid]\n",
    "        # Here we want to find which seasons are missing, then populate them with 'empty' rows. \n",
    "        placeholder_value1 = -500\n",
    "        for season in seasons:\n",
    "            if season not in list(player_data['Season']):\n",
    "                # Add the season row with blanks\n",
    "                # print(f'{pid} needs {season}')\n",
    "                player_name = player_data['Player'].mode()[0]\n",
    "                nation = player_data['Nation'].mode()[0]\n",
    "                born = player_data['Born'].mode()[0]\n",
    "                position = player_data['Pos'].mode()[0]\n",
    "                squad = player_data['Squad'].mode()[0]\n",
    "                comp = player_data['Comp'].mode()[0]\n",
    "\n",
    "                age = int(season.split('-')[0]) - born              \n",
    "\n",
    "                new_row = [pid, season, player_name, nation, position, squad, comp, age, born] + [placeholder_value1] * 160\n",
    "                new_row_df = pd.DataFrame([new_row], columns=df.columns)\n",
    "                player_data = pd.concat([player_data, new_row_df], ignore_index=True)\n",
    "                \n",
    "        total = pd.concat([total, player_data], axis=0)\n",
    "\n",
    "# Now full careers contans all of the players who have at least 7 datapoints {p_id: df}  717 of them\n",
    "print(total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_careers[pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gb = total.groupby('p_id')\n",
    "for pid in test_gb.groups.keys():\n",
    "    print(pid)\n",
    "    assert test_gb.get_group(pid).shape == (7, 169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.sort_values(['p_id', 'Season'])\n",
    "\n",
    "placeholder_value = -999.0\n",
    "nan_total = total.fillna(placeholder_value)\n",
    "\n",
    "# Get from total\n",
    "num_players = total['p_id'].unique().shape[0]\n",
    "data = nan_total.to_numpy().reshape(num_players, 7, 169)\n",
    "\n",
    "# We now need to set up the 3d matrix and do a train/test split\n",
    "\n",
    "players = total['p_id'].unique()\n",
    "\"\"\"\n",
    "original technique\n",
    "# Reserving a small portion of the players for testing\n",
    "train_players, test_players = train_test_split(players, test_size=0.1, random_state=42, shuffle=False)  \n",
    "\n",
    "# Now put the portions of the players into the 3d format\n",
    "train_indices = [i for i, player in enumerate(players) if player in train_players]\n",
    "test_indices = [i for i, player in enumerate(players) if player in test_players]\n",
    "\"\"\"\n",
    "\n",
    "train_indices, test_indices = train_test_split(list(range(469)), test_size=0.1, random_state=42) \n",
    "train_indices, test_indices = list(train_indices), list(test_indices)\n",
    "\n",
    "# getting non-string features\n",
    "i_s = [4]\n",
    "i_s.extend(list(range(7, 169)))  # Numerical indicies for training\n",
    "ti_s = i_s[3:]  # Numerical indicies for prediction. \n",
    "# removing the first three features had a major impact, taking r2_score of the NN from -2000 (you read that right) to -2\n",
    "\n",
    "split_i = math.ceil(0.85 * num_players)\n",
    "X_train, Y_train = data[:split_i, :-1, i_s], data[:split_i, -1, ti_s]\n",
    "X_test, Y_test = data[split_i:, :-1, i_s], data[split_i:, -1, ti_s]\n",
    "\n",
    "# Ensuring floats for NN\n",
    "X_train = X_train.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "Y_test = Y_test.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Scaling\n",
    "ss = StandardScaler()\n",
    "X_train = X_train.reshape(-1, len(i_s))\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_train = X_train.reshape(split_i, 6, len(i_s))\n",
    "X_test = X_test.reshape(-1, len(i_s))\n",
    "X_test = ss.transform(X_test)\n",
    "X_test = X_test.reshape(num_players - split_i, 6, len(i_s))\n",
    "\n",
    "Y_train = Y_train.reshape(-1, len(ti_s))\n",
    "Y_train = ss.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(split_i, len(ti_s))\n",
    "Y_test = Y_test.reshape(-1, len(ti_s))\n",
    "Y_test = ss.transform(Y_test)\n",
    "Y_test = Y_test.reshape(num_players - split_i, len(ti_s))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1236, 6, 163)\n",
      "Y_train shape: (1236, 160)\n",
      "X_test shape: (218, 6, 163)\n",
      "Y_test shape: (218, 160)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# Print shapes for verification\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n",
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "14/14 [==============================] - 13s 217ms/step - loss: 10.1121 - val_loss: 10.0911\n",
      "Epoch 2/1000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 10.0636 - val_loss: 10.0487\n",
      "Epoch 3/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 10.0222 - val_loss: 10.0084\n",
      "Epoch 4/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.9821 - val_loss: 9.9686\n",
      "Epoch 5/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 9.9424 - val_loss: 9.9291\n",
      "Epoch 6/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 9.9028 - val_loss: 9.8897\n",
      "Epoch 7/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.8636 - val_loss: 9.8505\n",
      "Epoch 8/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 9.8243 - val_loss: 9.8114\n",
      "Epoch 9/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 9.7853 - val_loss: 9.7724\n",
      "Epoch 10/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 9.7462 - val_loss: 9.7335\n",
      "Epoch 11/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 9.7072 - val_loss: 9.6948\n",
      "Epoch 12/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.6683 - val_loss: 9.6562\n",
      "Epoch 13/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.6299 - val_loss: 9.6177\n",
      "Epoch 14/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.5912 - val_loss: 9.5793\n",
      "Epoch 15/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.5529 - val_loss: 9.5411\n",
      "Epoch 16/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 9.5148 - val_loss: 9.5030\n",
      "Epoch 17/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 9.4769 - val_loss: 9.4650\n",
      "Epoch 18/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 9.4387 - val_loss: 9.4271\n",
      "Epoch 19/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 9.4008 - val_loss: 9.3894\n",
      "Epoch 20/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 9.3629 - val_loss: 9.3517\n",
      "Epoch 21/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 9.3252 - val_loss: 9.3142\n",
      "Epoch 22/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.2878 - val_loss: 9.2768\n",
      "Epoch 23/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.2504 - val_loss: 9.2396\n",
      "Epoch 24/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 9.2134 - val_loss: 9.2025\n",
      "Epoch 25/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 9.1759 - val_loss: 9.1654\n",
      "Epoch 26/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 9.1389 - val_loss: 9.1285\n",
      "Epoch 27/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 9.1022 - val_loss: 9.0918\n",
      "Epoch 28/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 9.0651 - val_loss: 9.0551\n",
      "Epoch 29/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 9.0285 - val_loss: 9.0186\n",
      "Epoch 30/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.9920 - val_loss: 8.9822\n",
      "Epoch 31/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.9556 - val_loss: 8.9459\n",
      "Epoch 32/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.9191 - val_loss: 8.9097\n",
      "Epoch 33/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.8830 - val_loss: 8.8736\n",
      "Epoch 34/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.8470 - val_loss: 8.8377\n",
      "Epoch 35/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.8113 - val_loss: 8.8019\n",
      "Epoch 36/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.7752 - val_loss: 8.7662\n",
      "Epoch 37/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.7399 - val_loss: 8.7306\n",
      "Epoch 38/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.7038 - val_loss: 8.6952\n",
      "Epoch 39/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.6684 - val_loss: 8.6598\n",
      "Epoch 40/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.6334 - val_loss: 8.6246\n",
      "Epoch 41/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 8.5978 - val_loss: 8.5895\n",
      "Epoch 42/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 8.5626 - val_loss: 8.5545\n",
      "Epoch 43/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.5277 - val_loss: 8.5197\n",
      "Epoch 44/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.4928 - val_loss: 8.4849\n",
      "Epoch 45/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.4576 - val_loss: 8.4503\n",
      "Epoch 46/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.4233 - val_loss: 8.4157\n",
      "Epoch 47/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 8.3885 - val_loss: 8.3813\n",
      "Epoch 48/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 8.3537 - val_loss: 8.3470\n",
      "Epoch 49/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.3199 - val_loss: 8.3129\n",
      "Epoch 50/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 8.2853 - val_loss: 8.2788\n",
      "Epoch 51/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.2515 - val_loss: 8.2449\n",
      "Epoch 52/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 8.2173 - val_loss: 8.2110\n",
      "Epoch 53/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 8.1835 - val_loss: 8.1773\n",
      "Epoch 54/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 8.1505 - val_loss: 8.1437\n",
      "Epoch 55/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 8.1166 - val_loss: 8.1102\n",
      "Epoch 56/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.0829 - val_loss: 8.0769\n",
      "Epoch 57/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.0492 - val_loss: 8.0436\n",
      "Epoch 58/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 8.0161 - val_loss: 8.0104\n",
      "Epoch 59/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 7.9830 - val_loss: 7.9774\n",
      "Epoch 60/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 7.9494 - val_loss: 7.9445\n",
      "Epoch 61/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 7.9167 - val_loss: 7.9117\n",
      "Epoch 62/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 7.8828 - val_loss: 7.8790\n",
      "Epoch 63/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.8509 - val_loss: 7.8464\n",
      "Epoch 64/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.8182 - val_loss: 7.8139\n",
      "Epoch 65/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 7.7859 - val_loss: 7.7815\n",
      "Epoch 66/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 7.7528 - val_loss: 7.7492\n",
      "Epoch 67/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 7.7206 - val_loss: 7.7171\n",
      "Epoch 68/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 7.6887 - val_loss: 7.6850\n",
      "Epoch 69/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.6570 - val_loss: 7.6531\n",
      "Epoch 70/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 7.6243 - val_loss: 7.6212\n",
      "Epoch 71/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.5922 - val_loss: 7.5895\n",
      "Epoch 72/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 7.5603 - val_loss: 7.5578\n",
      "Epoch 73/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 7.5287 - val_loss: 7.5263\n",
      "Epoch 74/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 7.4974 - val_loss: 7.4950\n",
      "Epoch 75/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 7.4662 - val_loss: 7.4636\n",
      "Epoch 76/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 7.4336 - val_loss: 7.4325\n",
      "Epoch 77/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 7.4024 - val_loss: 7.4014\n",
      "Epoch 78/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.3710 - val_loss: 7.3704\n",
      "Epoch 79/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.3405 - val_loss: 7.3395\n",
      "Epoch 80/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.3090 - val_loss: 7.3087\n",
      "Epoch 81/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 7.2790 - val_loss: 7.2780\n",
      "Epoch 82/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 7.2465 - val_loss: 7.2474\n",
      "Epoch 83/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 7.2180 - val_loss: 7.2169\n",
      "Epoch 84/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 7.1859 - val_loss: 7.1865\n",
      "Epoch 85/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 7.1539 - val_loss: 7.1562\n",
      "Epoch 86/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.1238 - val_loss: 7.1260\n",
      "Epoch 87/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.0945 - val_loss: 7.0959\n",
      "Epoch 88/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.0639 - val_loss: 7.0659\n",
      "Epoch 89/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.0331 - val_loss: 7.0359\n",
      "Epoch 90/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 7.0035 - val_loss: 7.0061\n",
      "Epoch 91/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.9731 - val_loss: 6.9763\n",
      "Epoch 92/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 6.9441 - val_loss: 6.9467\n",
      "Epoch 93/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 6.9133 - val_loss: 6.9171\n",
      "Epoch 94/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 6.8833 - val_loss: 6.8876\n",
      "Epoch 95/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 6.8529 - val_loss: 6.8581\n",
      "Epoch 96/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.8244 - val_loss: 6.8288\n",
      "Epoch 97/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.7930 - val_loss: 6.7995\n",
      "Epoch 98/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.7640 - val_loss: 6.7704\n",
      "Epoch 99/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.7345 - val_loss: 6.7413\n",
      "Epoch 100/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.7074 - val_loss: 6.7122\n",
      "Epoch 101/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 6.6752 - val_loss: 6.6833\n",
      "Epoch 102/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 6.6463 - val_loss: 6.6544\n",
      "Epoch 103/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 6.6159 - val_loss: 6.6256\n",
      "Epoch 104/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 6.5904 - val_loss: 6.5969\n",
      "Epoch 105/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 6.5596 - val_loss: 6.5683\n",
      "Epoch 106/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.5305 - val_loss: 6.5398\n",
      "Epoch 107/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 6.5030 - val_loss: 6.5113\n",
      "Epoch 108/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.4736 - val_loss: 6.4829\n",
      "Epoch 109/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.4453 - val_loss: 6.4544\n",
      "Epoch 110/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.4190 - val_loss: 6.4261\n",
      "Epoch 111/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.3864 - val_loss: 6.3979\n",
      "Epoch 112/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 6.3613 - val_loss: 6.3699\n",
      "Epoch 113/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 6.3316 - val_loss: 6.3418\n",
      "Epoch 114/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 6.3016 - val_loss: 6.3138\n",
      "Epoch 115/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 6.2767 - val_loss: 6.2859\n",
      "Epoch 116/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 6.2500 - val_loss: 6.2582\n",
      "Epoch 117/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 6.2204 - val_loss: 6.2306\n",
      "Epoch 118/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 6.1947 - val_loss: 6.2030\n",
      "Epoch 119/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.1662 - val_loss: 6.1757\n",
      "Epoch 120/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.1381 - val_loss: 6.1485\n",
      "Epoch 121/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.1138 - val_loss: 6.1216\n",
      "Epoch 122/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 6.0898 - val_loss: 6.0947\n",
      "Epoch 123/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 6.0601 - val_loss: 6.0678\n",
      "Epoch 124/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 6.0354 - val_loss: 6.0411\n",
      "Epoch 125/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 6.0080 - val_loss: 6.0147\n",
      "Epoch 126/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.9813 - val_loss: 5.9882\n",
      "Epoch 127/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.9529 - val_loss: 5.9621\n",
      "Epoch 128/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.9310 - val_loss: 5.9362\n",
      "Epoch 129/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 5.9085 - val_loss: 5.9101\n",
      "Epoch 130/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 5.8801 - val_loss: 5.8841\n",
      "Epoch 131/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 5.8526 - val_loss: 5.8585\n",
      "Epoch 132/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.8246 - val_loss: 5.8329\n",
      "Epoch 133/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.8056 - val_loss: 5.8074\n",
      "Epoch 134/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.7785 - val_loss: 5.7822\n",
      "Epoch 135/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 5.7539 - val_loss: 5.7569\n",
      "Epoch 136/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.7253 - val_loss: 5.7323\n",
      "Epoch 137/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.6972 - val_loss: 5.7071\n",
      "Epoch 138/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.6796 - val_loss: 5.6821\n",
      "Epoch 139/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 5.6521 - val_loss: 5.6574\n",
      "Epoch 140/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 5.6302 - val_loss: 5.6329\n",
      "Epoch 141/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 5.5985 - val_loss: 5.6084\n",
      "Epoch 142/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.5801 - val_loss: 5.5843\n",
      "Epoch 143/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 5.5559 - val_loss: 5.5601\n",
      "Epoch 144/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.5332 - val_loss: 5.5360\n",
      "Epoch 145/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 5.5099 - val_loss: 5.5119\n",
      "Epoch 146/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.4857 - val_loss: 5.4879\n",
      "Epoch 147/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.4611 - val_loss: 5.4638\n",
      "Epoch 148/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 5.4318 - val_loss: 5.4402\n",
      "Epoch 149/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 5.4128 - val_loss: 5.4165\n",
      "Epoch 150/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 5.3875 - val_loss: 5.3924\n",
      "Epoch 151/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.3596 - val_loss: 5.3689\n",
      "Epoch 152/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.3439 - val_loss: 5.3459\n",
      "Epoch 153/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.3183 - val_loss: 5.3228\n",
      "Epoch 154/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.2992 - val_loss: 5.2998\n",
      "Epoch 155/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 5.2726 - val_loss: 5.2766\n",
      "Epoch 156/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 5.2547 - val_loss: 5.2539\n",
      "Epoch 157/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 5.2275 - val_loss: 5.2312\n",
      "Epoch 158/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 5.2042 - val_loss: 5.2082\n",
      "Epoch 159/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.1812 - val_loss: 5.1866\n",
      "Epoch 160/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.1588 - val_loss: 5.1642\n",
      "Epoch 161/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.1377 - val_loss: 5.1415\n",
      "Epoch 162/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.1189 - val_loss: 5.1195\n",
      "Epoch 163/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 5.0974 - val_loss: 5.0970\n",
      "Epoch 164/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 5.0731 - val_loss: 5.0746\n",
      "Epoch 165/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 5.0472 - val_loss: 5.0526\n",
      "Epoch 166/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 5.0244 - val_loss: 5.0307\n",
      "Epoch 167/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 5.0070 - val_loss: 5.0086\n",
      "Epoch 168/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 4.9856 - val_loss: 4.9871\n",
      "Epoch 169/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.9625 - val_loss: 4.9656\n",
      "Epoch 170/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.9427 - val_loss: 4.9441\n",
      "Epoch 171/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 4.9225 - val_loss: 4.9235\n",
      "Epoch 172/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.8953 - val_loss: 4.9026\n",
      "Epoch 173/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 4.8778 - val_loss: 4.8813\n",
      "Epoch 174/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 4.8540 - val_loss: 4.8607\n",
      "Epoch 175/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.8359 - val_loss: 4.8393\n",
      "Epoch 176/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.8167 - val_loss: 4.8186\n",
      "Epoch 177/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.7943 - val_loss: 4.7978\n",
      "Epoch 178/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.7743 - val_loss: 4.7767\n",
      "Epoch 179/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.7528 - val_loss: 4.7562\n",
      "Epoch 180/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.7358 - val_loss: 4.7358\n",
      "Epoch 181/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 4.7156 - val_loss: 4.7149\n",
      "Epoch 182/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 4.6905 - val_loss: 4.6947\n",
      "Epoch 183/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 4.6701 - val_loss: 4.6751\n",
      "Epoch 184/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.6506 - val_loss: 4.6548\n",
      "Epoch 185/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.6315 - val_loss: 4.6350\n",
      "Epoch 186/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.6103 - val_loss: 4.6150\n",
      "Epoch 187/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.5919 - val_loss: 4.5955\n",
      "Epoch 188/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.5724 - val_loss: 4.5753\n",
      "Epoch 189/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.5505 - val_loss: 4.5556\n",
      "Epoch 190/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 4.5317 - val_loss: 4.5354\n",
      "Epoch 191/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.5116 - val_loss: 4.5165\n",
      "Epoch 192/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.4865 - val_loss: 4.4965\n",
      "Epoch 193/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 4.4757 - val_loss: 4.4774\n",
      "Epoch 194/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.4572 - val_loss: 4.4583\n",
      "Epoch 195/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 4.4332 - val_loss: 4.4390\n",
      "Epoch 196/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 4.4177 - val_loss: 4.4194\n",
      "Epoch 197/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.3984 - val_loss: 4.4008\n",
      "Epoch 198/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 4.3780 - val_loss: 4.3820\n",
      "Epoch 199/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.3570 - val_loss: 4.3633\n",
      "Epoch 200/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 4.3429 - val_loss: 4.3444\n",
      "Epoch 201/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.3220 - val_loss: 4.3258\n",
      "Epoch 202/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 4.3036 - val_loss: 4.3062\n",
      "Epoch 203/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 4.2881 - val_loss: 4.2881\n",
      "Epoch 204/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.2679 - val_loss: 4.2695\n",
      "Epoch 205/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 4.2438 - val_loss: 4.2516\n",
      "Epoch 206/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 4.2293 - val_loss: 4.2336\n",
      "Epoch 207/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.2169 - val_loss: 4.2169\n",
      "Epoch 208/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.1934 - val_loss: 4.1984\n",
      "Epoch 209/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 4.1779 - val_loss: 4.1805\n",
      "Epoch 210/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 4.1567 - val_loss: 4.1627\n",
      "Epoch 211/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.1417 - val_loss: 4.1460\n",
      "Epoch 212/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 4.1231 - val_loss: 4.1279\n",
      "Epoch 213/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 4.1058 - val_loss: 4.1100\n",
      "Epoch 214/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 4.0903 - val_loss: 4.0921\n",
      "Epoch 215/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.0729 - val_loss: 4.0751\n",
      "Epoch 216/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.0517 - val_loss: 4.0576\n",
      "Epoch 217/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.0359 - val_loss: 4.0404\n",
      "Epoch 218/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.0180 - val_loss: 4.0230\n",
      "Epoch 219/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.0036 - val_loss: 4.0051\n",
      "Epoch 220/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 3.9848 - val_loss: 3.9893\n",
      "Epoch 221/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 3.9683 - val_loss: 3.9733\n",
      "Epoch 222/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.9493 - val_loss: 3.9565\n",
      "Epoch 223/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.9374 - val_loss: 3.9392\n",
      "Epoch 224/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.9203 - val_loss: 3.9230\n",
      "Epoch 225/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.9010 - val_loss: 3.9047\n",
      "Epoch 226/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.8872 - val_loss: 3.8881\n",
      "Epoch 227/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.8694 - val_loss: 3.8717\n",
      "Epoch 228/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.8506 - val_loss: 3.8561\n",
      "Epoch 229/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.8398 - val_loss: 3.8391\n",
      "Epoch 230/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 3.8214 - val_loss: 3.8238\n",
      "Epoch 231/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.8049 - val_loss: 3.8075\n",
      "Epoch 232/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.7882 - val_loss: 3.7915\n",
      "Epoch 233/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.7703 - val_loss: 3.7755\n",
      "Epoch 234/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 3.7557 - val_loss: 3.7598\n",
      "Epoch 235/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 3.7391 - val_loss: 3.7444\n",
      "Epoch 236/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.7243 - val_loss: 3.7278\n",
      "Epoch 237/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.7074 - val_loss: 3.7137\n",
      "Epoch 238/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 3.6940 - val_loss: 3.6975\n",
      "Epoch 239/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 3.6812 - val_loss: 3.6806\n",
      "Epoch 240/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.6635 - val_loss: 3.6669\n",
      "Epoch 241/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 3.6459 - val_loss: 3.6514\n",
      "Epoch 242/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.6298 - val_loss: 3.6366\n",
      "Epoch 243/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.6179 - val_loss: 3.6224\n",
      "Epoch 244/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.6052 - val_loss: 3.6072\n",
      "Epoch 245/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 3.5899 - val_loss: 3.5908\n",
      "Epoch 246/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 3.5737 - val_loss: 3.5750\n",
      "Epoch 247/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 3.5602 - val_loss: 3.5605\n",
      "Epoch 248/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 3.5402 - val_loss: 3.5465\n",
      "Epoch 249/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 3.5263 - val_loss: 3.5297\n",
      "Epoch 250/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.5130 - val_loss: 3.5161\n",
      "Epoch 251/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 3.5017 - val_loss: 3.5028\n",
      "Epoch 252/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.4823 - val_loss: 3.4880\n",
      "Epoch 253/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 3.4694 - val_loss: 3.4736\n",
      "Epoch 254/1000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 3.4546 - val_loss: 3.4599\n",
      "Epoch 255/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 3.4423 - val_loss: 3.4441\n",
      "Epoch 256/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 3.4241 - val_loss: 3.4300\n",
      "Epoch 257/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.4167 - val_loss: 3.4146\n",
      "Epoch 258/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.4017 - val_loss: 3.4007\n",
      "Epoch 259/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 3.3833 - val_loss: 3.3879\n",
      "Epoch 260/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.3678 - val_loss: 3.3747\n",
      "Epoch 261/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.3535 - val_loss: 3.3597\n",
      "Epoch 262/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 3.3476 - val_loss: 3.3462\n",
      "Epoch 263/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 3.3283 - val_loss: 3.3325\n",
      "Epoch 264/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.3178 - val_loss: 3.3181\n",
      "Epoch 265/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 3.3045 - val_loss: 3.3054\n",
      "Epoch 266/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.2893 - val_loss: 3.2922\n",
      "Epoch 267/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.2758 - val_loss: 3.2793\n",
      "Epoch 268/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.2634 - val_loss: 3.2655\n",
      "Epoch 269/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.2476 - val_loss: 3.2522\n",
      "Epoch 270/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 3.2339 - val_loss: 3.2400\n",
      "Epoch 271/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 3.2225 - val_loss: 3.2255\n",
      "Epoch 272/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.2056 - val_loss: 3.2119\n",
      "Epoch 273/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.1935 - val_loss: 3.1990\n",
      "Epoch 274/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.1852 - val_loss: 3.1862\n",
      "Epoch 275/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 3.1704 - val_loss: 3.1725\n",
      "Epoch 276/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.1549 - val_loss: 3.1596\n",
      "Epoch 277/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 3.1417 - val_loss: 3.1468\n",
      "Epoch 278/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 3.1283 - val_loss: 3.1336\n",
      "Epoch 279/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.1151 - val_loss: 3.1201\n",
      "Epoch 280/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.1033 - val_loss: 3.1075\n",
      "Epoch 281/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 3.0945 - val_loss: 3.0963\n",
      "Epoch 282/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.0828 - val_loss: 3.0839\n",
      "Epoch 283/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.0663 - val_loss: 3.0712\n",
      "Epoch 284/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 3.0567 - val_loss: 3.0587\n",
      "Epoch 285/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 3.0411 - val_loss: 3.0440\n",
      "Epoch 286/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 3.0336 - val_loss: 3.0322\n",
      "Epoch 287/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 3.0191 - val_loss: 3.0189\n",
      "Epoch 288/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 3.0066 - val_loss: 3.0076\n",
      "Epoch 289/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.9931 - val_loss: 2.9955\n",
      "Epoch 290/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.9834 - val_loss: 2.9837\n",
      "Epoch 291/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.9721 - val_loss: 2.9732\n",
      "Epoch 292/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.9612 - val_loss: 2.9607\n",
      "Epoch 293/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 2.9449 - val_loss: 2.9485\n",
      "Epoch 294/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.9345 - val_loss: 2.9372\n",
      "Epoch 295/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.9265 - val_loss: 2.9266\n",
      "Epoch 296/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.9151 - val_loss: 2.9154\n",
      "Epoch 297/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.9035 - val_loss: 2.9043\n",
      "Epoch 298/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.8854 - val_loss: 2.8910\n",
      "Epoch 299/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.8819 - val_loss: 2.8798\n",
      "Epoch 300/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 2.8688 - val_loss: 2.8703\n",
      "Epoch 301/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.8583 - val_loss: 2.8589\n",
      "Epoch 302/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.8505 - val_loss: 2.8483\n",
      "Epoch 303/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.8332 - val_loss: 2.8358\n",
      "Epoch 304/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.8225 - val_loss: 2.8247\n",
      "Epoch 305/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.8134 - val_loss: 2.8135\n",
      "Epoch 306/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.7990 - val_loss: 2.8027\n",
      "Epoch 307/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.7842 - val_loss: 2.7924\n",
      "Epoch 308/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.7784 - val_loss: 2.7812\n",
      "Epoch 309/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 2.7654 - val_loss: 2.7703\n",
      "Epoch 310/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.7606 - val_loss: 2.7605\n",
      "Epoch 311/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.7469 - val_loss: 2.7492\n",
      "Epoch 312/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.7396 - val_loss: 2.7388\n",
      "Epoch 313/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 2.7258 - val_loss: 2.7263\n",
      "Epoch 314/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.7142 - val_loss: 2.7154\n",
      "Epoch 315/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.7055 - val_loss: 2.7061\n",
      "Epoch 316/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.6937 - val_loss: 2.6953\n",
      "Epoch 317/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 2.6842 - val_loss: 2.6859\n",
      "Epoch 318/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.6712 - val_loss: 2.6762\n",
      "Epoch 319/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 2.6619 - val_loss: 2.6678\n",
      "Epoch 320/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.6481 - val_loss: 2.6555\n",
      "Epoch 321/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.6422 - val_loss: 2.6447\n",
      "Epoch 322/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.6355 - val_loss: 2.6360\n",
      "Epoch 323/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 2.6242 - val_loss: 2.6264\n",
      "Epoch 324/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 2.6124 - val_loss: 2.6157\n",
      "Epoch 325/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 2.6095 - val_loss: 2.6061\n",
      "Epoch 326/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.5896 - val_loss: 2.5965\n",
      "Epoch 327/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.5803 - val_loss: 2.5861\n",
      "Epoch 328/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.5789 - val_loss: 2.5773\n",
      "Epoch 329/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.5656 - val_loss: 2.5679\n",
      "Epoch 330/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.5548 - val_loss: 2.5582\n",
      "Epoch 331/1000\n",
      "14/14 [==============================] - 0s 36ms/step - loss: 2.5469 - val_loss: 2.5490\n",
      "Epoch 332/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.5364 - val_loss: 2.5398\n",
      "Epoch 333/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.5273 - val_loss: 2.5294\n",
      "Epoch 334/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.5174 - val_loss: 2.5183\n",
      "Epoch 335/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.5045 - val_loss: 2.5089\n",
      "Epoch 336/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.4972 - val_loss: 2.4993\n",
      "Epoch 337/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.4887 - val_loss: 2.4901\n",
      "Epoch 338/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.4830 - val_loss: 2.4830\n",
      "Epoch 339/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.4739 - val_loss: 2.4730\n",
      "Epoch 340/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 2.4599 - val_loss: 2.4637\n",
      "Epoch 341/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.4514 - val_loss: 2.4542\n",
      "Epoch 342/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.4455 - val_loss: 2.4473\n",
      "Epoch 343/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.4347 - val_loss: 2.4388\n",
      "Epoch 344/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.4262 - val_loss: 2.4281\n",
      "Epoch 345/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 2.4177 - val_loss: 2.4203\n",
      "Epoch 346/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.4089 - val_loss: 2.4105\n",
      "Epoch 347/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 2.4025 - val_loss: 2.4037\n",
      "Epoch 348/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.3912 - val_loss: 2.3947\n",
      "Epoch 349/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.3834 - val_loss: 2.3866\n",
      "Epoch 350/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.3737 - val_loss: 2.3782\n",
      "Epoch 351/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 2.3680 - val_loss: 2.3681\n",
      "Epoch 352/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.3597 - val_loss: 2.3616\n",
      "Epoch 353/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 2.3538 - val_loss: 2.3534\n",
      "Epoch 354/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.3397 - val_loss: 2.3445\n",
      "Epoch 355/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.3349 - val_loss: 2.3354\n",
      "Epoch 356/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 2.3205 - val_loss: 2.3267\n",
      "Epoch 357/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.3174 - val_loss: 2.3179\n",
      "Epoch 358/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.3082 - val_loss: 2.3111\n",
      "Epoch 359/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 2.3030 - val_loss: 2.3025\n",
      "Epoch 360/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.2863 - val_loss: 2.2935\n",
      "Epoch 361/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.2840 - val_loss: 2.2849\n",
      "Epoch 362/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.2739 - val_loss: 2.2758\n",
      "Epoch 363/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.2664 - val_loss: 2.2689\n",
      "Epoch 364/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.2648 - val_loss: 2.2599\n",
      "Epoch 365/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.2468 - val_loss: 2.2531\n",
      "Epoch 366/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.2420 - val_loss: 2.2467\n",
      "Epoch 367/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.2367 - val_loss: 2.2400\n",
      "Epoch 368/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.2263 - val_loss: 2.2336\n",
      "Epoch 369/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.2190 - val_loss: 2.2269\n",
      "Epoch 370/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.2110 - val_loss: 2.2167\n",
      "Epoch 371/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 2.2103 - val_loss: 2.2074\n",
      "Epoch 372/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.1973 - val_loss: 2.2002\n",
      "Epoch 373/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.1898 - val_loss: 2.1927\n",
      "Epoch 374/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.1838 - val_loss: 2.1854\n",
      "Epoch 375/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 2.1792 - val_loss: 2.1787\n",
      "Epoch 376/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.1668 - val_loss: 2.1716\n",
      "Epoch 377/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.1582 - val_loss: 2.1631\n",
      "Epoch 378/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.1540 - val_loss: 2.1559\n",
      "Epoch 379/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 2.1458 - val_loss: 2.1479\n",
      "Epoch 380/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.1352 - val_loss: 2.1403\n",
      "Epoch 381/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.1355 - val_loss: 2.1338\n",
      "Epoch 382/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.1262 - val_loss: 2.1269\n",
      "Epoch 383/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.1149 - val_loss: 2.1193\n",
      "Epoch 384/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.1105 - val_loss: 2.1124\n",
      "Epoch 385/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.1041 - val_loss: 2.1054\n",
      "Epoch 386/1000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 2.0940 - val_loss: 2.0987\n",
      "Epoch 387/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.0885 - val_loss: 2.0916\n",
      "Epoch 388/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.0827 - val_loss: 2.0855\n",
      "Epoch 389/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.0808 - val_loss: 2.0791\n",
      "Epoch 390/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.0680 - val_loss: 2.0709\n",
      "Epoch 391/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.0609 - val_loss: 2.0653\n",
      "Epoch 392/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.0534 - val_loss: 2.0583\n",
      "Epoch 393/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.0540 - val_loss: 2.0533\n",
      "Epoch 394/1000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.0386 - val_loss: 2.0458\n",
      "Epoch 395/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 2.0385 - val_loss: 2.0389\n",
      "Epoch 396/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.0309 - val_loss: 2.0329\n",
      "Epoch 397/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 2.0218 - val_loss: 2.0288\n",
      "Epoch 398/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.0187 - val_loss: 2.0213\n",
      "Epoch 399/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.0102 - val_loss: 2.0137\n",
      "Epoch 400/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.0042 - val_loss: 2.0061\n",
      "Epoch 401/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.9958 - val_loss: 1.9997\n",
      "Epoch 402/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.9934 - val_loss: 1.9941\n",
      "Epoch 403/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.9877 - val_loss: 1.9874\n",
      "Epoch 404/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.9770 - val_loss: 1.9805\n",
      "Epoch 405/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.9710 - val_loss: 1.9742\n",
      "Epoch 406/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.9718 - val_loss: 1.9689\n",
      "Epoch 407/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.9599 - val_loss: 1.9614\n",
      "Epoch 408/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.9562 - val_loss: 1.9572\n",
      "Epoch 409/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.9462 - val_loss: 1.9517\n",
      "Epoch 410/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.9431 - val_loss: 1.9444\n",
      "Epoch 411/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.9361 - val_loss: 1.9412\n",
      "Epoch 412/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.9302 - val_loss: 1.9360\n",
      "Epoch 413/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.9249 - val_loss: 1.9260\n",
      "Epoch 414/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.9186 - val_loss: 1.9199\n",
      "Epoch 415/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.9089 - val_loss: 1.9182\n",
      "Epoch 416/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.9055 - val_loss: 1.9109\n",
      "Epoch 417/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.8994 - val_loss: 1.9019\n",
      "Epoch 418/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.8940 - val_loss: 1.8972\n",
      "Epoch 419/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.8904 - val_loss: 1.8907\n",
      "Epoch 420/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.8812 - val_loss: 1.8835\n",
      "Epoch 421/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.8809 - val_loss: 1.8796\n",
      "Epoch 422/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.8749 - val_loss: 1.8769\n",
      "Epoch 423/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.8694 - val_loss: 1.8701\n",
      "Epoch 424/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.8636 - val_loss: 1.8641\n",
      "Epoch 425/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.8530 - val_loss: 1.8585\n",
      "Epoch 426/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.8558 - val_loss: 1.8530\n",
      "Epoch 427/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.8445 - val_loss: 1.8448\n",
      "Epoch 428/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.8442 - val_loss: 1.8413\n",
      "Epoch 429/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.8361 - val_loss: 1.8364\n",
      "Epoch 430/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.8268 - val_loss: 1.8295\n",
      "Epoch 431/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.8212 - val_loss: 1.8223\n",
      "Epoch 432/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.8162 - val_loss: 1.8202\n",
      "Epoch 433/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.8152 - val_loss: 1.8144\n",
      "Epoch 434/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.8077 - val_loss: 1.8108\n",
      "Epoch 435/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.8010 - val_loss: 1.8034\n",
      "Epoch 436/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.7952 - val_loss: 1.7991\n",
      "Epoch 437/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.7914 - val_loss: 1.7954\n",
      "Epoch 438/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.7878 - val_loss: 1.7937\n",
      "Epoch 439/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.7853 - val_loss: 1.7873\n",
      "Epoch 440/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.7757 - val_loss: 1.7816\n",
      "Epoch 441/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.7698 - val_loss: 1.7736\n",
      "Epoch 442/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.7685 - val_loss: 1.7676\n",
      "Epoch 443/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.7625 - val_loss: 1.7657\n",
      "Epoch 444/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.7575 - val_loss: 1.7618\n",
      "Epoch 445/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.7573 - val_loss: 1.7566\n",
      "Epoch 446/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.7433 - val_loss: 1.7514\n",
      "Epoch 447/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.7470 - val_loss: 1.7440\n",
      "Epoch 448/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.7349 - val_loss: 1.7401\n",
      "Epoch 449/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.7350 - val_loss: 1.7350\n",
      "Epoch 450/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.7249 - val_loss: 1.7307\n",
      "Epoch 451/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.7238 - val_loss: 1.7258\n",
      "Epoch 452/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.7149 - val_loss: 1.7214\n",
      "Epoch 453/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.7163 - val_loss: 1.7159\n",
      "Epoch 454/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.7085 - val_loss: 1.7127\n",
      "Epoch 455/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.7058 - val_loss: 1.7081\n",
      "Epoch 456/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.7021 - val_loss: 1.7039\n",
      "Epoch 457/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.6953 - val_loss: 1.6991\n",
      "Epoch 458/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.6933 - val_loss: 1.6955\n",
      "Epoch 459/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.6840 - val_loss: 1.6904\n",
      "Epoch 460/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.6812 - val_loss: 1.6880\n",
      "Epoch 461/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.6770 - val_loss: 1.6821\n",
      "Epoch 462/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.6714 - val_loss: 1.6770\n",
      "Epoch 463/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.6680 - val_loss: 1.6716\n",
      "Epoch 464/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.6653 - val_loss: 1.6687\n",
      "Epoch 465/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.6582 - val_loss: 1.6637\n",
      "Epoch 466/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.6519 - val_loss: 1.6579\n",
      "Epoch 467/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.6519 - val_loss: 1.6529\n",
      "Epoch 468/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.6451 - val_loss: 1.6504\n",
      "Epoch 469/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.6407 - val_loss: 1.6458\n",
      "Epoch 470/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.6354 - val_loss: 1.6415\n",
      "Epoch 471/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.6323 - val_loss: 1.6378\n",
      "Epoch 472/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.6338 - val_loss: 1.6363\n",
      "Epoch 473/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.6252 - val_loss: 1.6286\n",
      "Epoch 474/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.6239 - val_loss: 1.6245\n",
      "Epoch 475/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.6208 - val_loss: 1.6205\n",
      "Epoch 476/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.6129 - val_loss: 1.6171\n",
      "Epoch 477/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.6099 - val_loss: 1.6129\n",
      "Epoch 478/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.6075 - val_loss: 1.6089\n",
      "Epoch 479/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.6017 - val_loss: 1.6064\n",
      "Epoch 480/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.5996 - val_loss: 1.6014\n",
      "Epoch 481/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.5934 - val_loss: 1.5973\n",
      "Epoch 482/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.5927 - val_loss: 1.5944\n",
      "Epoch 483/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.5818 - val_loss: 1.5895\n",
      "Epoch 484/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.5850 - val_loss: 1.5864\n",
      "Epoch 485/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.5778 - val_loss: 1.5831\n",
      "Epoch 486/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.5738 - val_loss: 1.5810\n",
      "Epoch 487/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.5716 - val_loss: 1.5740\n",
      "Epoch 488/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.5670 - val_loss: 1.5701\n",
      "Epoch 489/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.5631 - val_loss: 1.5673\n",
      "Epoch 490/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.5601 - val_loss: 1.5622\n",
      "Epoch 491/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.5544 - val_loss: 1.5600\n",
      "Epoch 492/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.5493 - val_loss: 1.5574\n",
      "Epoch 493/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.5461 - val_loss: 1.5534\n",
      "Epoch 494/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.5461 - val_loss: 1.5479\n",
      "Epoch 495/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.5415 - val_loss: 1.5432\n",
      "Epoch 496/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.5396 - val_loss: 1.5405\n",
      "Epoch 497/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.5342 - val_loss: 1.5365\n",
      "Epoch 498/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.5303 - val_loss: 1.5331\n",
      "Epoch 499/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.5278 - val_loss: 1.5287\n",
      "Epoch 500/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.5276 - val_loss: 1.5272\n",
      "Epoch 501/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.5181 - val_loss: 1.5229\n",
      "Epoch 502/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.5183 - val_loss: 1.5202\n",
      "Epoch 503/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.5137 - val_loss: 1.5145\n",
      "Epoch 504/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.5068 - val_loss: 1.5135\n",
      "Epoch 505/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.5073 - val_loss: 1.5098\n",
      "Epoch 506/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.5004 - val_loss: 1.5058\n",
      "Epoch 507/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.4995 - val_loss: 1.5018\n",
      "Epoch 508/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.4949 - val_loss: 1.4982\n",
      "Epoch 509/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.4890 - val_loss: 1.4948\n",
      "Epoch 510/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.4888 - val_loss: 1.4915\n",
      "Epoch 511/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.4870 - val_loss: 1.4894\n",
      "Epoch 512/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.4822 - val_loss: 1.4880\n",
      "Epoch 513/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.4817 - val_loss: 1.4851\n",
      "Epoch 514/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.4770 - val_loss: 1.4788\n",
      "Epoch 515/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.4726 - val_loss: 1.4759\n",
      "Epoch 516/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.4726 - val_loss: 1.4733\n",
      "Epoch 517/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.4644 - val_loss: 1.4733\n",
      "Epoch 518/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.4613 - val_loss: 1.4687\n",
      "Epoch 519/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.4606 - val_loss: 1.4636\n",
      "Epoch 520/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.4570 - val_loss: 1.4598\n",
      "Epoch 521/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.4547 - val_loss: 1.4571\n",
      "Epoch 522/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.4507 - val_loss: 1.4538\n",
      "Epoch 523/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.4484 - val_loss: 1.4499\n",
      "Epoch 524/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.4427 - val_loss: 1.4464\n",
      "Epoch 525/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.4373 - val_loss: 1.4436\n",
      "Epoch 526/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.4350 - val_loss: 1.4390\n",
      "Epoch 527/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.4355 - val_loss: 1.4376\n",
      "Epoch 528/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.4300 - val_loss: 1.4338\n",
      "Epoch 529/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.4265 - val_loss: 1.4301\n",
      "Epoch 530/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4287\n",
      "Epoch 531/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.4227 - val_loss: 1.4265\n",
      "Epoch 532/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.4176 - val_loss: 1.4231\n",
      "Epoch 533/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.4162 - val_loss: 1.4205\n",
      "Epoch 534/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.4161 - val_loss: 1.4209\n",
      "Epoch 535/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.4122 - val_loss: 1.4176\n",
      "Epoch 536/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.4083 - val_loss: 1.4126\n",
      "Epoch 537/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.4082 - val_loss: 1.4074\n",
      "Epoch 538/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.4012 - val_loss: 1.4055\n",
      "Epoch 539/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3995 - val_loss: 1.4012\n",
      "Epoch 540/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.3984 - val_loss: 1.3991\n",
      "Epoch 541/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.3963 - val_loss: 1.3981\n",
      "Epoch 542/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3923 - val_loss: 1.3953\n",
      "Epoch 543/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.3892 - val_loss: 1.3910\n",
      "Epoch 544/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.3837 - val_loss: 1.3880\n",
      "Epoch 545/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.3763 - val_loss: 1.3863\n",
      "Epoch 546/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.3800 - val_loss: 1.3851\n",
      "Epoch 547/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.3766 - val_loss: 1.3806\n",
      "Epoch 548/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3752 - val_loss: 1.3801\n",
      "Epoch 549/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.3687 - val_loss: 1.3744\n",
      "Epoch 550/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.3636 - val_loss: 1.3726\n",
      "Epoch 551/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3671 - val_loss: 1.3695\n",
      "Epoch 552/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3625 - val_loss: 1.3677\n",
      "Epoch 553/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.3599 - val_loss: 1.3638\n",
      "Epoch 554/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.3554 - val_loss: 1.3592\n",
      "Epoch 555/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.3538 - val_loss: 1.3576\n",
      "Epoch 556/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.3523 - val_loss: 1.3569\n",
      "Epoch 557/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3493 - val_loss: 1.3535\n",
      "Epoch 558/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.3483 - val_loss: 1.3527\n",
      "Epoch 559/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3441 - val_loss: 1.3483\n",
      "Epoch 560/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3428 - val_loss: 1.3444\n",
      "Epoch 561/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.3400 - val_loss: 1.3429\n",
      "Epoch 562/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.3329 - val_loss: 1.3404\n",
      "Epoch 563/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.3339 - val_loss: 1.3386\n",
      "Epoch 564/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.3330 - val_loss: 1.3365\n",
      "Epoch 565/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.3285 - val_loss: 1.3336\n",
      "Epoch 566/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3241 - val_loss: 1.3306\n",
      "Epoch 567/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3226 - val_loss: 1.3292\n",
      "Epoch 568/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3201 - val_loss: 1.3254\n",
      "Epoch 569/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3191 - val_loss: 1.3224\n",
      "Epoch 570/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.3126 - val_loss: 1.3221\n",
      "Epoch 571/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.3142 - val_loss: 1.3180\n",
      "Epoch 572/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.3092 - val_loss: 1.3151\n",
      "Epoch 573/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.3103 - val_loss: 1.3101\n",
      "Epoch 574/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.3076 - val_loss: 1.3088\n",
      "Epoch 575/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3015 - val_loss: 1.3078\n",
      "Epoch 576/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.2968 - val_loss: 1.3043\n",
      "Epoch 577/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3015 - val_loss: 1.3028\n",
      "Epoch 578/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2947 - val_loss: 1.3032\n",
      "Epoch 579/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.2963 - val_loss: 1.3010\n",
      "Epoch 580/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.2908 - val_loss: 1.2971\n",
      "Epoch 581/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.2898 - val_loss: 1.2938\n",
      "Epoch 582/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.2874 - val_loss: 1.2913\n",
      "Epoch 583/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.2835 - val_loss: 1.2895\n",
      "Epoch 584/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.2794 - val_loss: 1.2862\n",
      "Epoch 585/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.2822 - val_loss: 1.2840\n",
      "Epoch 586/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.2782 - val_loss: 1.2832\n",
      "Epoch 587/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.2763 - val_loss: 1.2817\n",
      "Epoch 588/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.2733 - val_loss: 1.2808\n",
      "Epoch 589/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.2726 - val_loss: 1.2756\n",
      "Epoch 590/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.2654 - val_loss: 1.2739\n",
      "Epoch 591/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.2674 - val_loss: 1.2702\n",
      "Epoch 592/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.2647 - val_loss: 1.2688\n",
      "Epoch 593/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.2628 - val_loss: 1.2664\n",
      "Epoch 594/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.2585 - val_loss: 1.2607\n",
      "Epoch 595/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.2526 - val_loss: 1.2604\n",
      "Epoch 596/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.2512 - val_loss: 1.2592\n",
      "Epoch 597/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.2533 - val_loss: 1.2557\n",
      "Epoch 598/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.2514 - val_loss: 1.2544\n",
      "Epoch 599/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.2489 - val_loss: 1.2518\n",
      "Epoch 600/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.2474 - val_loss: 1.2513\n",
      "Epoch 601/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.2410 - val_loss: 1.2491\n",
      "Epoch 602/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.2407 - val_loss: 1.2469\n",
      "Epoch 603/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.2378 - val_loss: 1.2445\n",
      "Epoch 604/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.2377 - val_loss: 1.2413\n",
      "Epoch 605/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.2331 - val_loss: 1.2392\n",
      "Epoch 606/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.2347 - val_loss: 1.2359\n",
      "Epoch 607/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.2306 - val_loss: 1.2352\n",
      "Epoch 608/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.2305 - val_loss: 1.2328\n",
      "Epoch 609/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.2287 - val_loss: 1.2317\n",
      "Epoch 610/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.2236 - val_loss: 1.2274\n",
      "Epoch 611/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.2243 - val_loss: 1.2251\n",
      "Epoch 612/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.2194 - val_loss: 1.2220\n",
      "Epoch 613/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.2142 - val_loss: 1.2213\n",
      "Epoch 614/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.2132 - val_loss: 1.2215\n",
      "Epoch 615/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.2107 - val_loss: 1.2182\n",
      "Epoch 616/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.2120 - val_loss: 1.2193\n",
      "Epoch 617/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.2089 - val_loss: 1.2188\n",
      "Epoch 618/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.2094 - val_loss: 1.2122\n",
      "Epoch 619/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.2042 - val_loss: 1.2102\n",
      "Epoch 620/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.2038 - val_loss: 1.2089\n",
      "Epoch 621/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.2010 - val_loss: 1.2054\n",
      "Epoch 622/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.2019 - val_loss: 1.2047\n",
      "Epoch 623/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.1948 - val_loss: 1.2006\n",
      "Epoch 624/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1944 - val_loss: 1.1987\n",
      "Epoch 625/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.1946 - val_loss: 1.1984\n",
      "Epoch 626/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1868 - val_loss: 1.1954\n",
      "Epoch 627/1000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.1897 - val_loss: 1.1925\n",
      "Epoch 628/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.1876 - val_loss: 1.1924\n",
      "Epoch 629/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1848 - val_loss: 1.1899\n",
      "Epoch 630/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1839 - val_loss: 1.1874\n",
      "Epoch 631/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.1802 - val_loss: 1.1868\n",
      "Epoch 632/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1828 - val_loss: 1.1853\n",
      "Epoch 633/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.1744 - val_loss: 1.1832\n",
      "Epoch 634/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1765 - val_loss: 1.1808\n",
      "Epoch 635/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.1748 - val_loss: 1.1805\n",
      "Epoch 636/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1719 - val_loss: 1.1771\n",
      "Epoch 637/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1731 - val_loss: 1.1770\n",
      "Epoch 638/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1682 - val_loss: 1.1741\n",
      "Epoch 639/1000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.1649 - val_loss: 1.1734\n",
      "Epoch 640/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1695 - val_loss: 1.1710\n",
      "Epoch 641/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1640 - val_loss: 1.1682\n",
      "Epoch 642/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1627 - val_loss: 1.1649\n",
      "Epoch 643/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1589 - val_loss: 1.1645\n",
      "Epoch 644/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.1562 - val_loss: 1.1620\n",
      "Epoch 645/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1587 - val_loss: 1.1613\n",
      "Epoch 646/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.1562 - val_loss: 1.1573\n",
      "Epoch 647/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1524 - val_loss: 1.1562\n",
      "Epoch 648/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.1481 - val_loss: 1.1552\n",
      "Epoch 649/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1479 - val_loss: 1.1533\n",
      "Epoch 650/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1470 - val_loss: 1.1502\n",
      "Epoch 651/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.1475 - val_loss: 1.1488\n",
      "Epoch 652/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1461 - val_loss: 1.1464\n",
      "Epoch 653/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.1430 - val_loss: 1.1469\n",
      "Epoch 654/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.1400 - val_loss: 1.1433\n",
      "Epoch 655/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1375 - val_loss: 1.1400\n",
      "Epoch 656/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1344 - val_loss: 1.1384\n",
      "Epoch 657/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.1353 - val_loss: 1.1400\n",
      "Epoch 658/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1326 - val_loss: 1.1411\n",
      "Epoch 659/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1301 - val_loss: 1.1356\n",
      "Epoch 660/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.1316 - val_loss: 1.1338\n",
      "Epoch 661/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.1271 - val_loss: 1.1326\n",
      "Epoch 662/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1226 - val_loss: 1.1322\n",
      "Epoch 663/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1255 - val_loss: 1.1295\n",
      "Epoch 664/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1214 - val_loss: 1.1258\n",
      "Epoch 665/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.1179 - val_loss: 1.1260\n",
      "Epoch 666/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.1197 - val_loss: 1.1251\n",
      "Epoch 667/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.1193 - val_loss: 1.1229\n",
      "Epoch 668/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1124 - val_loss: 1.1217\n",
      "Epoch 669/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.1122 - val_loss: 1.1182\n",
      "Epoch 670/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1107 - val_loss: 1.1153\n",
      "Epoch 671/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1136 - val_loss: 1.1153\n",
      "Epoch 672/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1102 - val_loss: 1.1118\n",
      "Epoch 673/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.1060 - val_loss: 1.1120\n",
      "Epoch 674/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1043 - val_loss: 1.1105\n",
      "Epoch 675/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.1035 - val_loss: 1.1070\n",
      "Epoch 676/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1028 - val_loss: 1.1051\n",
      "Epoch 677/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.1018 - val_loss: 1.1051\n",
      "Epoch 678/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.1014 - val_loss: 1.1025\n",
      "Epoch 679/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0981 - val_loss: 1.1005\n",
      "Epoch 680/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.1000 - val_loss: 1.0999\n",
      "Epoch 681/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0905 - val_loss: 1.0980\n",
      "Epoch 682/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0874 - val_loss: 1.0960\n",
      "Epoch 683/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0903 - val_loss: 1.0965\n",
      "Epoch 684/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.0889 - val_loss: 1.0923\n",
      "Epoch 685/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0891 - val_loss: 1.0920\n",
      "Epoch 686/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0884 - val_loss: 1.0879\n",
      "Epoch 687/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.0826 - val_loss: 1.0875\n",
      "Epoch 688/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0825 - val_loss: 1.0867\n",
      "Epoch 689/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.0779 - val_loss: 1.0846\n",
      "Epoch 690/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0816 - val_loss: 1.0850\n",
      "Epoch 691/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0759 - val_loss: 1.0818\n",
      "Epoch 692/1000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.0760 - val_loss: 1.0813\n",
      "Epoch 693/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0740 - val_loss: 1.0811\n",
      "Epoch 694/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.0738 - val_loss: 1.0776\n",
      "Epoch 695/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0718 - val_loss: 1.0758\n",
      "Epoch 696/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0707 - val_loss: 1.0727\n",
      "Epoch 697/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0705 - val_loss: 1.0725\n",
      "Epoch 698/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.0694 - val_loss: 1.0710\n",
      "Epoch 699/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.0662 - val_loss: 1.0694\n",
      "Epoch 700/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0669 - val_loss: 1.0688\n",
      "Epoch 701/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0616 - val_loss: 1.0669\n",
      "Epoch 702/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0607 - val_loss: 1.0654\n",
      "Epoch 703/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0602 - val_loss: 1.0641\n",
      "Epoch 704/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0583 - val_loss: 1.0623\n",
      "Epoch 705/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.0550 - val_loss: 1.0607\n",
      "Epoch 706/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0574 - val_loss: 1.0600\n",
      "Epoch 707/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.0538 - val_loss: 1.0559\n",
      "Epoch 708/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.0521 - val_loss: 1.0567\n",
      "Epoch 709/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.0499 - val_loss: 1.0545\n",
      "Epoch 710/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0489 - val_loss: 1.0553\n",
      "Epoch 711/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0505 - val_loss: 1.0527\n",
      "Epoch 712/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0469 - val_loss: 1.0509\n",
      "Epoch 713/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0444 - val_loss: 1.0499\n",
      "Epoch 714/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.0459 - val_loss: 1.0478\n",
      "Epoch 715/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.0421 - val_loss: 1.0457\n",
      "Epoch 716/1000\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 1.0428 - val_loss: 1.0452\n",
      "Epoch 717/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0335 - val_loss: 1.0430\n",
      "Epoch 718/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0347 - val_loss: 1.0425\n",
      "Epoch 719/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0368 - val_loss: 1.0410\n",
      "Epoch 720/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0349 - val_loss: 1.0384\n",
      "Epoch 721/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0343 - val_loss: 1.0379\n",
      "Epoch 722/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.0363 - val_loss: 1.0378\n",
      "Epoch 723/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0339 - val_loss: 1.0368\n",
      "Epoch 724/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.0256 - val_loss: 1.0348\n",
      "Epoch 725/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.0293 - val_loss: 1.0318\n",
      "Epoch 726/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0264 - val_loss: 1.0327\n",
      "Epoch 727/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0260 - val_loss: 1.0305\n",
      "Epoch 728/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.0237 - val_loss: 1.0316\n",
      "Epoch 729/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0246 - val_loss: 1.0288\n",
      "Epoch 730/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0185 - val_loss: 1.0253\n",
      "Epoch 731/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0206 - val_loss: 1.0250\n",
      "Epoch 732/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0177 - val_loss: 1.0229\n",
      "Epoch 733/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0173 - val_loss: 1.0223\n",
      "Epoch 734/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0135 - val_loss: 1.0193\n",
      "Epoch 735/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.0189 - val_loss: 1.0198\n",
      "Epoch 736/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0110 - val_loss: 1.0191\n",
      "Epoch 737/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0088 - val_loss: 1.0193\n",
      "Epoch 738/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0103 - val_loss: 1.0142\n",
      "Epoch 739/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.0072 - val_loss: 1.0120\n",
      "Epoch 740/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0078 - val_loss: 1.0122\n",
      "Epoch 741/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0040 - val_loss: 1.0102\n",
      "Epoch 742/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0068 - val_loss: 1.0088\n",
      "Epoch 743/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0007 - val_loss: 1.0073\n",
      "Epoch 744/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0014 - val_loss: 1.0062\n",
      "Epoch 745/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.0016 - val_loss: 1.0068\n",
      "Epoch 746/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0046 - val_loss: 1.0039\n",
      "Epoch 747/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.0013 - val_loss: 1.0041\n",
      "Epoch 748/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.0001 - val_loss: 1.0031\n",
      "Epoch 749/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9970 - val_loss: 1.0032\n",
      "Epoch 750/1000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.9955 - val_loss: 1.0015\n",
      "Epoch 751/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9945 - val_loss: 0.9986\n",
      "Epoch 752/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9918 - val_loss: 0.9974\n",
      "Epoch 753/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9902 - val_loss: 0.9968\n",
      "Epoch 754/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9924 - val_loss: 0.9945\n",
      "Epoch 755/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9877 - val_loss: 0.9916\n",
      "Epoch 756/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9867 - val_loss: 0.9932\n",
      "Epoch 757/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9881 - val_loss: 0.9912\n",
      "Epoch 758/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9870 - val_loss: 0.9892\n",
      "Epoch 759/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9845 - val_loss: 0.9856\n",
      "Epoch 760/1000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.9819 - val_loss: 0.9858\n",
      "Epoch 761/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9822 - val_loss: 0.9865\n",
      "Epoch 762/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9822 - val_loss: 0.9834\n",
      "Epoch 763/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9747 - val_loss: 0.9818\n",
      "Epoch 764/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9827 - val_loss: 0.9816\n",
      "Epoch 765/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9779 - val_loss: 0.9806\n",
      "Epoch 766/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9736 - val_loss: 0.9794\n",
      "Epoch 767/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9755 - val_loss: 0.9781\n",
      "Epoch 768/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9774 - val_loss: 0.9782\n",
      "Epoch 769/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9736 - val_loss: 0.9782\n",
      "Epoch 770/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9710 - val_loss: 0.9762\n",
      "Epoch 771/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9687 - val_loss: 0.9742\n",
      "Epoch 772/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9667 - val_loss: 0.9747\n",
      "Epoch 773/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9636 - val_loss: 0.9706\n",
      "Epoch 774/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9602 - val_loss: 0.9703\n",
      "Epoch 775/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9658 - val_loss: 0.9705\n",
      "Epoch 776/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9636 - val_loss: 0.9671\n",
      "Epoch 777/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9607 - val_loss: 0.9660\n",
      "Epoch 778/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9589 - val_loss: 0.9640\n",
      "Epoch 779/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9571 - val_loss: 0.9652\n",
      "Epoch 780/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9587 - val_loss: 0.9630\n",
      "Epoch 781/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9598 - val_loss: 0.9616\n",
      "Epoch 782/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9603 - val_loss: 0.9616\n",
      "Epoch 783/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9546 - val_loss: 0.9588\n",
      "Epoch 784/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9572 - val_loss: 0.9588\n",
      "Epoch 785/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9542 - val_loss: 0.9562\n",
      "Epoch 786/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9539 - val_loss: 0.9569\n",
      "Epoch 787/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9475 - val_loss: 0.9533\n",
      "Epoch 788/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9515 - val_loss: 0.9525\n",
      "Epoch 789/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9509 - val_loss: 0.9510\n",
      "Epoch 790/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9481 - val_loss: 0.9500\n",
      "Epoch 791/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9527 - val_loss: 0.9503\n",
      "Epoch 792/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9460 - val_loss: 0.9489\n",
      "Epoch 793/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9454 - val_loss: 0.9485\n",
      "Epoch 794/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9448 - val_loss: 0.9479\n",
      "Epoch 795/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9468 - val_loss: 0.9474\n",
      "Epoch 796/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9432 - val_loss: 0.9446\n",
      "Epoch 797/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9442 - val_loss: 0.9460\n",
      "Epoch 798/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9412 - val_loss: 0.9449\n",
      "Epoch 799/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9403 - val_loss: 0.9414\n",
      "Epoch 800/1000\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.9398 - val_loss: 0.9407\n",
      "Epoch 801/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9412 - val_loss: 0.9396\n",
      "Epoch 802/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9339 - val_loss: 0.9398\n",
      "Epoch 803/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9335 - val_loss: 0.9391\n",
      "Epoch 804/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9378 - val_loss: 0.9379\n",
      "Epoch 805/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9337 - val_loss: 0.9384\n",
      "Epoch 806/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9323 - val_loss: 0.9368\n",
      "Epoch 807/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9288 - val_loss: 0.9346\n",
      "Epoch 808/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9290 - val_loss: 0.9344\n",
      "Epoch 809/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9293 - val_loss: 0.9323\n",
      "Epoch 810/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.9289 - val_loss: 0.9318\n",
      "Epoch 811/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9303 - val_loss: 0.9296\n",
      "Epoch 812/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9231 - val_loss: 0.9276\n",
      "Epoch 813/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9269 - val_loss: 0.9284\n",
      "Epoch 814/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9218 - val_loss: 0.9277\n",
      "Epoch 815/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9231 - val_loss: 0.9248\n",
      "Epoch 816/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9219 - val_loss: 0.9233\n",
      "Epoch 817/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9185 - val_loss: 0.9240\n",
      "Epoch 818/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9193 - val_loss: 0.9234\n",
      "Epoch 819/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9188 - val_loss: 0.9228\n",
      "Epoch 820/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9181 - val_loss: 0.9216\n",
      "Epoch 821/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9198 - val_loss: 0.9220\n",
      "Epoch 822/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9174 - val_loss: 0.9226\n",
      "Epoch 823/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.9119 - val_loss: 0.9210\n",
      "Epoch 824/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9133 - val_loss: 0.9198\n",
      "Epoch 825/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9140 - val_loss: 0.9170\n",
      "Epoch 826/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9135 - val_loss: 0.9176\n",
      "Epoch 827/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9160 - val_loss: 0.9184\n",
      "Epoch 828/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9126 - val_loss: 0.9151\n",
      "Epoch 829/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.9097 - val_loss: 0.9136\n",
      "Epoch 830/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9078 - val_loss: 0.9116\n",
      "Epoch 831/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9085 - val_loss: 0.9106\n",
      "Epoch 832/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9054 - val_loss: 0.9096\n",
      "Epoch 833/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9047 - val_loss: 0.9086\n",
      "Epoch 834/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9046 - val_loss: 0.9065\n",
      "Epoch 835/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.9065 - val_loss: 0.9059\n",
      "Epoch 836/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.9034 - val_loss: 0.9083\n",
      "Epoch 837/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9022 - val_loss: 0.9042\n",
      "Epoch 838/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9030 - val_loss: 0.9042\n",
      "Epoch 839/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9069 - val_loss: 0.9048\n",
      "Epoch 840/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9004 - val_loss: 0.9042\n",
      "Epoch 841/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8986 - val_loss: 0.9009\n",
      "Epoch 842/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8998 - val_loss: 0.9026\n",
      "Epoch 843/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8955 - val_loss: 0.8998\n",
      "Epoch 844/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8967 - val_loss: 0.8993\n",
      "Epoch 845/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8952 - val_loss: 0.8973\n",
      "Epoch 846/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8938 - val_loss: 0.8976\n",
      "Epoch 847/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8960 - val_loss: 0.8955\n",
      "Epoch 848/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8931 - val_loss: 0.8951\n",
      "Epoch 849/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8905 - val_loss: 0.8960\n",
      "Epoch 850/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8914 - val_loss: 0.8942\n",
      "Epoch 851/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8929 - val_loss: 0.8953\n",
      "Epoch 852/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8851 - val_loss: 0.8921\n",
      "Epoch 853/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8888 - val_loss: 0.8930\n",
      "Epoch 854/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8866 - val_loss: 0.8913\n",
      "Epoch 855/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8808 - val_loss: 0.8896\n",
      "Epoch 856/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8855 - val_loss: 0.8888\n",
      "Epoch 857/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8871 - val_loss: 0.8876\n",
      "Epoch 858/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8835 - val_loss: 0.8879\n",
      "Epoch 859/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8833 - val_loss: 0.8851\n",
      "Epoch 860/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8800 - val_loss: 0.8860\n",
      "Epoch 861/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8789 - val_loss: 0.8858\n",
      "Epoch 862/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8766 - val_loss: 0.8825\n",
      "Epoch 863/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8792 - val_loss: 0.8839\n",
      "Epoch 864/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8817 - val_loss: 0.8821\n",
      "Epoch 865/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8772 - val_loss: 0.8825\n",
      "Epoch 866/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8779 - val_loss: 0.8810\n",
      "Epoch 867/1000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8806 - val_loss: 0.8791\n",
      "Epoch 868/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8774 - val_loss: 0.8786\n",
      "Epoch 869/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8776 - val_loss: 0.8774\n",
      "Epoch 870/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8757 - val_loss: 0.8770\n",
      "Epoch 871/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8759 - val_loss: 0.8766\n",
      "Epoch 872/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8720 - val_loss: 0.8756\n",
      "Epoch 873/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8734 - val_loss: 0.8763\n",
      "Epoch 874/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8688 - val_loss: 0.8741\n",
      "Epoch 875/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8739 - val_loss: 0.8740\n",
      "Epoch 876/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8711 - val_loss: 0.8718\n",
      "Epoch 877/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8689 - val_loss: 0.8711\n",
      "Epoch 878/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8695 - val_loss: 0.8725\n",
      "Epoch 879/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8706 - val_loss: 0.8721\n",
      "Epoch 880/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8664 - val_loss: 0.8715\n",
      "Epoch 881/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8652 - val_loss: 0.8715\n",
      "Epoch 882/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8645 - val_loss: 0.8698\n",
      "Epoch 883/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8627 - val_loss: 0.8682\n",
      "Epoch 884/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8642 - val_loss: 0.8685\n",
      "Epoch 885/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8615 - val_loss: 0.8670\n",
      "Epoch 886/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8625 - val_loss: 0.8663\n",
      "Epoch 887/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8641 - val_loss: 0.8653\n",
      "Epoch 888/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8627 - val_loss: 0.8658\n",
      "Epoch 889/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8584 - val_loss: 0.8656\n",
      "Epoch 890/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8612 - val_loss: 0.8624\n",
      "Epoch 891/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8593 - val_loss: 0.8626\n",
      "Epoch 892/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8605 - val_loss: 0.8605\n",
      "Epoch 893/1000\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.8609 - val_loss: 0.8594\n",
      "Epoch 894/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8601 - val_loss: 0.8589\n",
      "Epoch 895/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8591 - val_loss: 0.8593\n",
      "Epoch 896/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8554 - val_loss: 0.8584\n",
      "Epoch 897/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8523 - val_loss: 0.8574\n",
      "Epoch 898/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8525 - val_loss: 0.8555\n",
      "Epoch 899/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8515 - val_loss: 0.8551\n",
      "Epoch 900/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8511 - val_loss: 0.8543\n",
      "Epoch 901/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8531 - val_loss: 0.8533\n",
      "Epoch 902/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8511 - val_loss: 0.8536\n",
      "Epoch 903/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8483 - val_loss: 0.8550\n",
      "Epoch 904/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8473 - val_loss: 0.8540\n",
      "Epoch 905/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8472 - val_loss: 0.8514\n",
      "Epoch 906/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8461 - val_loss: 0.8511\n",
      "Epoch 907/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8472 - val_loss: 0.8503\n",
      "Epoch 908/1000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.8456 - val_loss: 0.8493\n",
      "Epoch 909/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8475 - val_loss: 0.8500\n",
      "Epoch 910/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8443 - val_loss: 0.8482\n",
      "Epoch 911/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8424 - val_loss: 0.8478\n",
      "Epoch 912/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8437 - val_loss: 0.8455\n",
      "Epoch 913/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8435 - val_loss: 0.8442\n",
      "Epoch 914/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8414 - val_loss: 0.8455\n",
      "Epoch 915/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8405 - val_loss: 0.8446\n",
      "Epoch 916/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8411 - val_loss: 0.8449\n",
      "Epoch 917/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8405 - val_loss: 0.8449\n",
      "Epoch 918/1000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.8406 - val_loss: 0.8437\n",
      "Epoch 919/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8388 - val_loss: 0.8428\n",
      "Epoch 920/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8365 - val_loss: 0.8433\n",
      "Epoch 921/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8384 - val_loss: 0.8400\n",
      "Epoch 922/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8330 - val_loss: 0.8396\n",
      "Epoch 923/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8387 - val_loss: 0.8397\n",
      "Epoch 924/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8361 - val_loss: 0.8385\n",
      "Epoch 925/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8342 - val_loss: 0.8370\n",
      "Epoch 926/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8339 - val_loss: 0.8372\n",
      "Epoch 927/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8300 - val_loss: 0.8362\n",
      "Epoch 928/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8337 - val_loss: 0.8352\n",
      "Epoch 929/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8318 - val_loss: 0.8352\n",
      "Epoch 930/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8315 - val_loss: 0.8351\n",
      "Epoch 931/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8341 - val_loss: 0.8341\n",
      "Epoch 932/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8304 - val_loss: 0.8337\n",
      "Epoch 933/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8301 - val_loss: 0.8333\n",
      "Epoch 934/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8296 - val_loss: 0.8325\n",
      "Epoch 935/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8329 - val_loss: 0.8316\n",
      "Epoch 936/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8260 - val_loss: 0.8315\n",
      "Epoch 937/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8254 - val_loss: 0.8318\n",
      "Epoch 938/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8290 - val_loss: 0.8303\n",
      "Epoch 939/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8265 - val_loss: 0.8297\n",
      "Epoch 940/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8244 - val_loss: 0.8295\n",
      "Epoch 941/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8268 - val_loss: 0.8293\n",
      "Epoch 942/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8270 - val_loss: 0.8283\n",
      "Epoch 943/1000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8251 - val_loss: 0.8268\n",
      "Epoch 944/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8241 - val_loss: 0.8274\n",
      "Epoch 945/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8220 - val_loss: 0.8264\n",
      "Epoch 946/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8242 - val_loss: 0.8257\n",
      "Epoch 947/1000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8238 - val_loss: 0.8261\n",
      "Epoch 948/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8242 - val_loss: 0.8264\n",
      "Epoch 949/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8228 - val_loss: 0.8257\n",
      "Epoch 950/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8234 - val_loss: 0.8240\n",
      "Epoch 951/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8217 - val_loss: 0.8238\n",
      "Epoch 952/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8181 - val_loss: 0.8221\n",
      "Epoch 953/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8204 - val_loss: 0.8215\n",
      "Epoch 954/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8158 - val_loss: 0.8210\n",
      "Epoch 955/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8125 - val_loss: 0.8200\n",
      "Epoch 956/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8133 - val_loss: 0.8205\n",
      "Epoch 957/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8137 - val_loss: 0.8201\n",
      "Epoch 958/1000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8179 - val_loss: 0.8195\n",
      "Epoch 959/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8193 - val_loss: 0.8181\n",
      "Epoch 960/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8193 - val_loss: 0.8182\n",
      "Epoch 961/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8155 - val_loss: 0.8169\n",
      "Epoch 962/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8119 - val_loss: 0.8175\n",
      "Epoch 963/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8104 - val_loss: 0.8165\n",
      "Epoch 964/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8127 - val_loss: 0.8166\n",
      "Epoch 965/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8132 - val_loss: 0.8164\n",
      "Epoch 966/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8091 - val_loss: 0.8156\n",
      "Epoch 967/1000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8099 - val_loss: 0.8151\n",
      "Epoch 968/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8095 - val_loss: 0.8142\n",
      "Epoch 969/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8103 - val_loss: 0.8124\n",
      "Epoch 970/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8105 - val_loss: 0.8121\n",
      "Epoch 971/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8084 - val_loss: 0.8121\n",
      "Epoch 972/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.8102 - val_loss: 0.8141\n",
      "Epoch 973/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8099 - val_loss: 0.8111\n",
      "Epoch 974/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8116 - val_loss: 0.8110\n",
      "Epoch 975/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8061 - val_loss: 0.8121\n",
      "Epoch 976/1000\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.8064 - val_loss: 0.8104\n",
      "Epoch 977/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8047 - val_loss: 0.8111\n",
      "Epoch 978/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.8022 - val_loss: 0.8109\n",
      "Epoch 979/1000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8050 - val_loss: 0.8095\n",
      "Epoch 980/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.8067 - val_loss: 0.8085\n",
      "Epoch 981/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8041 - val_loss: 0.8073\n",
      "Epoch 982/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8041 - val_loss: 0.8078\n",
      "Epoch 983/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.8037 - val_loss: 0.8075\n",
      "Epoch 984/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8024 - val_loss: 0.8061\n",
      "Epoch 985/1000\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.8010 - val_loss: 0.8062\n",
      "Epoch 986/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.7972 - val_loss: 0.8056\n",
      "Epoch 987/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.7979 - val_loss: 0.8049\n",
      "Epoch 988/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.7998 - val_loss: 0.8047\n",
      "Epoch 989/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8002 - val_loss: 0.8051\n",
      "Epoch 990/1000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8004 - val_loss: 0.8050\n",
      "Epoch 991/1000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.7982 - val_loss: 0.8051\n",
      "Epoch 992/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.7968 - val_loss: 0.8044\n",
      "Epoch 993/1000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.7952 - val_loss: 0.8023\n",
      "Epoch 994/1000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.8022 - val_loss: 0.8026\n",
      "Epoch 995/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.7953 - val_loss: 0.8022\n",
      "Epoch 996/1000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.7953 - val_loss: 0.8021\n",
      "Epoch 997/1000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7933 - val_loss: 0.8014\n",
      "Epoch 998/1000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.7892 - val_loss: 0.7997\n",
      "Epoch 999/1000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.7938 - val_loss: 0.7984\n",
      "Epoch 1000/1000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7923 - val_loss: 0.7983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7321414947509766"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelling\n",
    "# Create initial model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add just one layer for now\n",
    "model.add(Masking(mask_value=placeholder_value))\n",
    "model.add(Masking(mask_value=placeholder_value1))\n",
    "model.add(layers.LSTM(256, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(layers.LSTM(128, activation='tanh', return_sequences=False, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(layers.Dense(100, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(layers.Dense(80, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(layers.Dense(120, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(len(i_s) - 3, activation='linear')) # output layer has ~160 linear nodes\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.00001), \n",
    "              loss=keras.losses.MeanAbsoluteError())\n",
    "\n",
    "# Define an Early Stop callback\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience = 5, \n",
    "                                           verbose=1, restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    validation_split=0.3,\n",
    "                    batch_size=64, epochs=1000, verbose=1)\n",
    "\n",
    "model.evaluate(X_test, Y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -30\n",
    "player = players[i]\n",
    "print(player)\n",
    "hist, act = X_test[i], Y_test[i]\n",
    "hist = hist.reshape(1, 6, 163)\n",
    "pred = model.predict(hist)\n",
    "print(hist.shape)\n",
    "for feat_i in range(X_train.shape[2]):\n",
    "    print(f'  feature: {df.columns[i_s[feat_i + 3]]}')\n",
    "    print(f'    {hist[0, :, feat_i+3]}, -> {np.round(pred, 2)[0, feat_i]} vs {np.round(act[feat_i], 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_player(p_id: str, szn: str) -> str:\n",
    "    \"\"\"\n",
    "    a function That, given a players id, will return the player whose next season more resembles that player\n",
    "    pid: the player we most want to replicate\n",
    "    sn: the season of that player we most want to replicate. \n",
    "    \"\"\"\n",
    "    szn_hist = df.loc[(df['p_id'] == p_id) & (df['Season'] == szn)].reset_index()\n",
    "    pos1 = szn_hist['Pos'][0]\n",
    "    szn_hist = szn_hist.fillna(placeholder_value)\n",
    "    szn_hist = szn_hist.to_numpy()\n",
    "\n",
    "    total.sort_values(['p_id', 'Season'])\n",
    "    pids = total['p_id'].unique()  # Get a list of all the unique player ids\n",
    "\n",
    "    coss = []\n",
    "    mses = []\n",
    "\n",
    "    total = ss.fit_transform(total)\n",
    "\n",
    "    for player in pids:\n",
    "        p_df = total.loc[total['p_id'] == player]  # Get the historical data for this player. \n",
    "        # We need to pad the data\n",
    "        player_data = p_df\n",
    "        placeholder_value1 = -500\n",
    "        for season in seasons:\n",
    "            if season not in list(player_data['Season']):\n",
    "                # Add the season row with blanks\n",
    "                # print(f'{pid} needs {season}')\n",
    "                player_name = player_data['Player'].mode()[0]\n",
    "                nation = player_data['Nation'].mode()[0]\n",
    "                born = player_data['Born'].mode()[0]\n",
    "                position = player_data['Pos'].mode()[0]\n",
    "                squad = player_data['Squad'].mode()[0]\n",
    "                comp = player_data['Comp'].mode()[0]\n",
    "\n",
    "                age = int(season.split('-')[0]) - born              \n",
    "\n",
    "                new_row = [pid, season, player_name, nation, position, squad, comp, age, born] + [placeholder_value1] * 160\n",
    "                new_row_df = pd.DataFrame([new_row], columns=df.columns)\n",
    "                player_data = pd.concat([player_data, new_row_df], ignore_index=True)\n",
    "        # player_data.reset_index(inplace=True)\n",
    "        pos2 = player_data['Pos'].mode()[0]\n",
    "        # now we take the 6 most recent seasons as our model expects 6 seasons\n",
    "        player_data = player_data.fillna(placeholder_value)\n",
    "\n",
    "        player_data = player_data.to_numpy().reshape(1, 7, 169)\n",
    "        p_hist = player_data[:, -6:, i_s].astype(np.float32)\n",
    "        p_pred = model.predict(p_hist)\n",
    "        \n",
    "        pos_sim = float(((7 - abs(pos1 - pos2))/7))\n",
    "        print(f'pos1: {pos1}, pos2: {pos2}, pos_sim: {pos_sim}')\n",
    "        print(f'hist: {p_pred.shape}, szn: {szn_hist[:, i_s[3:]].shape}')\n",
    "        cos = (cosine_similarity(p_pred, szn_hist[:, i_s[3:]])) * (pos_sim)\n",
    "        mse = (mean_squared_error(p_pred, szn_hist[:, i_s[3:]], squared=True)) * (pos_sim)\n",
    "        coss.append(cos)\n",
    "        mses.append(mse)\n",
    "    print(f'Most similar by cosine similariy: {pids[coss.index(max(coss))]} with {max(coss)}')\n",
    "    print(f'Most similar by MSE: {pids[mses.index(max(mses))]} with {max(mse)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'total' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfind_sim_player\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMohamed Salah-EGY-1992\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2017-2018\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[166], line 12\u001b[0m, in \u001b[0;36mfind_sim_player\u001b[1;34m(p_id, szn)\u001b[0m\n\u001b[0;32m      9\u001b[0m szn_hist \u001b[38;5;241m=\u001b[39m szn_hist\u001b[38;5;241m.\u001b[39mfillna(placeholder_value)\n\u001b[0;32m     10\u001b[0m szn_hist \u001b[38;5;241m=\u001b[39m szn_hist\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtotal\u001b[49m\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeason\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m pids \u001b[38;5;241m=\u001b[39m total[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()  \u001b[38;5;66;03m# Get a list of all the unique player ids\u001b[39;00m\n\u001b[0;32m     15\u001b[0m coss \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'total' referenced before assignment"
     ]
    }
   ],
   "source": [
    "find_sim_player('Mohamed Salah-EGY-1992', '2017-2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92663232]] 202.4258125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[31, 20, 1812, 20.1, 14, 1, 15, 14, 0, 0, 0, 0, 11.9, 11.9, 2.1,\n",
       "        14.0, 16.0, 30.0, 76.0, 0.7, 0.05, 0.75, 0.7, 0.75, 0.59, 0.1,\n",
       "        0.7, 0.59, 0.7, 31, 1812, 58, 53.0, 20.1, 20, 83.0, 13.0, 11,\n",
       "        14.0, 0, 0.94, 28.0, 34.0, -6.0, -0.3, 0.37, 29.5, 26.5, 3.0,\n",
       "        0.15, 0.5, 14, 48.0, 26, 54.2, 2.38, 1.29, 0.29, 0.54, 10.2, 0.0,\n",
       "        0, 0, 11.9, 11.9, 0.25, 2.1, 2.1, 278.0, 433.0, 64.2, 3415.0,\n",
       "        784.0, 178.0, 260.0, 68.5, 70.0, 108.0, 64.8, 7.0, 11.0, 63.6, 1,\n",
       "        2.1, 1.1, -1.1, 22.0, 19.0, 3.0, 0.0, 30.0, 433.0, 428.0, 5.0,\n",
       "        0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 278.0, 0.0, 12.0,\n",
       "        44.0, 2.19, 33.0, 0.0, 4.0, 7.0, 0.0, 0.0, 4.0, 0.2, 1.0, 0.0,\n",
       "        3.0, 0.0, 0.0, 0.0, 7.0, 4.0, 1.0, 4.0, 2.0, 3.0, 9.0, 33.3, 6.0,\n",
       "        11.0, 3.0, 8.0, 1.0, 8.0, 26.0, 0.0, 628.0, 34.0, 80.0, 311.0,\n",
       "        237.0, 92.0, 628.0, 17.0, 6.0, 35.3, 9.0, 52.9, 295.0, 949.0,\n",
       "        477.0, 16.0, 10.0, 7.0, 50.0, 21.0, 467.0, 76.0]], dtype=object)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo = df.loc[(df['Player'] == 'Mohamed Salah') & (df['Season'] == '2017-2018')]\n",
    "lm = df.loc[(df['Player'] == 'Chris Wood') & (df['Season'] == '2023-2024')]\n",
    "\n",
    "pd.concat([mo, lm], axis=0)\n",
    "\n",
    "mo = mo.fillna(placeholder_value)\n",
    "mo = mo.to_numpy()[:, i_s[3:]]\n",
    "lm = lm.fillna(placeholder_value)\n",
    "lm = lm.to_numpy()[:, i_s[3:]]\n",
    "\n",
    "print(cosine_similarity(mo, lm), mean_squared_error(mo, lm, squared=False))\n",
    "lm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bebo_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
