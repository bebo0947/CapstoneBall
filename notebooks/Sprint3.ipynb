{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abdelrahman Alkhawas Sprint 3\n",
    "This is my sprint two notebook where I will be Reconfiguring my data for some initial NN experimentation. This notebook assumes that the user has access to `eeaao.csv` or has run the latest iteration of `webScraping1.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu==2.10 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (24.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorflow-gpu==2.10) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu==2.10) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\abdel\\anaconda3\\envs\\bebo_tf_env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu==2.10) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%pip install tensorflow-gpu==2.10\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 200\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_id</th>\n",
       "      <th>Season</th>\n",
       "      <th>Player</th>\n",
       "      <th>Nation</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Squad</th>\n",
       "      <th>Comp</th>\n",
       "      <th>Age</th>\n",
       "      <th>Born</th>\n",
       "      <th>Playing Time, MP</th>\n",
       "      <th>Playing Time, Starts</th>\n",
       "      <th>Playing Time, Min</th>\n",
       "      <th>Playing Time, 90s</th>\n",
       "      <th>Performance, Gls</th>\n",
       "      <th>Performance, Ast</th>\n",
       "      <th>Performance, G+A</th>\n",
       "      <th>Performance, G-PK</th>\n",
       "      <th>Performance, PK</th>\n",
       "      <th>Performance, PKatt</th>\n",
       "      <th>Performance, CrdY</th>\n",
       "      <th>Performance, CrdR</th>\n",
       "      <th>Expected, xG</th>\n",
       "      <th>Expected, npxG</th>\n",
       "      <th>Expected, xAG</th>\n",
       "      <th>Expected, npxG+xAG</th>\n",
       "      <th>Progression, PrgC</th>\n",
       "      <th>Progression, PrgP</th>\n",
       "      <th>Progression, PrgR</th>\n",
       "      <th>Per 90 Minutes, Gls</th>\n",
       "      <th>Per 90 Minutes, Ast</th>\n",
       "      <th>Per 90 Minutes, G+A</th>\n",
       "      <th>Per 90 Minutes, G-PK</th>\n",
       "      <th>Per 90 Minutes, G+A-PK</th>\n",
       "      <th>Per 90 Minutes, xG</th>\n",
       "      <th>Per 90 Minutes, xAG</th>\n",
       "      <th>Per 90 Minutes, xG+xAG</th>\n",
       "      <th>Per 90 Minutes, npxG</th>\n",
       "      <th>Per 90 Minutes, npxG+xAG</th>\n",
       "      <th>Playing Time, MP.1</th>\n",
       "      <th>Playing Time, Min.1</th>\n",
       "      <th>Playing Time, Mn/MP</th>\n",
       "      <th>Playing Time, Min%</th>\n",
       "      <th>Playing Time, 90s.1</th>\n",
       "      <th>Starts, Starts</th>\n",
       "      <th>Starts, Mn/Start</th>\n",
       "      <th>Starts, Compl</th>\n",
       "      <th>Subs, Subs</th>\n",
       "      <th>Subs, Mn/Sub</th>\n",
       "      <th>Subs, unSub</th>\n",
       "      <th>Team Success, PPM</th>\n",
       "      <th>Team Success, onG</th>\n",
       "      <th>Team Success, onGA</th>\n",
       "      <th>Team Success, +/-</th>\n",
       "      <th>Team Success, +/-90</th>\n",
       "      <th>Team Success, On-Off</th>\n",
       "      <th>Team Success (xG), onxG</th>\n",
       "      <th>Team Success (xG), onxGA</th>\n",
       "      <th>Team Success (xG), xG+/-</th>\n",
       "      <th>Team Success (xG), xG+/-90</th>\n",
       "      <th>Team Success (xG), On-Off</th>\n",
       "      <th>Standard, Gls</th>\n",
       "      <th>Standard, Sh</th>\n",
       "      <th>Standard, SoT</th>\n",
       "      <th>Standard, SoT%</th>\n",
       "      <th>Standard, Sh/90</th>\n",
       "      <th>Standard, SoT/90</th>\n",
       "      <th>Standard, G/Sh</th>\n",
       "      <th>Standard, G/SoT</th>\n",
       "      <th>Standard, Dist</th>\n",
       "      <th>Standard, FK</th>\n",
       "      <th>Standard, PK</th>\n",
       "      <th>Standard, PKatt</th>\n",
       "      <th>Expected, xG.1</th>\n",
       "      <th>Expected, npxG.1</th>\n",
       "      <th>Expected, npxG/Sh</th>\n",
       "      <th>Expected, G-xG</th>\n",
       "      <th>Expected, np:G-xG</th>\n",
       "      <th>Total, Cmp</th>\n",
       "      <th>Total, Att</th>\n",
       "      <th>Total, Cmp%</th>\n",
       "      <th>Total, TotDist</th>\n",
       "      <th>Total, PrgDist</th>\n",
       "      <th>Short, Cmp</th>\n",
       "      <th>Short, Att</th>\n",
       "      <th>Short, Cmp%</th>\n",
       "      <th>Medium, Cmp</th>\n",
       "      <th>Medium, Att</th>\n",
       "      <th>Medium, Cmp%</th>\n",
       "      <th>Long, Cmp</th>\n",
       "      <th>Long, Att</th>\n",
       "      <th>Long, Cmp%</th>\n",
       "      <th>Ast</th>\n",
       "      <th>xAG</th>\n",
       "      <th>Expected, xA</th>\n",
       "      <th>Expected, A-xAG</th>\n",
       "      <th>KP</th>\n",
       "      <th>1/3</th>\n",
       "      <th>PPA</th>\n",
       "      <th>CrsPA</th>\n",
       "      <th>PrgP</th>\n",
       "      <th>Att</th>\n",
       "      <th>Pass Types, Live</th>\n",
       "      <th>Pass Types, Dead</th>\n",
       "      <th>Pass Types, FK</th>\n",
       "      <th>Pass Types, TB</th>\n",
       "      <th>Pass Types, Sw</th>\n",
       "      <th>Pass Types, Crs</th>\n",
       "      <th>Pass Types, TI</th>\n",
       "      <th>Pass Types, CK</th>\n",
       "      <th>Corner Kicks, In</th>\n",
       "      <th>Corner Kicks, Out</th>\n",
       "      <th>Corner Kicks, Str</th>\n",
       "      <th>Outcomes, Cmp</th>\n",
       "      <th>Outcomes, Off</th>\n",
       "      <th>Outcomes, Blocks</th>\n",
       "      <th>SCA, SCA</th>\n",
       "      <th>SCA, SCA90</th>\n",
       "      <th>SCA Types, PassLive</th>\n",
       "      <th>SCA Types, PassDead</th>\n",
       "      <th>SCA Types, TO</th>\n",
       "      <th>SCA Types, Sh</th>\n",
       "      <th>SCA Types, Fld</th>\n",
       "      <th>SCA Types, Def</th>\n",
       "      <th>GCA, GCA</th>\n",
       "      <th>GCA, GCA90</th>\n",
       "      <th>GCA Types, PassLive</th>\n",
       "      <th>GCA Types, PassDead</th>\n",
       "      <th>GCA Types, TO</th>\n",
       "      <th>GCA Types, Sh</th>\n",
       "      <th>GCA Types, Fld</th>\n",
       "      <th>GCA Types, Def</th>\n",
       "      <th>Tackles, Tkl</th>\n",
       "      <th>Tackles, TklW</th>\n",
       "      <th>Tackles, Def 3rd</th>\n",
       "      <th>Tackles, Mid 3rd</th>\n",
       "      <th>Tackles, Att 3rd</th>\n",
       "      <th>Challenges, Tkl</th>\n",
       "      <th>Challenges, Att</th>\n",
       "      <th>Challenges, Tkl%</th>\n",
       "      <th>Challenges, Lost</th>\n",
       "      <th>Blocks, Blocks</th>\n",
       "      <th>Blocks, Sh</th>\n",
       "      <th>Blocks, Pass</th>\n",
       "      <th>Int</th>\n",
       "      <th>Tkl+Int</th>\n",
       "      <th>Clr</th>\n",
       "      <th>Err</th>\n",
       "      <th>Touches, Touches</th>\n",
       "      <th>Touches, Def Pen</th>\n",
       "      <th>Touches, Def 3rd</th>\n",
       "      <th>Touches, Mid 3rd</th>\n",
       "      <th>Touches, Att 3rd</th>\n",
       "      <th>Touches, Att Pen</th>\n",
       "      <th>Touches, Live</th>\n",
       "      <th>Take-Ons, Att</th>\n",
       "      <th>Take-Ons, Succ</th>\n",
       "      <th>Take-Ons, Succ%</th>\n",
       "      <th>Take-Ons, Tkld</th>\n",
       "      <th>Take-Ons, Tkld%</th>\n",
       "      <th>Carries, Carries</th>\n",
       "      <th>Carries, TotDist</th>\n",
       "      <th>Carries, PrgDist</th>\n",
       "      <th>Carries, PrgC</th>\n",
       "      <th>Carries, 1/3</th>\n",
       "      <th>Carries, CPA</th>\n",
       "      <th>Carries, Mis</th>\n",
       "      <th>Carries, Dis</th>\n",
       "      <th>Receiving, Rec</th>\n",
       "      <th>Receiving, PrgR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patrick van Aanholt-NED-1990</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Patrick van Aanholt</td>\n",
       "      <td>nl NED</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>eng Premier League</td>\n",
       "      <td>26</td>\n",
       "      <td>1990</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>2184</td>\n",
       "      <td>24.3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>46.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>28</td>\n",
       "      <td>2184</td>\n",
       "      <td>78</td>\n",
       "      <td>63.9</td>\n",
       "      <td>24.3</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.21</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>32.2</td>\n",
       "      <td>34.7</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>5</td>\n",
       "      <td>33.0</td>\n",
       "      <td>11</td>\n",
       "      <td>33.3</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>23.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>884.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>75.2</td>\n",
       "      <td>14197.0</td>\n",
       "      <td>6422.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>74.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>48.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>897.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>47.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>83.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>717.0</td>\n",
       "      <td>3714.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rolando Aarons-ENG-1995</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Rolando Aarons</td>\n",
       "      <td>eng ENG</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Newcastle Utd</td>\n",
       "      <td>eng Premier League</td>\n",
       "      <td>21</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>35</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>65.9</td>\n",
       "      <td>376.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rolando Aarons-ENG-1995</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Rolando Aarons</td>\n",
       "      <td>eng ENG</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hellas Verona</td>\n",
       "      <td>it Serie A</td>\n",
       "      <td>21</td>\n",
       "      <td>1995</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>517</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>11</td>\n",
       "      <td>517</td>\n",
       "      <td>47</td>\n",
       "      <td>15.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-1.74</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>2.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>87.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>83.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>71.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>53.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46.7</td>\n",
       "      <td>105.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ignazio Abate-ITA-1986</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Ignazio Abate</td>\n",
       "      <td>it ITA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Milan</td>\n",
       "      <td>it Serie A</td>\n",
       "      <td>30</td>\n",
       "      <td>1986</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>1057</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>17</td>\n",
       "      <td>1057</td>\n",
       "      <td>62</td>\n",
       "      <td>30.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>1.71</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.33</td>\n",
       "      <td>13.8</td>\n",
       "      <td>11.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>17.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>625.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>10991.0</td>\n",
       "      <td>4535.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>92.9</td>\n",
       "      <td>287.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>86.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>48.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aymen Abdennour-TUN-1989</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>Aymen Abdennour</td>\n",
       "      <td>tn TUN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>fr Ligue 1</td>\n",
       "      <td>27</td>\n",
       "      <td>1989</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>499</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>8</td>\n",
       "      <td>499</td>\n",
       "      <td>62</td>\n",
       "      <td>14.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18</td>\n",
       "      <td>2.38</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>310.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>93.1</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>1557.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>98.4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>95.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>70.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>1522.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           p_id     Season               Player   Nation  Pos           Squad                Comp  Age  Born  Playing Time, MP  Playing Time, Starts  Playing Time, Min  Playing Time, 90s  Performance, Gls  Performance, Ast  Performance, G+A  Performance, G-PK  Performance, PK  Performance, PKatt  Performance, CrdY  Performance, CrdR  Expected, xG  Expected, npxG  Expected, xAG  Expected, npxG+xAG  Progression, PrgC  Progression, PrgP  Progression, PrgR  Per 90 Minutes, Gls  Per 90 Minutes, Ast  Per 90 Minutes, G+A  Per 90 Minutes, G-PK  Per 90 Minutes, G+A-PK  Per 90 Minutes, xG  Per 90 Minutes, xAG  Per 90 Minutes, xG+xAG  Per 90 Minutes, npxG  Per 90 Minutes, npxG+xAG  Playing Time, MP.1  Playing Time, Min.1  Playing Time, Mn/MP  Playing Time, Min%  Playing Time, 90s.1  Starts, Starts  Starts, Mn/Start  Starts, Compl  Subs, Subs  Subs, Mn/Sub  Subs, unSub  Team Success, PPM  Team Success, onG  Team Success, onGA  Team Success, +/-  Team Success, +/-90  \\\n",
       "0  Patrick van Aanholt-NED-1990  2017-2018  Patrick van Aanholt   nl NED  1.0  Crystal Palace  eng Premier League   26  1990                28                    25               2184               24.3                 5                 1                 6                  5                0                   0                  7                  0           3.1             3.1            2.1                 5.2               46.0               92.0               86.0                 0.21                 0.04                 0.25                  0.21                    0.25                0.13                 0.09                    0.21                  0.13                      0.21                  28                 2184                   78                63.9                 24.3              25               NaN           21.0           3           NaN            8               1.21               31.0                38.0               -7.0                -0.29   \n",
       "1       Rolando Aarons-ENG-1995  2017-2018       Rolando Aarons  eng ENG  5.0   Newcastle Utd  eng Premier League   21  1995                 4                     1                139                1.5                 0                 0                 0                  0                0                   0                  0                  0           0.1             0.1            0.0                 0.1                7.0                3.0                4.0                 0.00                 0.00                 0.00                  0.00                    0.00                0.04                 0.00                    0.04                  0.04                      0.04                   4                  139                   35                 4.1                  1.5               1               NaN            0.0           3           NaN            7               0.25                1.0                 2.0               -1.0                -0.65   \n",
       "2       Rolando Aarons-ENG-1995  2017-2018       Rolando Aarons  eng ENG  5.0   Hellas Verona          it Serie A   21  1995                11                     6                517                5.7                 0                 0                 0                  0                0                   0                  0                  0           0.2             0.2            0.2                 0.3                9.0               17.0               31.0                 0.00                 0.00                 0.00                  0.00                    0.00                0.03                 0.03                    0.06                  0.03                      0.06                  11                  517                   47                15.1                  5.7               6               NaN            1.0           5           NaN            4               0.55                2.0                12.0              -10.0                -1.74   \n",
       "3        Ignazio Abate-ITA-1986  2017-2018        Ignazio Abate   it ITA  1.0           Milan          it Serie A   30  1986                17                    11               1057               11.7                 1                 0                 1                  1                0                   0                  3                  0           0.2             0.2            0.5                 0.7               20.0               81.0               65.0                 0.09                 0.00                 0.09                  0.09                    0.09                0.01                 0.04                    0.06                  0.01                      0.06                  17                 1057                   62                30.9                 11.7              11               NaN            7.0           6           NaN           15               1.71               17.0                10.0                7.0                 0.60   \n",
       "4      Aymen Abdennour-TUN-1989  2017-2018      Aymen Abdennour   tn TUN  1.0       Marseille          fr Ligue 1   27  1989                 8                     6                499                5.5                 0                 0                 0                  0                0                   0                  3                  0           0.1             0.1            0.0                 0.1                2.0               12.0                0.0                 0.00                 0.00                 0.00                  0.00                    0.00                0.02                 0.00                    0.02                  0.02                      0.02                   8                  499                   62                14.6                  5.5               6              82.0            4.0           2           3.0           18               2.38               10.0                 4.0                6.0                 1.08   \n",
       "\n",
       "   Team Success, On-Off  Team Success (xG), onxG  Team Success (xG), onxGA  Team Success (xG), xG+/-  Team Success (xG), xG+/-90  Team Success (xG), On-Off  Standard, Gls  Standard, Sh  Standard, SoT  Standard, SoT%  Standard, Sh/90  Standard, SoT/90  Standard, G/Sh  Standard, G/SoT  Standard, Dist  Standard, FK  Standard, PK  Standard, PKatt  Expected, xG.1  Expected, npxG.1  Expected, npxG/Sh  Expected, G-xG  Expected, np:G-xG  Total, Cmp  Total, Att  Total, Cmp%  Total, TotDist  Total, PrgDist  Short, Cmp  Short, Att  Short, Cmp%  Medium, Cmp  Medium, Att  Medium, Cmp%  Long, Cmp  Long, Att  Long, Cmp%  Ast  xAG  Expected, xA  Expected, A-xAG    KP   1/3   PPA  CrsPA  PrgP     Att  Pass Types, Live  Pass Types, Dead  Pass Types, FK  Pass Types, TB  Pass Types, Sw  Pass Types, Crs  Pass Types, TI  Pass Types, CK  Corner Kicks, In  Corner Kicks, Out  Corner Kicks, Str  Outcomes, Cmp  Outcomes, Off  Outcomes, Blocks  SCA, SCA  SCA, SCA90  SCA Types, PassLive  SCA Types, PassDead  \\\n",
       "0                 -0.07                     32.2                      34.7                      -2.5                       -0.10                      -0.66              5          33.0             11            33.3             1.36              0.45            0.15             0.45            23.4           4.0             0                0             3.1               3.1               0.09             1.9                1.9       884.0      1176.0         75.2         14197.0          6422.0       479.0       532.0         90.0        311.0        431.0          72.2       74.0      152.0        48.7    1  2.1           1.8             -1.1  18.0  63.0  28.0    6.0  92.0  1176.0             897.0             276.0            30.0             3.0             2.0             56.0           235.0            11.0               3.0                5.0                2.0          884.0            3.0              31.0      46.0        1.90                 24.0                 12.0   \n",
       "1                 -0.46                      0.4                       2.9                      -2.6                       -1.66                      -1.43              0           2.0              0             0.0             1.29              0.00            0.00              NaN            19.9           0.0             0                0             0.1               0.1               0.03            -0.1               -0.1        29.0        44.0         65.9           376.0            77.0        24.0        30.0         80.0          2.0          5.0          40.0        2.0        3.0        66.7    0  0.0           0.0              0.0   0.0   2.0   1.0    1.0   3.0    44.0              41.0               2.0             2.0             0.0             1.0              2.0             0.0             0.0               0.0                0.0                0.0           29.0            1.0               3.0       1.0        0.65                  0.0                  0.0   \n",
       "2                 -0.56                      2.7                      10.0                      -7.3                       -1.26                      -0.34              0           3.0              0             0.0             0.52              0.00            0.00              NaN            17.4           0.0             0                0             0.2               0.2               0.05            -0.2               -0.2        87.0       120.0         72.5          1174.0           325.0        50.0        60.0         83.3         25.0         35.0          71.4        4.0        8.0        50.0    0  0.2           0.1             -0.2   3.0   8.0   7.0    1.0  17.0   120.0             110.0              10.0             4.0             0.0             0.0             10.0             4.0             0.0               0.0                0.0                0.0           87.0            0.0               5.0       5.0        0.87                  4.0                  1.0   \n",
       "3                  0.33                     13.8                      11.4                       2.5                        0.21                      -0.24              1           4.0              2            50.0             0.34              0.17            0.25             0.50            17.1           0.0             0                0             0.2               0.2               0.04             0.8                0.8       625.0       776.0         80.5         10991.0          4535.0       273.0       294.0         92.9        287.0        333.0          86.2       58.0      120.0        48.3    0  0.5           0.8             -0.5  10.0  55.0  20.0    7.0  81.0   776.0             653.0             119.0             7.0             0.0             2.0             31.0           112.0             0.0               0.0                0.0                0.0          625.0            4.0              21.0      27.0        2.30                 24.0                  1.0   \n",
       "4                  0.25                     12.0                       4.2                       7.8                        1.41                       0.53              0           2.0              1            50.0             0.36              0.18            0.00             0.00            11.5           0.0             0                0             0.1               0.1               0.06            -0.1               -0.1       310.0       333.0         93.1          5550.0          1557.0       126.0       128.0         98.4        148.0        155.0          95.5       24.0       34.0        70.6    0  0.0           0.0              0.0   0.0   8.0   0.0    0.0  12.0   333.0             325.0               7.0             7.0             0.0             3.0              0.0             0.0             0.0               0.0                0.0                0.0          310.0            1.0               2.0       2.0        0.36                  2.0                  0.0   \n",
       "\n",
       "   SCA Types, TO  SCA Types, Sh  SCA Types, Fld  SCA Types, Def  GCA, GCA  GCA, GCA90  GCA Types, PassLive  GCA Types, PassDead  GCA Types, TO  GCA Types, Sh  GCA Types, Fld  GCA Types, Def  Tackles, Tkl  Tackles, TklW  Tackles, Def 3rd  Tackles, Mid 3rd  Tackles, Att 3rd  Challenges, Tkl  Challenges, Att  Challenges, Tkl%  Challenges, Lost  Blocks, Blocks  Blocks, Sh  Blocks, Pass   Int  Tkl+Int   Clr  Err  Touches, Touches  Touches, Def Pen  Touches, Def 3rd  Touches, Mid 3rd  Touches, Att 3rd  Touches, Att Pen  Touches, Live  Take-Ons, Att  Take-Ons, Succ  Take-Ons, Succ%  Take-Ons, Tkld  Take-Ons, Tkld%  Carries, Carries  Carries, TotDist  Carries, PrgDist  Carries, PrgC  Carries, 1/3  Carries, CPA  Carries, Mis  Carries, Dis  Receiving, Rec  Receiving, PrgR  \n",
       "0            6.0            2.0             1.0             1.0       4.0        0.16                  2.0                  0.0            2.0            0.0             0.0             0.0          47.0           32.0              29.0              15.0               3.0             16.0             34.0              47.1              18.0            24.0         5.0          19.0  47.0     94.0  64.0  2.0            1435.0              90.0             445.0             616.0             391.0              32.0         1435.0           31.0            26.0             83.9             5.0             16.1             717.0            3714.0            1966.0           46.0          33.0           7.0          37.0          18.0           711.0             86.0  \n",
       "1            0.0            1.0             0.0             0.0       0.0        0.00                  0.0                  0.0            0.0            0.0             0.0             0.0           4.0            4.0               3.0               1.0               0.0              4.0              6.0              66.7               2.0             3.0         0.0           3.0   1.0      5.0   0.0  0.0              65.0               0.0              11.0              30.0              25.0               3.0           65.0            6.0             4.0             66.7             2.0             33.3              37.0             380.0             127.0            7.0           3.0           3.0           4.0           6.0            39.0              4.0  \n",
       "2            0.0            0.0             0.0             0.0       0.0        0.00                  0.0                  0.0            0.0            0.0             0.0             0.0          13.0            8.0               4.0               4.0               5.0              1.0              7.0              14.3               6.0             3.0         1.0           2.0   2.0     15.0   0.0  0.0             166.0               3.0              18.0              71.0              80.0               7.0          166.0           15.0             8.0             53.3             7.0             46.7             105.0             712.0             352.0            9.0          11.0           2.0           9.0          13.0           125.0             31.0  \n",
       "3            0.0            2.0             0.0             0.0       4.0        0.34                  3.0                  0.0            0.0            1.0             0.0             0.0          20.0           17.0              10.0               9.0               1.0              6.0             11.0              54.5               5.0            22.0         2.0          20.0   8.0     28.0  29.0  0.0             864.0              35.0             237.0             428.0             206.0               8.0          864.0            1.0             0.0              0.0             1.0            100.0             386.0            1802.0            1124.0           20.0          18.0           0.0           7.0           6.0           563.0             65.0  \n",
       "4            0.0            0.0             0.0             0.0       0.0        0.00                  0.0                  0.0            0.0            0.0             0.0             0.0           7.0            3.0               5.0               2.0               0.0              4.0              7.0              57.1               3.0             5.0         5.0           0.0   4.0     11.0  20.0  0.0             375.0              27.0             175.0             200.0               4.0               2.0          375.0            1.0             1.0            100.0             0.0              0.0             296.0            1522.0             829.0            2.0           1.0           0.0           0.0           1.0           278.0              0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing my Data. \n",
    "df = pd.read_csv('../data/eeaao.csv')\n",
    "df = df.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns: print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some initial thoughts\n",
    "Following are some initial thoughts I was planning on tackling this problem that couldnt take off the ground, and whose EDA/planning was too messy/incoherent to make it to this notebook.\n",
    "<br><br>\n",
    "I was looking to treat each datapoint as a sentence in typical RNN fashion. This would skip the usual token vectorization since the data is already numeric. However, now comes the question of 'what is a datapoint?' for any given player, do we have 7 datapoints (one for each season) of sentence length 160 and we try to predict the entire next year, or do we have 160 datapoints (one for each feature) of size 7 and we try to predict the next element (word) of that features sequence (sentence)?\n",
    "<br>\n",
    "The issue with the first way is that sentences are extremely long and the idea of generating sentences that way can lead to 'jumbled sentenced'. The issue with the second way is that we have to teach 160 models how to interact with each other and we lose any value of using NNs due to lack of interaction of features. \n",
    "<br><br>\n",
    "Whilst the second approach makes no sense to use, the first approach has somoe promise, with some alterations to the structure of the data and the preprocessing techniques. \n",
    "\n",
    "---\n",
    "\n",
    "#### The approach\n",
    "1. We will runn a similar 'cleaning' operation as in Sprint 2 on the entire dataset to make sure each player has only one datapoint per season\n",
    "2. For our first attempt we will also filter to only players that have datapoints for every season (so we dont have to worry about putting NaNs into the model) and only try to predict our previously defined 'impact' metrics\n",
    "3. Generate the cleaned 3D matrix and to a traintest separate X_train and Y_train\n",
    "4. train a simple (not too many layers/ big layers) LTSM NN \n",
    "5. evaluate\n",
    "##### Problems with Approach 1\n",
    "- Missing important players (obv)\n",
    "- Bias towards aging players or players on teams that stay up\n",
    "##### Places to progress\n",
    "- train on all players. Will also require funky preprocessing. (update: done. we inserted 'empty' rows for players with missing seasons)\n",
    "- bigger NN (update: Turns out we are very prone to overfitting and bigger =/= better)\n",
    "- different activations (update: not really much to do here)\n",
    "- combine all the above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10178, 169)\n"
     ]
    }
   ],
   "source": [
    "# The objective of this block of code is to consolidate player datapoints such that each player for each season has 'at most' one row. \n",
    "# We are also not dealing with any funny business with transfers.\n",
    "tgb = df.groupby('p_id')  # Group by the unique players. \n",
    "\n",
    "seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024']\n",
    "\n",
    "df_by_pid = {}  # {pid: df}\n",
    "for pid in tgb.groups.keys():\n",
    "    if tgb.get_group(pid).shape[0] >= 5:\n",
    "        df_by_pid[pid] = tgb.get_group(pid)\n",
    "\n",
    "full_careers = {}  # {pid: cleaned_df}\n",
    "total = pd.DataFrame()\n",
    "n_full = 0\n",
    "\n",
    "# want to get rid of rows where trnsfer window gets in the way. This means for the same season, there are two rows\n",
    "# If for one of them has Playing Time, Min > 180, ignore this player. Otherwise, take the max. \n",
    "for pid in df_by_pid.keys():\n",
    "    # print(pid)\n",
    "    omit_player = False\n",
    "    pid_df = pd.DataFrame()\n",
    "    # gor into pdf and ensure for each \n",
    "    # go through each season JUST TO CHECK WHETHER WE SHOULD OMIT THS PLAYER\n",
    "    df_by_pid_season = df_by_pid[pid].groupby('Season')  # groupby object where keys are seasons\n",
    "    for season in df_by_pid_season.groups.keys(): \n",
    "        season_df = df_by_pid_season.get_group(season)\n",
    "        if season_df.shape[0] >= 2 and sum(season_df['Playing Time, Min'] > 270) >= 2: \n",
    "            # So we want to drop this season from this player.\n",
    "            # # We can do this by just ignoring this season and moving on to the next.\n",
    "            pass\n",
    "        elif season_df.shape[0] >= 2:\n",
    "            # This player has two rows for this season, but only one of them is keepable\n",
    "            # print(f'{pid} transferred in {season}')\n",
    "            most_mins = season_df['Playing Time, Min'].max()\n",
    "            season_df = season_df.loc[season_df['Playing Time, Min'] == most_mins]\n",
    "            pid_df = pd.concat([pid_df, season_df], axis=0)\n",
    "        else:\n",
    "            # This season is good to add to this players dataframe. \n",
    "            pid_df = pd.concat([pid_df, season_df], axis=0)\n",
    "    full_careers[pid] = pid_df\n",
    "\n",
    "    # full_careers[pid] = df_by_pid[pid].loc[df_by_pid_season['Playing Time, Min'].idxmax()]\n",
    "\n",
    "    if not omit_player and full_careers[pid].shape[0] >= 5:  # This player has at least 5 seasons\n",
    "        n_full+= 1\n",
    "        player_data = full_careers[pid]\n",
    "        # Here we want to find which seasons are missing, then populate them with 'empty' rows. \n",
    "        placeholder_value1 = -500\n",
    "        for season in seasons:\n",
    "            if season not in list(player_data['Season']):\n",
    "                # Add the season row with blanks\n",
    "                # print(f'{pid} needs {season}')\n",
    "                player_name = player_data['Player'].mode()[0]\n",
    "                nation = player_data['Nation'].mode()[0]\n",
    "                born = player_data['Born'].mode()[0]\n",
    "                position = player_data['Pos'].mode()[0]\n",
    "                squad = player_data['Squad'].mode()[0]\n",
    "                comp = player_data['Comp'].mode()[0]\n",
    "\n",
    "                age = int(season.split('-')[0]) - born              \n",
    "\n",
    "                new_row = [pid, season, player_name, nation, position, squad, comp, age, born] + [placeholder_value1] * 160\n",
    "                new_row_df = pd.DataFrame([new_row], columns=df.columns)\n",
    "                player_data = pd.concat([player_data, new_row_df], ignore_index=True)\n",
    "                \n",
    "        total = pd.concat([total, player_data], axis=0)\n",
    "\n",
    "# Now full careers contans all of the players who have at least 7 datapoints {p_id: df}  717 of them\n",
    "print(total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_careers[pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gb = total.groupby('p_id')\n",
    "for pid in test_gb.groups.keys():\n",
    "    print(pid)\n",
    "    assert test_gb.get_group(pid).shape == (7, 169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.sort_values(['p_id', 'Season'])\n",
    "\n",
    "placeholder_value = -999.0\n",
    "nan_total = total.fillna(placeholder_value)\n",
    "\n",
    "# Get from total\n",
    "num_players = total['p_id'].unique().shape[0]\n",
    "data = nan_total.to_numpy().reshape(num_players, 7, 169)\n",
    "\n",
    "# We now need to set up the 3d matrix and do a train/test split\n",
    "\n",
    "players = total['p_id'].unique()\n",
    "\"\"\"\n",
    "original technique\n",
    "# Reserving a small portion of the players for testing\n",
    "train_players, test_players = train_test_split(players, test_size=0.1, random_state=42, shuffle=False)  \n",
    "\n",
    "# Now put the portions of the players into the 3d format\n",
    "train_indices = [i for i, player in enumerate(players) if player in train_players]\n",
    "test_indices = [i for i, player in enumerate(players) if player in test_players]\n",
    "\"\"\"\n",
    "\n",
    "train_indices, test_indices = train_test_split(list(range(469)), test_size=0.1, random_state=42) \n",
    "train_indices, test_indices = list(train_indices), list(test_indices)\n",
    "\n",
    "# getting non-string features\n",
    "i_s = [4]\n",
    "i_s.extend(list(range(7, 169)))  # Numerical indicies for training\n",
    "ti_s = i_s[3:]  # Numerical indicies for prediction. \n",
    "# removing the first three features had a major impact, taking r2_score of the NN from -2000 (you read that right) to -2\n",
    "\n",
    "split_i = math.ceil(0.85 * num_players)\n",
    "X_train, Y_train = data[:split_i, :-1, i_s], data[:split_i, -1, ti_s]\n",
    "X_test, Y_test = data[split_i:, :-1, i_s], data[split_i:, -1, ti_s]\n",
    "\n",
    "# Ensuring floats for NN\n",
    "X_train = X_train.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "Y_test = Y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1236, 6, 163)\n",
      "Y_train shape: (1236, 160)\n",
      "X_test shape: (218, 6, 163)\n",
      "Y_test shape: (218, 160)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# Print shapes for verification\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n",
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "14/14 [==============================] - 20s 251ms/step - loss: 424.2100 - val_loss: 417.3659\n",
      "Epoch 2/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 424.1608 - val_loss: 417.3239\n",
      "Epoch 3/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 424.1199 - val_loss: 417.2839\n",
      "Epoch 4/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 424.0799 - val_loss: 417.2446\n",
      "Epoch 5/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 424.0405 - val_loss: 417.2054\n",
      "Epoch 6/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 424.0008 - val_loss: 417.1664\n",
      "Epoch 7/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.9619 - val_loss: 417.1275\n",
      "Epoch 8/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.9225 - val_loss: 417.0887\n",
      "Epoch 9/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 423.8841 - val_loss: 417.0501\n",
      "Epoch 10/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 423.8443 - val_loss: 417.0116\n",
      "Epoch 11/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 423.8059 - val_loss: 416.9733\n",
      "Epoch 12/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.7673 - val_loss: 416.9350\n",
      "Epoch 13/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 423.7294 - val_loss: 416.8969\n",
      "Epoch 14/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 423.6904 - val_loss: 416.8589\n",
      "Epoch 15/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 423.6525 - val_loss: 416.8210\n",
      "Epoch 16/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 423.6141 - val_loss: 416.7833\n",
      "Epoch 17/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.5759 - val_loss: 416.7457\n",
      "Epoch 18/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.5389 - val_loss: 416.7083\n",
      "Epoch 19/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 423.5009 - val_loss: 416.6709\n",
      "Epoch 20/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 423.4630 - val_loss: 416.6336\n",
      "Epoch 21/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 423.4260 - val_loss: 416.5965\n",
      "Epoch 22/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.3890 - val_loss: 416.5595\n",
      "Epoch 23/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 423.3512 - val_loss: 416.5226\n",
      "Epoch 24/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.3139 - val_loss: 416.4859\n",
      "Epoch 25/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 423.2770 - val_loss: 416.4493\n",
      "Epoch 26/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 423.2404 - val_loss: 416.4128\n",
      "Epoch 27/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 423.2042 - val_loss: 416.3764\n",
      "Epoch 28/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 423.1674 - val_loss: 416.3402\n",
      "Epoch 29/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.1312 - val_loss: 416.3041\n",
      "Epoch 30/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 423.0948 - val_loss: 416.2681\n",
      "Epoch 31/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 423.0584 - val_loss: 416.2322\n",
      "Epoch 32/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 423.0236 - val_loss: 416.1965\n",
      "Epoch 33/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.9862 - val_loss: 416.1608\n",
      "Epoch 34/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 422.9509 - val_loss: 416.1252\n",
      "Epoch 35/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 422.9160 - val_loss: 416.0898\n",
      "Epoch 36/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 422.8797 - val_loss: 416.0544\n",
      "Epoch 37/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.8444 - val_loss: 416.0192\n",
      "Epoch 38/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.8084 - val_loss: 415.9841\n",
      "Epoch 39/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.7737 - val_loss: 415.9492\n",
      "Epoch 40/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 422.7387 - val_loss: 415.9143\n",
      "Epoch 41/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.7039 - val_loss: 415.8796\n",
      "Epoch 42/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 422.6691 - val_loss: 415.8451\n",
      "Epoch 43/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 422.6341 - val_loss: 415.8106\n",
      "Epoch 44/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 422.5988 - val_loss: 415.7762\n",
      "Epoch 45/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 422.5648 - val_loss: 415.7419\n",
      "Epoch 46/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 422.5306 - val_loss: 415.7078\n",
      "Epoch 47/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.4963 - val_loss: 415.6738\n",
      "Epoch 48/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.4610 - val_loss: 415.6399\n",
      "Epoch 49/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 422.4281 - val_loss: 415.6060\n",
      "Epoch 50/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 422.3944 - val_loss: 415.5724\n",
      "Epoch 51/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 422.3615 - val_loss: 415.5388\n",
      "Epoch 52/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.3261 - val_loss: 415.5051\n",
      "Epoch 53/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.2935 - val_loss: 415.4718\n",
      "Epoch 54/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.2590 - val_loss: 415.4385\n",
      "Epoch 55/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 422.2262 - val_loss: 415.4054\n",
      "Epoch 56/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 422.1914 - val_loss: 415.3724\n",
      "Epoch 57/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.1589 - val_loss: 415.3394\n",
      "Epoch 58/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.1260 - val_loss: 415.3066\n",
      "Epoch 59/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 422.0926 - val_loss: 415.2740\n",
      "Epoch 60/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 422.0619 - val_loss: 415.2415\n",
      "Epoch 61/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 422.0266 - val_loss: 415.2089\n",
      "Epoch 62/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.9952 - val_loss: 415.1765\n",
      "Epoch 63/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.9617 - val_loss: 415.1442\n",
      "Epoch 64/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.9295 - val_loss: 415.1120\n",
      "Epoch 65/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 421.8973 - val_loss: 415.0799\n",
      "Epoch 66/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.8661 - val_loss: 415.0480\n",
      "Epoch 67/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.8319 - val_loss: 415.0161\n",
      "Epoch 68/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.8004 - val_loss: 414.9844\n",
      "Epoch 69/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 421.7693 - val_loss: 414.9526\n",
      "Epoch 70/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 421.7368 - val_loss: 414.9210\n",
      "Epoch 71/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.7049 - val_loss: 414.8895\n",
      "Epoch 72/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.6740 - val_loss: 414.8581\n",
      "Epoch 73/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.6405 - val_loss: 414.8267\n",
      "Epoch 74/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.6082 - val_loss: 414.7955\n",
      "Epoch 75/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.5760 - val_loss: 414.7643\n",
      "Epoch 76/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.5480 - val_loss: 414.7333\n",
      "Epoch 77/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 421.5175 - val_loss: 414.7023\n",
      "Epoch 78/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 421.4829 - val_loss: 414.6714\n",
      "Epoch 79/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.4522 - val_loss: 414.6406\n",
      "Epoch 80/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.4220 - val_loss: 414.6099\n",
      "Epoch 81/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.3884 - val_loss: 414.5793\n",
      "Epoch 82/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.3592 - val_loss: 414.5486\n",
      "Epoch 83/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 421.3289 - val_loss: 414.5182\n",
      "Epoch 84/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.2968 - val_loss: 414.4878\n",
      "Epoch 85/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 421.2661 - val_loss: 414.4575\n",
      "Epoch 86/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.2357 - val_loss: 414.4272\n",
      "Epoch 87/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 421.2049 - val_loss: 414.3969\n",
      "Epoch 88/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 421.1734 - val_loss: 414.3667\n",
      "Epoch 89/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 421.1454 - val_loss: 414.3366\n",
      "Epoch 90/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 421.1107 - val_loss: 414.3066\n",
      "Epoch 91/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.0819 - val_loss: 414.2767\n",
      "Epoch 92/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 421.0504 - val_loss: 414.2468\n",
      "Epoch 93/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 421.0254 - val_loss: 414.2170\n",
      "Epoch 94/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.9883 - val_loss: 414.1869\n",
      "Epoch 95/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.9599 - val_loss: 414.1573\n",
      "Epoch 96/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.9304 - val_loss: 414.1276\n",
      "Epoch 97/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 420.9039 - val_loss: 414.0980\n",
      "Epoch 98/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 420.8701 - val_loss: 414.0682\n",
      "Epoch 99/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 420.8353 - val_loss: 414.0385\n",
      "Epoch 100/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 420.8086 - val_loss: 414.0091\n",
      "Epoch 101/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 420.7743 - val_loss: 413.9793\n",
      "Epoch 102/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.7462 - val_loss: 413.9497\n",
      "Epoch 103/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 420.7142 - val_loss: 413.9203\n",
      "Epoch 104/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 420.6835 - val_loss: 413.8907\n",
      "Epoch 105/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.6565 - val_loss: 413.8614\n",
      "Epoch 106/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 420.6262 - val_loss: 413.8319\n",
      "Epoch 107/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 420.5965 - val_loss: 413.8026\n",
      "Epoch 108/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.5654 - val_loss: 413.7732\n",
      "Epoch 109/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.5315 - val_loss: 413.7436\n",
      "Epoch 110/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.5052 - val_loss: 413.7142\n",
      "Epoch 111/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 420.4672 - val_loss: 413.6844\n",
      "Epoch 112/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.4432 - val_loss: 413.6549\n",
      "Epoch 113/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.4145 - val_loss: 413.6254\n",
      "Epoch 114/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 420.3758 - val_loss: 413.5957\n",
      "Epoch 115/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.3475 - val_loss: 413.5662\n",
      "Epoch 116/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 420.3140 - val_loss: 413.5362\n",
      "Epoch 117/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.2894 - val_loss: 413.5066\n",
      "Epoch 118/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 420.2597 - val_loss: 413.4772\n",
      "Epoch 119/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.2346 - val_loss: 413.4475\n",
      "Epoch 120/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.1885 - val_loss: 413.4174\n",
      "Epoch 121/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.1743 - val_loss: 413.3874\n",
      "Epoch 122/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.1266 - val_loss: 413.3574\n",
      "Epoch 123/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 420.1019 - val_loss: 413.3276\n",
      "Epoch 124/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 420.0627 - val_loss: 413.2971\n",
      "Epoch 125/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 420.0486 - val_loss: 413.2676\n",
      "Epoch 126/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 420.0079 - val_loss: 413.2376\n",
      "Epoch 127/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 419.9664 - val_loss: 413.2070\n",
      "Epoch 128/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 419.9509 - val_loss: 413.1770\n",
      "Epoch 129/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 419.9057 - val_loss: 413.1465\n",
      "Epoch 130/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 419.8722 - val_loss: 413.1160\n",
      "Epoch 131/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 419.8448 - val_loss: 413.0854\n",
      "Epoch 132/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 419.7992 - val_loss: 413.0540\n",
      "Epoch 133/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 419.7817 - val_loss: 413.0229\n",
      "Epoch 134/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 419.7525 - val_loss: 412.9918\n",
      "Epoch 135/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 419.7283 - val_loss: 412.9608\n",
      "Epoch 136/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 419.6748 - val_loss: 412.9284\n",
      "Epoch 137/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 419.6510 - val_loss: 412.8976\n",
      "Epoch 138/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 419.6079 - val_loss: 412.8655\n",
      "Epoch 139/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 419.5795 - val_loss: 412.8333\n",
      "Epoch 140/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 419.5213 - val_loss: 412.8003\n",
      "Epoch 141/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 419.5049 - val_loss: 412.7677\n",
      "Epoch 142/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 419.4871 - val_loss: 412.7360\n",
      "Epoch 143/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 419.4210 - val_loss: 412.7029\n",
      "Epoch 144/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 419.3763 - val_loss: 412.6696\n",
      "Epoch 145/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 419.3813 - val_loss: 412.6367\n",
      "Epoch 146/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 419.3417 - val_loss: 412.6039\n",
      "Epoch 147/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 419.2617 - val_loss: 412.5694\n",
      "Epoch 148/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 419.2367 - val_loss: 412.5345\n",
      "Epoch 149/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 419.2195 - val_loss: 412.5010\n",
      "Epoch 150/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 419.1770 - val_loss: 412.4664\n",
      "Epoch 151/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 419.1381 - val_loss: 412.4312\n",
      "Epoch 152/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 419.1104 - val_loss: 412.3963\n",
      "Epoch 153/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 419.0666 - val_loss: 412.3608\n",
      "Epoch 154/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 419.0267 - val_loss: 412.3254\n",
      "Epoch 155/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 418.9948 - val_loss: 412.2906\n",
      "Epoch 156/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 418.9392 - val_loss: 412.2540\n",
      "Epoch 157/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 418.8917 - val_loss: 412.2167\n",
      "Epoch 158/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 418.8731 - val_loss: 412.1817\n",
      "Epoch 159/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.8257 - val_loss: 412.1434\n",
      "Epoch 160/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 418.7493 - val_loss: 412.1055\n",
      "Epoch 161/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 418.6987 - val_loss: 412.0665\n",
      "Epoch 162/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.6918 - val_loss: 412.0301\n",
      "Epoch 163/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 418.6582 - val_loss: 411.9916\n",
      "Epoch 164/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 418.6013 - val_loss: 411.9515\n",
      "Epoch 165/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.5665 - val_loss: 411.9126\n",
      "Epoch 166/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.5089 - val_loss: 411.8729\n",
      "Epoch 167/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 418.4727 - val_loss: 411.8336\n",
      "Epoch 168/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 418.4828 - val_loss: 411.7925\n",
      "Epoch 169/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 418.3510 - val_loss: 411.7498\n",
      "Epoch 170/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.3352 - val_loss: 411.7086\n",
      "Epoch 171/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.3050 - val_loss: 411.6681\n",
      "Epoch 172/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 418.2899 - val_loss: 411.6288\n",
      "Epoch 173/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.1872 - val_loss: 411.5865\n",
      "Epoch 174/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.1641 - val_loss: 411.5430\n",
      "Epoch 175/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 418.1306 - val_loss: 411.5003\n",
      "Epoch 176/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 418.0591 - val_loss: 411.4542\n",
      "Epoch 177/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 417.9980 - val_loss: 411.4103\n",
      "Epoch 178/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 417.9761 - val_loss: 411.3673\n",
      "Epoch 179/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 417.9578 - val_loss: 411.3227\n",
      "Epoch 180/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 417.8709 - val_loss: 411.2767\n",
      "Epoch 181/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 417.8597 - val_loss: 411.2303\n",
      "Epoch 182/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 417.7928 - val_loss: 411.1848\n",
      "Epoch 183/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 417.7432 - val_loss: 411.1388\n",
      "Epoch 184/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 417.6786 - val_loss: 411.0908\n",
      "Epoch 185/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 417.6322 - val_loss: 411.0442\n",
      "Epoch 186/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 417.5442 - val_loss: 410.9944\n",
      "Epoch 187/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 417.4814 - val_loss: 410.9446\n",
      "Epoch 188/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 417.4586 - val_loss: 410.8945\n",
      "Epoch 189/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 417.3593 - val_loss: 410.8452\n",
      "Epoch 190/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 417.3102 - val_loss: 410.7948\n",
      "Epoch 191/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 417.2602 - val_loss: 410.7434\n",
      "Epoch 192/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 417.2364 - val_loss: 410.6914\n",
      "Epoch 193/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 417.1208 - val_loss: 410.6385\n",
      "Epoch 194/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 417.1239 - val_loss: 410.5870\n",
      "Epoch 195/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 417.0737 - val_loss: 410.5356\n",
      "Epoch 196/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 416.9354 - val_loss: 410.4810\n",
      "Epoch 197/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 416.9257 - val_loss: 410.4262\n",
      "Epoch 198/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 416.9208 - val_loss: 410.3737\n",
      "Epoch 199/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 416.7665 - val_loss: 410.3157\n",
      "Epoch 200/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 416.7386 - val_loss: 410.2585\n",
      "Epoch 201/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 416.7100 - val_loss: 410.2017\n",
      "Epoch 202/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 416.5572 - val_loss: 410.1434\n",
      "Epoch 203/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 416.4890 - val_loss: 410.0847\n",
      "Epoch 204/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 416.4416 - val_loss: 410.0251\n",
      "Epoch 205/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 416.3350 - val_loss: 409.9636\n",
      "Epoch 206/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 416.2757 - val_loss: 409.9024\n",
      "Epoch 207/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 416.3005 - val_loss: 409.8409\n",
      "Epoch 208/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 416.1516 - val_loss: 409.7776\n",
      "Epoch 209/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 415.9953 - val_loss: 409.7119\n",
      "Epoch 210/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 416.1435 - val_loss: 409.6504\n",
      "Epoch 211/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 416.0628 - val_loss: 409.5852\n",
      "Epoch 212/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 415.9370 - val_loss: 409.5224\n",
      "Epoch 213/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 415.8114 - val_loss: 409.4546\n",
      "Epoch 214/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 415.6912 - val_loss: 409.3855\n",
      "Epoch 215/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 415.6450 - val_loss: 409.3190\n",
      "Epoch 216/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 415.6265 - val_loss: 409.2522\n",
      "Epoch 217/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 415.4396 - val_loss: 409.1791\n",
      "Epoch 218/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 415.3889 - val_loss: 409.1111\n",
      "Epoch 219/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 415.2809 - val_loss: 409.0404\n",
      "Epoch 220/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 415.2806 - val_loss: 408.9669\n",
      "Epoch 221/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 415.1494 - val_loss: 408.8936\n",
      "Epoch 222/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 414.9680 - val_loss: 408.8194\n",
      "Epoch 223/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 415.0670 - val_loss: 408.7468\n",
      "Epoch 224/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 414.9911 - val_loss: 408.6733\n",
      "Epoch 225/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 414.8622 - val_loss: 408.5930\n",
      "Epoch 226/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 414.8729 - val_loss: 408.5158\n",
      "Epoch 227/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 414.7051 - val_loss: 408.4360\n",
      "Epoch 228/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 414.6912 - val_loss: 408.3573\n",
      "Epoch 229/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 414.5403 - val_loss: 408.2783\n",
      "Epoch 230/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 414.3526 - val_loss: 408.1962\n",
      "Epoch 231/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 414.4288 - val_loss: 408.1124\n",
      "Epoch 232/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 414.1219 - val_loss: 408.0291\n",
      "Epoch 233/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 414.1268 - val_loss: 407.9424\n",
      "Epoch 234/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 414.1981 - val_loss: 407.8598\n",
      "Epoch 235/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 413.9807 - val_loss: 407.7727\n",
      "Epoch 236/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 413.9716 - val_loss: 407.6843\n",
      "Epoch 237/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 413.6578 - val_loss: 407.5922\n",
      "Epoch 238/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 413.5415 - val_loss: 407.5017\n",
      "Epoch 239/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 413.8050 - val_loss: 407.4178\n",
      "Epoch 240/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 413.2859 - val_loss: 407.3214\n",
      "Epoch 241/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 413.3475 - val_loss: 407.2284\n",
      "Epoch 242/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 413.1958 - val_loss: 407.1324\n",
      "Epoch 243/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 413.1618 - val_loss: 407.0354\n",
      "Epoch 244/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 413.0328 - val_loss: 406.9408\n",
      "Epoch 245/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 412.9621 - val_loss: 406.8403\n",
      "Epoch 246/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 413.0443 - val_loss: 406.7415\n",
      "Epoch 247/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 412.5854 - val_loss: 406.6360\n",
      "Epoch 248/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 412.5487 - val_loss: 406.5340\n",
      "Epoch 249/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 412.5145 - val_loss: 406.4365\n",
      "Epoch 250/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 412.2504 - val_loss: 406.3307\n",
      "Epoch 251/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 412.1969 - val_loss: 406.2284\n",
      "Epoch 252/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 412.1419 - val_loss: 406.1229\n",
      "Epoch 253/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 412.0681 - val_loss: 406.0128\n",
      "Epoch 254/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 412.1615 - val_loss: 405.9090\n",
      "Epoch 255/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 411.7253 - val_loss: 405.7980\n",
      "Epoch 256/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 411.6632 - val_loss: 405.6881\n",
      "Epoch 257/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 411.2581 - val_loss: 405.5697\n",
      "Epoch 258/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 411.6425 - val_loss: 405.4613\n",
      "Epoch 259/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 411.0024 - val_loss: 405.3372\n",
      "Epoch 260/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 411.1105 - val_loss: 405.2195\n",
      "Epoch 261/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 411.1412 - val_loss: 405.0978\n",
      "Epoch 262/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 410.8676 - val_loss: 404.9815\n",
      "Epoch 263/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 410.9563 - val_loss: 404.8576\n",
      "Epoch 264/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 410.4589 - val_loss: 404.7328\n",
      "Epoch 265/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 410.5767 - val_loss: 404.6039\n",
      "Epoch 266/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 410.4411 - val_loss: 404.4816\n",
      "Epoch 267/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 410.1929 - val_loss: 404.3495\n",
      "Epoch 268/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 409.9861 - val_loss: 404.2168\n",
      "Epoch 269/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 409.9771 - val_loss: 404.0849\n",
      "Epoch 270/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 409.7870 - val_loss: 403.9529\n",
      "Epoch 271/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 409.0292 - val_loss: 403.8114\n",
      "Epoch 272/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 409.5246 - val_loss: 403.6805\n",
      "Epoch 273/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 409.1371 - val_loss: 403.5371\n",
      "Epoch 274/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 409.0023 - val_loss: 403.3934\n",
      "Epoch 275/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 408.7776 - val_loss: 403.2518\n",
      "Epoch 276/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 408.8370 - val_loss: 403.1064\n",
      "Epoch 277/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 408.9986 - val_loss: 402.9648\n",
      "Epoch 278/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 408.4095 - val_loss: 402.8117\n",
      "Epoch 279/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 408.6490 - val_loss: 402.6653\n",
      "Epoch 280/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 408.2393 - val_loss: 402.5099\n",
      "Epoch 281/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 407.9417 - val_loss: 402.3525\n",
      "Epoch 282/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 407.7044 - val_loss: 402.2038\n",
      "Epoch 283/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 407.5743 - val_loss: 402.0458\n",
      "Epoch 284/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 407.4129 - val_loss: 401.8865\n",
      "Epoch 285/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 407.1312 - val_loss: 401.7196\n",
      "Epoch 286/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 406.7412 - val_loss: 401.5587\n",
      "Epoch 287/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 407.1565 - val_loss: 401.3968\n",
      "Epoch 288/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 406.4022 - val_loss: 401.2285\n",
      "Epoch 289/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 406.6781 - val_loss: 401.0588\n",
      "Epoch 290/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 406.2823 - val_loss: 400.8860\n",
      "Epoch 291/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 406.7490 - val_loss: 400.7203\n",
      "Epoch 292/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 406.6010 - val_loss: 400.5539\n",
      "Epoch 293/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 405.9737 - val_loss: 400.3760\n",
      "Epoch 294/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 405.8490 - val_loss: 400.1912\n",
      "Epoch 295/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 405.4395 - val_loss: 400.0093\n",
      "Epoch 296/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 404.8640 - val_loss: 399.8269\n",
      "Epoch 297/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 404.9796 - val_loss: 399.6448\n",
      "Epoch 298/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 404.4184 - val_loss: 399.4527\n",
      "Epoch 299/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 404.3218 - val_loss: 399.2682\n",
      "Epoch 300/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 404.3013 - val_loss: 399.0740\n",
      "Epoch 301/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 404.4468 - val_loss: 398.8756\n",
      "Epoch 302/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 403.9320 - val_loss: 398.6819\n",
      "Epoch 303/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 403.8760 - val_loss: 398.4794\n",
      "Epoch 304/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 403.2117 - val_loss: 398.2668\n",
      "Epoch 305/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 403.6304 - val_loss: 398.0739\n",
      "Epoch 306/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 403.4790 - val_loss: 397.8714\n",
      "Epoch 307/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 402.8268 - val_loss: 397.6562\n",
      "Epoch 308/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 402.4908 - val_loss: 397.4462\n",
      "Epoch 309/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 402.6477 - val_loss: 397.2427\n",
      "Epoch 310/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 402.2079 - val_loss: 397.0204\n",
      "Epoch 311/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 401.2462 - val_loss: 396.7950\n",
      "Epoch 312/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 402.0764 - val_loss: 396.5723\n",
      "Epoch 313/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 401.8078 - val_loss: 396.3591\n",
      "Epoch 314/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 401.6397 - val_loss: 396.1319\n",
      "Epoch 315/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 401.6376 - val_loss: 395.9015\n",
      "Epoch 316/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 400.8963 - val_loss: 395.6759\n",
      "Epoch 317/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 400.4617 - val_loss: 395.4311\n",
      "Epoch 318/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 400.1744 - val_loss: 395.1827\n",
      "Epoch 319/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 400.1074 - val_loss: 394.9628\n",
      "Epoch 320/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 399.4289 - val_loss: 394.7219\n",
      "Epoch 321/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 399.5034 - val_loss: 394.4691\n",
      "Epoch 322/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 399.0772 - val_loss: 394.2176\n",
      "Epoch 323/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 398.5407 - val_loss: 393.9656\n",
      "Epoch 324/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 398.6440 - val_loss: 393.7007\n",
      "Epoch 325/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 398.4538 - val_loss: 393.4341\n",
      "Epoch 326/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 398.9755 - val_loss: 393.1845\n",
      "Epoch 327/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 397.4653 - val_loss: 392.9217\n",
      "Epoch 328/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 397.1796 - val_loss: 392.6403\n",
      "Epoch 329/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 397.0161 - val_loss: 392.3803\n",
      "Epoch 330/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 396.5475 - val_loss: 392.0903\n",
      "Epoch 331/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 395.7488 - val_loss: 391.8021\n",
      "Epoch 332/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 396.1822 - val_loss: 391.5313\n",
      "Epoch 333/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 395.0155 - val_loss: 391.2339\n",
      "Epoch 334/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 395.1529 - val_loss: 390.9353\n",
      "Epoch 335/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 395.5311 - val_loss: 390.6407\n",
      "Epoch 336/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 395.0083 - val_loss: 390.3366\n",
      "Epoch 337/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 394.6360 - val_loss: 390.0367\n",
      "Epoch 338/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 394.8170 - val_loss: 389.7362\n",
      "Epoch 339/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 393.9073 - val_loss: 389.4205\n",
      "Epoch 340/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 393.6161 - val_loss: 389.1039\n",
      "Epoch 341/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 393.2509 - val_loss: 388.7671\n",
      "Epoch 342/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 392.8613 - val_loss: 388.4310\n",
      "Epoch 343/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 392.1067 - val_loss: 388.1131\n",
      "Epoch 344/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 392.6647 - val_loss: 387.7841\n",
      "Epoch 345/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 391.9695 - val_loss: 387.4441\n",
      "Epoch 346/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 392.3640 - val_loss: 387.1180\n",
      "Epoch 347/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 391.3268 - val_loss: 386.7563\n",
      "Epoch 348/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 391.0807 - val_loss: 386.4153\n",
      "Epoch 349/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 390.3853 - val_loss: 386.0641\n",
      "Epoch 350/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 389.1163 - val_loss: 385.7014\n",
      "Epoch 351/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 390.5604 - val_loss: 385.3459\n",
      "Epoch 352/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 389.3579 - val_loss: 384.9659\n",
      "Epoch 353/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 388.8017 - val_loss: 384.5673\n",
      "Epoch 354/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 388.4084 - val_loss: 384.1968\n",
      "Epoch 355/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 387.1481 - val_loss: 383.8074\n",
      "Epoch 356/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 388.1230 - val_loss: 383.4185\n",
      "Epoch 357/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 386.2894 - val_loss: 383.0062\n",
      "Epoch 358/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 386.1700 - val_loss: 382.5983\n",
      "Epoch 359/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 386.3836 - val_loss: 382.1768\n",
      "Epoch 360/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 384.4583 - val_loss: 381.7714\n",
      "Epoch 361/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 385.0405 - val_loss: 381.3416\n",
      "Epoch 362/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 383.6590 - val_loss: 380.9187\n",
      "Epoch 363/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 383.9461 - val_loss: 380.4767\n",
      "Epoch 364/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 383.7264 - val_loss: 380.0491\n",
      "Epoch 365/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 383.0402 - val_loss: 379.5987\n",
      "Epoch 366/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 382.0992 - val_loss: 379.1418\n",
      "Epoch 367/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 382.1608 - val_loss: 378.6765\n",
      "Epoch 368/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 382.9275 - val_loss: 378.2092\n",
      "Epoch 369/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 380.8274 - val_loss: 377.7210\n",
      "Epoch 370/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 380.1362 - val_loss: 377.2520\n",
      "Epoch 371/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 378.6279 - val_loss: 376.7458\n",
      "Epoch 372/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 378.2543 - val_loss: 376.2483\n",
      "Epoch 373/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 379.4069 - val_loss: 375.7335\n",
      "Epoch 374/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 378.1931 - val_loss: 375.2381\n",
      "Epoch 375/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 377.9428 - val_loss: 374.7077\n",
      "Epoch 376/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 377.5628 - val_loss: 374.1994\n",
      "Epoch 377/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 376.4615 - val_loss: 373.6577\n",
      "Epoch 378/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 377.0347 - val_loss: 373.1052\n",
      "Epoch 379/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 375.0735 - val_loss: 372.5854\n",
      "Epoch 380/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 374.2002 - val_loss: 372.0425\n",
      "Epoch 381/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 373.1399 - val_loss: 371.4699\n",
      "Epoch 382/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 373.2854 - val_loss: 370.9324\n",
      "Epoch 383/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 372.5659 - val_loss: 370.3499\n",
      "Epoch 384/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 371.2157 - val_loss: 369.7670\n",
      "Epoch 385/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 371.9080 - val_loss: 369.1656\n",
      "Epoch 386/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 369.7803 - val_loss: 368.5520\n",
      "Epoch 387/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 369.6117 - val_loss: 367.9442\n",
      "Epoch 388/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 370.9591 - val_loss: 367.3614\n",
      "Epoch 389/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 367.6591 - val_loss: 366.7318\n",
      "Epoch 390/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 368.6181 - val_loss: 366.1143\n",
      "Epoch 391/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 367.5830 - val_loss: 365.4933\n",
      "Epoch 392/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 367.3072 - val_loss: 364.8225\n",
      "Epoch 393/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 367.0519 - val_loss: 364.2118\n",
      "Epoch 394/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 365.2805 - val_loss: 363.5493\n",
      "Epoch 395/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 364.9877 - val_loss: 362.8932\n",
      "Epoch 396/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 364.4757 - val_loss: 362.2193\n",
      "Epoch 397/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 363.2895 - val_loss: 361.5609\n",
      "Epoch 398/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 361.6572 - val_loss: 360.8755\n",
      "Epoch 399/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 362.2794 - val_loss: 360.1674\n",
      "Epoch 400/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 360.2987 - val_loss: 359.4825\n",
      "Epoch 401/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 359.5117 - val_loss: 358.7724\n",
      "Epoch 402/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 358.8885 - val_loss: 358.0698\n",
      "Epoch 403/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 358.7273 - val_loss: 357.3485\n",
      "Epoch 404/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 356.6486 - val_loss: 356.6350\n",
      "Epoch 405/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 355.4342 - val_loss: 355.9073\n",
      "Epoch 406/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 356.6170 - val_loss: 355.1725\n",
      "Epoch 407/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 355.0576 - val_loss: 354.4112\n",
      "Epoch 408/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 355.2142 - val_loss: 353.6276\n",
      "Epoch 409/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 354.1086 - val_loss: 352.8601\n",
      "Epoch 410/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 352.3933 - val_loss: 352.0559\n",
      "Epoch 411/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 351.8438 - val_loss: 351.2922\n",
      "Epoch 412/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 350.7382 - val_loss: 350.5325\n",
      "Epoch 413/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 349.2061 - val_loss: 349.7307\n",
      "Epoch 414/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 348.7453 - val_loss: 348.9361\n",
      "Epoch 415/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 348.1264 - val_loss: 348.1500\n",
      "Epoch 416/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 347.5540 - val_loss: 347.3234\n",
      "Epoch 417/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 345.7752 - val_loss: 346.4302\n",
      "Epoch 418/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 344.9636 - val_loss: 345.6195\n",
      "Epoch 419/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 345.1360 - val_loss: 344.8043\n",
      "Epoch 420/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 342.0742 - val_loss: 343.9162\n",
      "Epoch 421/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 342.8807 - val_loss: 343.0468\n",
      "Epoch 422/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 339.8821 - val_loss: 342.1984\n",
      "Epoch 423/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 338.8431 - val_loss: 341.2729\n",
      "Epoch 424/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 339.5484 - val_loss: 340.3847\n",
      "Epoch 425/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 337.3615 - val_loss: 339.4934\n",
      "Epoch 426/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 338.6443 - val_loss: 338.6046\n",
      "Epoch 427/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 339.0401 - val_loss: 337.6912\n",
      "Epoch 428/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 336.1016 - val_loss: 336.7585\n",
      "Epoch 429/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 335.0836 - val_loss: 335.8594\n",
      "Epoch 430/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 331.8978 - val_loss: 334.8460\n",
      "Epoch 431/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 332.0526 - val_loss: 333.9078\n",
      "Epoch 432/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 329.7879 - val_loss: 332.9232\n",
      "Epoch 433/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 330.2841 - val_loss: 331.9348\n",
      "Epoch 434/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 330.9927 - val_loss: 330.9277\n",
      "Epoch 435/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 327.5400 - val_loss: 329.9076\n",
      "Epoch 436/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 327.4206 - val_loss: 328.8955\n",
      "Epoch 437/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 324.9586 - val_loss: 327.9333\n",
      "Epoch 438/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 323.7132 - val_loss: 326.9078\n",
      "Epoch 439/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 323.9308 - val_loss: 325.8978\n",
      "Epoch 440/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 322.9059 - val_loss: 324.8214\n",
      "Epoch 441/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 320.2617 - val_loss: 323.7946\n",
      "Epoch 442/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 321.3707 - val_loss: 322.7634\n",
      "Epoch 443/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 317.4059 - val_loss: 321.7094\n",
      "Epoch 444/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 316.8542 - val_loss: 320.5493\n",
      "Epoch 445/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 316.2315 - val_loss: 319.4595\n",
      "Epoch 446/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 314.1263 - val_loss: 318.3947\n",
      "Epoch 447/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 311.4576 - val_loss: 317.2982\n",
      "Epoch 448/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 312.1522 - val_loss: 316.2917\n",
      "Epoch 449/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 310.4060 - val_loss: 315.0988\n",
      "Epoch 450/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 311.0758 - val_loss: 313.9542\n",
      "Epoch 451/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 306.5068 - val_loss: 312.7777\n",
      "Epoch 452/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 308.5741 - val_loss: 311.5682\n",
      "Epoch 453/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 306.1782 - val_loss: 310.4563\n",
      "Epoch 454/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 302.2648 - val_loss: 309.2296\n",
      "Epoch 455/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 303.5032 - val_loss: 308.0025\n",
      "Epoch 456/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 301.1042 - val_loss: 306.7611\n",
      "Epoch 457/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 300.4207 - val_loss: 305.6357\n",
      "Epoch 458/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 299.4375 - val_loss: 304.5432\n",
      "Epoch 459/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 298.9973 - val_loss: 303.1623\n",
      "Epoch 460/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 296.0060 - val_loss: 301.8087\n",
      "Epoch 461/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 297.8784 - val_loss: 300.4729\n",
      "Epoch 462/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 291.7760 - val_loss: 299.2874\n",
      "Epoch 463/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 296.2742 - val_loss: 297.9691\n",
      "Epoch 464/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 292.4046 - val_loss: 296.5977\n",
      "Epoch 465/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 290.8252 - val_loss: 295.4378\n",
      "Epoch 466/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 287.1282 - val_loss: 294.2676\n",
      "Epoch 467/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 286.4481 - val_loss: 292.8147\n",
      "Epoch 468/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 286.6936 - val_loss: 291.4984\n",
      "Epoch 469/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 282.0211 - val_loss: 290.1309\n",
      "Epoch 470/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 283.8646 - val_loss: 288.8911\n",
      "Epoch 471/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 280.8469 - val_loss: 287.3319\n",
      "Epoch 472/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 274.7808 - val_loss: 285.8672\n",
      "Epoch 473/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 276.5681 - val_loss: 284.4831\n",
      "Epoch 474/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 279.4529 - val_loss: 283.1011\n",
      "Epoch 475/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 276.3762 - val_loss: 281.7163\n",
      "Epoch 476/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 278.4079 - val_loss: 280.1656\n",
      "Epoch 477/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 270.7623 - val_loss: 279.0057\n",
      "Epoch 478/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 268.8891 - val_loss: 277.6848\n",
      "Epoch 479/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 271.3191 - val_loss: 275.9685\n",
      "Epoch 480/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 270.5054 - val_loss: 274.5176\n",
      "Epoch 481/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 268.9425 - val_loss: 273.1599\n",
      "Epoch 482/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 265.0620 - val_loss: 271.8553\n",
      "Epoch 483/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 265.8105 - val_loss: 270.3155\n",
      "Epoch 484/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 262.1587 - val_loss: 269.1314\n",
      "Epoch 485/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 260.2613 - val_loss: 267.7157\n",
      "Epoch 486/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 260.4490 - val_loss: 266.2296\n",
      "Epoch 487/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 261.4737 - val_loss: 264.9196\n",
      "Epoch 488/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 259.0174 - val_loss: 263.9720\n",
      "Epoch 489/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 258.7670 - val_loss: 262.2699\n",
      "Epoch 490/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 255.9345 - val_loss: 261.0364\n",
      "Epoch 491/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 255.3931 - val_loss: 259.9704\n",
      "Epoch 492/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 254.7344 - val_loss: 258.8196\n",
      "Epoch 493/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 253.1478 - val_loss: 257.8826\n",
      "Epoch 494/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 255.2420 - val_loss: 256.7726\n",
      "Epoch 495/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 253.7973 - val_loss: 255.1431\n",
      "Epoch 496/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 253.1917 - val_loss: 254.5577\n",
      "Epoch 497/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 252.0182 - val_loss: 253.3035\n",
      "Epoch 498/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 250.1854 - val_loss: 252.4262\n",
      "Epoch 499/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 250.7460 - val_loss: 251.2063\n",
      "Epoch 500/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 249.9980 - val_loss: 249.9858\n",
      "Epoch 501/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 250.8341 - val_loss: 249.0603\n",
      "Epoch 502/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 247.6278 - val_loss: 248.2268\n",
      "Epoch 503/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 250.1249 - val_loss: 247.1890\n",
      "Epoch 504/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 249.1377 - val_loss: 246.3489\n",
      "Epoch 505/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 249.0387 - val_loss: 245.8420\n",
      "Epoch 506/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 242.3630 - val_loss: 245.1154\n",
      "Epoch 507/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 245.7412 - val_loss: 244.4429\n",
      "Epoch 508/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 245.5702 - val_loss: 243.8514\n",
      "Epoch 509/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 244.9287 - val_loss: 243.4141\n",
      "Epoch 510/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 246.6068 - val_loss: 243.1289\n",
      "Epoch 511/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 244.6687 - val_loss: 242.3986\n",
      "Epoch 512/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 245.7462 - val_loss: 242.3717\n",
      "Epoch 513/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 244.1259 - val_loss: 241.4973\n",
      "Epoch 514/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 243.0254 - val_loss: 241.3129\n",
      "Epoch 515/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 244.4751 - val_loss: 240.7682\n",
      "Epoch 516/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 243.4926 - val_loss: 240.2917\n",
      "Epoch 517/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 241.7267 - val_loss: 239.9885\n",
      "Epoch 518/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 244.6489 - val_loss: 239.4490\n",
      "Epoch 519/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 242.4829 - val_loss: 239.1658\n",
      "Epoch 520/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 242.3383 - val_loss: 238.7408\n",
      "Epoch 521/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 243.9149 - val_loss: 238.8673\n",
      "Epoch 522/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 242.2362 - val_loss: 238.5091\n",
      "Epoch 523/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 241.7039 - val_loss: 238.1638\n",
      "Epoch 524/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 240.8652 - val_loss: 237.8343\n",
      "Epoch 525/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 239.8050 - val_loss: 237.5765\n",
      "Epoch 526/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 240.4831 - val_loss: 237.6973\n",
      "Epoch 527/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 241.3020 - val_loss: 237.0592\n",
      "Epoch 528/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 238.0731 - val_loss: 237.0305\n",
      "Epoch 529/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 240.6229 - val_loss: 236.5704\n",
      "Epoch 530/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 238.2789 - val_loss: 236.8034\n",
      "Epoch 531/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 241.6420 - val_loss: 236.2715\n",
      "Epoch 532/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 240.0903 - val_loss: 236.0396\n",
      "Epoch 533/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 238.6586 - val_loss: 236.0246\n",
      "Epoch 534/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 238.7599 - val_loss: 235.7266\n",
      "Epoch 535/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 236.6320 - val_loss: 235.5715\n",
      "Epoch 536/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 235.1708 - val_loss: 235.5041\n",
      "Epoch 537/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 234.0358 - val_loss: 235.2707\n",
      "Epoch 538/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 238.8035 - val_loss: 235.1862\n",
      "Epoch 539/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 237.3744 - val_loss: 234.8910\n",
      "Epoch 540/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 238.9829 - val_loss: 234.6279\n",
      "Epoch 541/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 236.5710 - val_loss: 234.5702\n",
      "Epoch 542/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 238.8505 - val_loss: 234.5305\n",
      "Epoch 543/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 238.2248 - val_loss: 234.0238\n",
      "Epoch 544/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 235.7686 - val_loss: 234.3336\n",
      "Epoch 545/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 236.7477 - val_loss: 233.7926\n",
      "Epoch 546/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 233.1372 - val_loss: 233.4416\n",
      "Epoch 547/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 236.4796 - val_loss: 233.7881\n",
      "Epoch 548/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 236.0846 - val_loss: 233.3587\n",
      "Epoch 549/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 235.9973 - val_loss: 233.4088\n",
      "Epoch 550/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 238.0059 - val_loss: 233.0335\n",
      "Epoch 551/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 234.7156 - val_loss: 233.1578\n",
      "Epoch 552/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 231.2959 - val_loss: 232.6342\n",
      "Epoch 553/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 234.3941 - val_loss: 232.4772\n",
      "Epoch 554/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 234.2956 - val_loss: 232.6320\n",
      "Epoch 555/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 234.1282 - val_loss: 232.3460\n",
      "Epoch 556/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 230.9324 - val_loss: 232.5682\n",
      "Epoch 557/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 233.8327 - val_loss: 232.4913\n",
      "Epoch 558/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 233.4329 - val_loss: 232.0208\n",
      "Epoch 559/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 233.0930 - val_loss: 231.9928\n",
      "Epoch 560/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 230.6055 - val_loss: 231.7611\n",
      "Epoch 561/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 234.3351 - val_loss: 231.4544\n",
      "Epoch 562/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 229.4926 - val_loss: 231.7898\n",
      "Epoch 563/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 227.6401 - val_loss: 231.4667\n",
      "Epoch 564/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 231.6096 - val_loss: 231.4840\n",
      "Epoch 565/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 230.4476 - val_loss: 231.3019\n",
      "Epoch 566/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 229.9478 - val_loss: 230.8148\n",
      "Epoch 567/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 233.3726 - val_loss: 230.8642\n",
      "Epoch 568/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 232.3400 - val_loss: 230.9420\n",
      "Epoch 569/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 233.3472 - val_loss: 230.9447\n",
      "Epoch 570/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 231.8410 - val_loss: 230.6008\n",
      "Epoch 571/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 231.2619 - val_loss: 230.0824\n",
      "Epoch 572/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 231.5081 - val_loss: 230.5843\n",
      "Epoch 573/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 231.4469 - val_loss: 230.0120\n",
      "Epoch 574/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 230.6802 - val_loss: 230.7269\n",
      "Epoch 575/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 230.8678 - val_loss: 229.6573\n",
      "Epoch 576/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 228.4965 - val_loss: 230.0949\n",
      "Epoch 577/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 230.2735 - val_loss: 229.6686\n",
      "Epoch 578/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 228.5048 - val_loss: 230.0585\n",
      "Epoch 579/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 230.1606 - val_loss: 229.5936\n",
      "Epoch 580/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 229.7588 - val_loss: 229.5649\n",
      "Epoch 581/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 233.1727 - val_loss: 229.2840\n",
      "Epoch 582/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 226.9447 - val_loss: 229.3623\n",
      "Epoch 583/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 230.5485 - val_loss: 228.7670\n",
      "Epoch 584/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 226.9427 - val_loss: 228.8530\n",
      "Epoch 585/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 226.3993 - val_loss: 228.7182\n",
      "Epoch 586/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 229.9307 - val_loss: 228.5092\n",
      "Epoch 587/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 228.7709 - val_loss: 228.9785\n",
      "Epoch 588/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 230.5795 - val_loss: 228.8776\n",
      "Epoch 589/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 228.5643 - val_loss: 228.8204\n",
      "Epoch 590/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 227.3683 - val_loss: 228.3682\n",
      "Epoch 591/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 226.2161 - val_loss: 228.6279\n",
      "Epoch 592/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 227.3515 - val_loss: 228.7818\n",
      "Epoch 593/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 228.7104 - val_loss: 229.1247\n",
      "Epoch 594/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 227.2496 - val_loss: 229.0145\n",
      "Epoch 595/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 225.5055 - val_loss: 228.5596\n",
      "Epoch 596/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 225.0038 - val_loss: 228.5648\n",
      "Epoch 597/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 227.9792 - val_loss: 228.4703\n",
      "Epoch 598/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 227.7002 - val_loss: 228.3870\n",
      "Epoch 599/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 226.0260 - val_loss: 228.7348\n",
      "Epoch 600/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 223.6261 - val_loss: 228.2720\n",
      "Epoch 601/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 221.3966 - val_loss: 228.6602\n",
      "Epoch 602/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 227.8898 - val_loss: 229.4118\n",
      "Epoch 603/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 222.3125 - val_loss: 228.4181\n",
      "Epoch 604/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 227.3933 - val_loss: 228.6836\n",
      "Epoch 605/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 224.6711 - val_loss: 228.6403\n",
      "Epoch 606/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 224.4426 - val_loss: 228.0433\n",
      "Epoch 607/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 224.5194 - val_loss: 227.9629\n",
      "Epoch 608/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 218.3009 - val_loss: 228.3292\n",
      "Epoch 609/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 224.7426 - val_loss: 228.0517\n",
      "Epoch 610/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 223.8192 - val_loss: 228.4413\n",
      "Epoch 611/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 224.3033 - val_loss: 227.5404\n",
      "Epoch 612/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 224.1845 - val_loss: 228.6406\n",
      "Epoch 613/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 221.7914 - val_loss: 227.7446\n",
      "Epoch 614/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 223.5724 - val_loss: 227.6115\n",
      "Epoch 615/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 221.6853 - val_loss: 228.3245\n",
      "Epoch 616/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 219.9270 - val_loss: 227.5626\n",
      "Epoch 617/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 220.5474 - val_loss: 228.7986\n",
      "Epoch 618/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 220.8660 - val_loss: 228.6449\n",
      "Epoch 619/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 223.2001 - val_loss: 227.9898\n",
      "Epoch 620/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 219.4312 - val_loss: 228.1339\n",
      "Epoch 621/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 222.8527 - val_loss: 228.2112\n",
      "Epoch 622/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 218.2401 - val_loss: 227.8810\n",
      "Epoch 623/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 222.7696 - val_loss: 227.9405\n",
      "Epoch 624/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 223.2097 - val_loss: 227.7014\n",
      "Epoch 625/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 219.8018 - val_loss: 228.5932\n",
      "Epoch 626/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 218.6665 - val_loss: 228.0882\n",
      "Epoch 627/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 216.7346 - val_loss: 228.6268\n",
      "Epoch 628/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 222.1747 - val_loss: 228.4356\n",
      "Epoch 629/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 220.5081 - val_loss: 228.3367\n",
      "Epoch 630/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 219.8450 - val_loss: 228.0499\n",
      "Epoch 631/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 219.8773 - val_loss: 228.3640\n",
      "Epoch 632/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 217.6602 - val_loss: 227.6073\n",
      "Epoch 633/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 218.6049 - val_loss: 227.6650\n",
      "Epoch 634/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 218.8180 - val_loss: 228.2004\n",
      "Epoch 635/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 219.9330 - val_loss: 227.8046\n",
      "Epoch 636/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 218.3676 - val_loss: 228.0592\n",
      "Epoch 637/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 218.7671 - val_loss: 226.9164\n",
      "Epoch 638/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 218.3276 - val_loss: 227.5379\n",
      "Epoch 639/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 216.1766 - val_loss: 227.6517\n",
      "Epoch 640/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 213.9041 - val_loss: 227.9092\n",
      "Epoch 641/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 219.5879 - val_loss: 227.4375\n",
      "Epoch 642/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 218.1675 - val_loss: 227.6974\n",
      "Epoch 643/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 213.9786 - val_loss: 227.5933\n",
      "Epoch 644/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 216.6985 - val_loss: 227.1139\n",
      "Epoch 645/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 217.5738 - val_loss: 228.5879\n",
      "Epoch 646/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 217.5068 - val_loss: 227.7505\n",
      "Epoch 647/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 217.2249 - val_loss: 226.3292\n",
      "Epoch 648/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 215.4484 - val_loss: 227.4197\n",
      "Epoch 649/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 219.0352 - val_loss: 227.8999\n",
      "Epoch 650/2000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 214.4619 - val_loss: 228.4872\n",
      "Epoch 651/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 212.4106 - val_loss: 226.9443\n",
      "Epoch 652/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 214.1288 - val_loss: 227.8282\n",
      "Epoch 653/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 213.9360 - val_loss: 227.3198\n",
      "Epoch 654/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 216.6529 - val_loss: 227.0832\n",
      "Epoch 655/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 213.7536 - val_loss: 227.3996\n",
      "Epoch 656/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 216.6095 - val_loss: 227.8714\n",
      "Epoch 657/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 214.6302 - val_loss: 226.7783\n",
      "Epoch 658/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 214.9955 - val_loss: 226.7392\n",
      "Epoch 659/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 211.8284 - val_loss: 227.6572\n",
      "Epoch 660/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 213.3777 - val_loss: 227.6954\n",
      "Epoch 661/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 216.1107 - val_loss: 227.2497\n",
      "Epoch 662/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 213.4266 - val_loss: 227.6970\n",
      "Epoch 663/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 212.8778 - val_loss: 228.1604\n",
      "Epoch 664/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 211.3653 - val_loss: 228.3705\n",
      "Epoch 665/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 212.9511 - val_loss: 227.9863\n",
      "Epoch 666/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 217.9835 - val_loss: 228.0617\n",
      "Epoch 667/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 213.1550 - val_loss: 227.7189\n",
      "Epoch 668/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 212.7827 - val_loss: 228.0244\n",
      "Epoch 669/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 212.1564 - val_loss: 228.3032\n",
      "Epoch 670/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 213.4018 - val_loss: 226.9982\n",
      "Epoch 671/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 209.7974 - val_loss: 227.3674\n",
      "Epoch 672/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 213.8442 - val_loss: 228.4478\n",
      "Epoch 673/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 208.4807 - val_loss: 227.1675\n",
      "Epoch 674/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 212.5916 - val_loss: 225.7806\n",
      "Epoch 675/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 211.2252 - val_loss: 227.8547\n",
      "Epoch 676/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 210.1015 - val_loss: 227.1273\n",
      "Epoch 677/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 207.2814 - val_loss: 229.3032\n",
      "Epoch 678/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 209.9784 - val_loss: 227.8960\n",
      "Epoch 679/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 210.8271 - val_loss: 227.9545\n",
      "Epoch 680/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 213.4139 - val_loss: 228.1659\n",
      "Epoch 681/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 213.2500 - val_loss: 228.0326\n",
      "Epoch 682/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 210.5060 - val_loss: 227.5988\n",
      "Epoch 683/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 209.8630 - val_loss: 227.4136\n",
      "Epoch 684/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 206.5474 - val_loss: 227.5234\n",
      "Epoch 685/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 210.8528 - val_loss: 227.5446\n",
      "Epoch 686/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 209.1974 - val_loss: 228.5565\n",
      "Epoch 687/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 212.8551 - val_loss: 229.0527\n",
      "Epoch 688/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 210.2025 - val_loss: 227.7233\n",
      "Epoch 689/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 207.7180 - val_loss: 226.2111\n",
      "Epoch 690/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 208.8474 - val_loss: 228.4967\n",
      "Epoch 691/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 208.8359 - val_loss: 227.7364\n",
      "Epoch 692/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 205.1503 - val_loss: 228.3908\n",
      "Epoch 693/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 209.7260 - val_loss: 227.2497\n",
      "Epoch 694/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 209.2075 - val_loss: 227.5365\n",
      "Epoch 695/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 209.6716 - val_loss: 227.5663\n",
      "Epoch 696/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 205.3575 - val_loss: 227.0795\n",
      "Epoch 697/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 204.9099 - val_loss: 228.2439\n",
      "Epoch 698/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 206.4291 - val_loss: 228.2582\n",
      "Epoch 699/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 206.2001 - val_loss: 229.8124\n",
      "Epoch 700/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 205.2401 - val_loss: 229.3888\n",
      "Epoch 701/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 205.7172 - val_loss: 228.3502\n",
      "Epoch 702/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 209.5072 - val_loss: 227.7964\n",
      "Epoch 703/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 200.9445 - val_loss: 229.1485\n",
      "Epoch 704/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 204.5920 - val_loss: 227.5395\n",
      "Epoch 705/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 205.5545 - val_loss: 227.9769\n",
      "Epoch 706/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 208.2006 - val_loss: 229.0666\n",
      "Epoch 707/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 205.0490 - val_loss: 229.2034\n",
      "Epoch 708/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 206.5973 - val_loss: 228.0339\n",
      "Epoch 709/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 206.0999 - val_loss: 229.4065\n",
      "Epoch 710/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 208.0670 - val_loss: 227.9513\n",
      "Epoch 711/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 202.7952 - val_loss: 228.9670\n",
      "Epoch 712/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 205.7753 - val_loss: 228.4686\n",
      "Epoch 713/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 207.7580 - val_loss: 229.0644\n",
      "Epoch 714/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 207.4573 - val_loss: 230.1956\n",
      "Epoch 715/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 201.6878 - val_loss: 231.3748\n",
      "Epoch 716/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 203.9950 - val_loss: 229.8848\n",
      "Epoch 717/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 201.4458 - val_loss: 226.7085\n",
      "Epoch 718/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 204.7589 - val_loss: 229.0269\n",
      "Epoch 719/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 201.6331 - val_loss: 230.1328\n",
      "Epoch 720/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 204.6680 - val_loss: 230.3796\n",
      "Epoch 721/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 204.5725 - val_loss: 231.2013\n",
      "Epoch 722/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 203.6425 - val_loss: 228.6415\n",
      "Epoch 723/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 202.3118 - val_loss: 228.4974\n",
      "Epoch 724/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 202.8912 - val_loss: 227.7823\n",
      "Epoch 725/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 201.8849 - val_loss: 228.2913\n",
      "Epoch 726/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 199.5597 - val_loss: 230.2622\n",
      "Epoch 727/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 199.9972 - val_loss: 227.3572\n",
      "Epoch 728/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 201.9361 - val_loss: 227.6293\n",
      "Epoch 729/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 197.6015 - val_loss: 228.7060\n",
      "Epoch 730/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 203.2902 - val_loss: 229.8010\n",
      "Epoch 731/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 205.9255 - val_loss: 227.8986\n",
      "Epoch 732/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 198.8603 - val_loss: 230.6966\n",
      "Epoch 733/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 203.4540 - val_loss: 228.1555\n",
      "Epoch 734/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 199.5316 - val_loss: 226.8980\n",
      "Epoch 735/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 198.8784 - val_loss: 228.0080\n",
      "Epoch 736/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 204.9282 - val_loss: 228.3383\n",
      "Epoch 737/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 203.3039 - val_loss: 228.3455\n",
      "Epoch 738/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 201.1485 - val_loss: 228.5294\n",
      "Epoch 739/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 203.9398 - val_loss: 227.7500\n",
      "Epoch 740/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 199.0228 - val_loss: 227.9510\n",
      "Epoch 741/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 200.7449 - val_loss: 227.7099\n",
      "Epoch 742/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 202.9530 - val_loss: 228.2580\n",
      "Epoch 743/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 199.8131 - val_loss: 228.0834\n",
      "Epoch 744/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 198.9896 - val_loss: 227.7806\n",
      "Epoch 745/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 194.7769 - val_loss: 231.0312\n",
      "Epoch 746/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 198.7757 - val_loss: 227.1237\n",
      "Epoch 747/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 202.6433 - val_loss: 227.9066\n",
      "Epoch 748/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 199.9362 - val_loss: 229.0442\n",
      "Epoch 749/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 200.5542 - val_loss: 227.8299\n",
      "Epoch 750/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 198.8343 - val_loss: 228.5831\n",
      "Epoch 751/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 198.2995 - val_loss: 228.6926\n",
      "Epoch 752/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 200.7520 - val_loss: 230.2314\n",
      "Epoch 753/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 195.1842 - val_loss: 228.4090\n",
      "Epoch 754/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 198.6696 - val_loss: 229.0858\n",
      "Epoch 755/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 198.0467 - val_loss: 228.4754\n",
      "Epoch 756/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 198.6288 - val_loss: 227.8816\n",
      "Epoch 757/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 200.2462 - val_loss: 229.1139\n",
      "Epoch 758/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 198.3233 - val_loss: 230.5082\n",
      "Epoch 759/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 197.7681 - val_loss: 230.1822\n",
      "Epoch 760/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 196.6713 - val_loss: 230.1494\n",
      "Epoch 761/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 200.0436 - val_loss: 230.9852\n",
      "Epoch 762/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 198.7122 - val_loss: 231.0003\n",
      "Epoch 763/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 200.6823 - val_loss: 231.5198\n",
      "Epoch 764/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 198.2592 - val_loss: 231.7313\n",
      "Epoch 765/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 196.3529 - val_loss: 229.7039\n",
      "Epoch 766/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 196.9973 - val_loss: 229.3645\n",
      "Epoch 767/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 199.9874 - val_loss: 229.2188\n",
      "Epoch 768/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 196.2187 - val_loss: 231.5062\n",
      "Epoch 769/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 196.3051 - val_loss: 230.3752\n",
      "Epoch 770/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 196.9585 - val_loss: 229.6044\n",
      "Epoch 771/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 200.9456 - val_loss: 230.7996\n",
      "Epoch 772/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 200.3052 - val_loss: 228.0028\n",
      "Epoch 773/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 198.3362 - val_loss: 229.6105\n",
      "Epoch 774/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 195.3760 - val_loss: 231.5416\n",
      "Epoch 775/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 194.2289 - val_loss: 230.5619\n",
      "Epoch 776/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 197.3004 - val_loss: 229.1519\n",
      "Epoch 777/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 196.6186 - val_loss: 228.8550\n",
      "Epoch 778/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 195.3853 - val_loss: 228.7962\n",
      "Epoch 779/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 194.0324 - val_loss: 230.8102\n",
      "Epoch 780/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 197.2568 - val_loss: 229.7530\n",
      "Epoch 781/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 199.4243 - val_loss: 230.6556\n",
      "Epoch 782/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 192.2281 - val_loss: 229.2109\n",
      "Epoch 783/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 193.6367 - val_loss: 228.5412\n",
      "Epoch 784/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 190.9178 - val_loss: 229.2738\n",
      "Epoch 785/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 193.0389 - val_loss: 229.0466\n",
      "Epoch 786/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 196.8739 - val_loss: 228.1498\n",
      "Epoch 787/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 197.3197 - val_loss: 227.5122\n",
      "Epoch 788/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 194.5295 - val_loss: 230.7044\n",
      "Epoch 789/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 193.5300 - val_loss: 229.8376\n",
      "Epoch 790/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 197.3644 - val_loss: 229.5874\n",
      "Epoch 791/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 190.9237 - val_loss: 228.0471\n",
      "Epoch 792/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 192.7622 - val_loss: 227.3061\n",
      "Epoch 793/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 194.6341 - val_loss: 229.9333\n",
      "Epoch 794/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 198.2892 - val_loss: 229.1909\n",
      "Epoch 795/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 193.5409 - val_loss: 228.5893\n",
      "Epoch 796/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 197.6270 - val_loss: 228.2164\n",
      "Epoch 797/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 190.7668 - val_loss: 229.6283\n",
      "Epoch 798/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 189.3109 - val_loss: 230.2304\n",
      "Epoch 799/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 194.6383 - val_loss: 230.0973\n",
      "Epoch 800/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 193.4710 - val_loss: 227.1378\n",
      "Epoch 801/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 190.4870 - val_loss: 226.7383\n",
      "Epoch 802/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 196.7338 - val_loss: 230.3708\n",
      "Epoch 803/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 194.4252 - val_loss: 228.4395\n",
      "Epoch 804/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 193.9430 - val_loss: 228.7846\n",
      "Epoch 805/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 193.8493 - val_loss: 229.0093\n",
      "Epoch 806/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 198.9086 - val_loss: 229.6163\n",
      "Epoch 807/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 196.5567 - val_loss: 228.8000\n",
      "Epoch 808/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 194.6551 - val_loss: 228.2601\n",
      "Epoch 809/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 190.0627 - val_loss: 229.1228\n",
      "Epoch 810/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 199.7294 - val_loss: 228.6757\n",
      "Epoch 811/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 190.1234 - val_loss: 229.2832\n",
      "Epoch 812/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 192.1442 - val_loss: 232.6986\n",
      "Epoch 813/2000\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 189.5345 - val_loss: 230.3507\n",
      "Epoch 814/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 186.6545 - val_loss: 230.0222\n",
      "Epoch 815/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 191.3138 - val_loss: 228.7350\n",
      "Epoch 816/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 193.9695 - val_loss: 232.8055\n",
      "Epoch 817/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 189.0967 - val_loss: 229.6464\n",
      "Epoch 818/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 188.6445 - val_loss: 230.9786\n",
      "Epoch 819/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 191.9478 - val_loss: 230.6827\n",
      "Epoch 820/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 196.0539 - val_loss: 230.6574\n",
      "Epoch 821/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 187.3994 - val_loss: 233.5956\n",
      "Epoch 822/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 189.5165 - val_loss: 231.5254\n",
      "Epoch 823/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 192.5558 - val_loss: 231.4850\n",
      "Epoch 824/2000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 188.5570 - val_loss: 231.6269\n",
      "Epoch 825/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 191.3730 - val_loss: 231.4139\n",
      "Epoch 826/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 190.4581 - val_loss: 231.7998\n",
      "Epoch 827/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 188.3146 - val_loss: 229.1821\n",
      "Epoch 828/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 186.4533 - val_loss: 230.1147\n",
      "Epoch 829/2000\n",
      "14/14 [==============================] - 0s 36ms/step - loss: 194.0141 - val_loss: 230.2482\n",
      "Epoch 830/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 190.6990 - val_loss: 227.0988\n",
      "Epoch 831/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 191.0845 - val_loss: 228.6817\n",
      "Epoch 832/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 191.5530 - val_loss: 230.0804\n",
      "Epoch 833/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 190.7186 - val_loss: 233.4254\n",
      "Epoch 834/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 192.0645 - val_loss: 231.6975\n",
      "Epoch 835/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 189.3505 - val_loss: 229.9932\n",
      "Epoch 836/2000\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 191.2230 - val_loss: 230.8625\n",
      "Epoch 837/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 192.2327 - val_loss: 232.1388\n",
      "Epoch 838/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 189.7132 - val_loss: 228.5696\n",
      "Epoch 839/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 189.3264 - val_loss: 230.0043\n",
      "Epoch 840/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 186.1027 - val_loss: 230.5122\n",
      "Epoch 841/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 193.4535 - val_loss: 230.0225\n",
      "Epoch 842/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 186.5006 - val_loss: 230.2216\n",
      "Epoch 843/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 188.8512 - val_loss: 234.6817\n",
      "Epoch 844/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 188.7517 - val_loss: 230.2577\n",
      "Epoch 845/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 189.7858 - val_loss: 230.7328\n",
      "Epoch 846/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 188.7042 - val_loss: 233.8077\n",
      "Epoch 847/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 186.2281 - val_loss: 231.7225\n",
      "Epoch 848/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 192.0567 - val_loss: 228.7407\n",
      "Epoch 849/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 187.6306 - val_loss: 231.8420\n",
      "Epoch 850/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 191.6261 - val_loss: 230.8078\n",
      "Epoch 851/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 187.9147 - val_loss: 229.8735\n",
      "Epoch 852/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 186.0810 - val_loss: 230.2814\n",
      "Epoch 853/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 183.5711 - val_loss: 232.5739\n",
      "Epoch 854/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 188.3349 - val_loss: 231.0486\n",
      "Epoch 855/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 188.4653 - val_loss: 230.8870\n",
      "Epoch 856/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 192.1758 - val_loss: 230.4863\n",
      "Epoch 857/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 191.5107 - val_loss: 231.4034\n",
      "Epoch 858/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 190.1127 - val_loss: 231.9746\n",
      "Epoch 859/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 193.0921 - val_loss: 229.2011\n",
      "Epoch 860/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 185.8228 - val_loss: 230.5845\n",
      "Epoch 861/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 188.9454 - val_loss: 232.3665\n",
      "Epoch 862/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 186.4484 - val_loss: 231.8890\n",
      "Epoch 863/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 187.0915 - val_loss: 229.5905\n",
      "Epoch 864/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 184.6996 - val_loss: 231.7306\n",
      "Epoch 865/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 187.0659 - val_loss: 232.3157\n",
      "Epoch 866/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 186.9661 - val_loss: 228.1613\n",
      "Epoch 867/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 185.4292 - val_loss: 231.6341\n",
      "Epoch 868/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 188.8982 - val_loss: 232.7663\n",
      "Epoch 869/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 187.8792 - val_loss: 229.1314\n",
      "Epoch 870/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 188.6713 - val_loss: 229.4025\n",
      "Epoch 871/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 187.0297 - val_loss: 231.7225\n",
      "Epoch 872/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 184.7665 - val_loss: 232.2699\n",
      "Epoch 873/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 190.4713 - val_loss: 229.9913\n",
      "Epoch 874/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 184.3167 - val_loss: 228.0642\n",
      "Epoch 875/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 185.0517 - val_loss: 231.2475\n",
      "Epoch 876/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 183.0325 - val_loss: 230.2960\n",
      "Epoch 877/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 189.5912 - val_loss: 230.9698\n",
      "Epoch 878/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 185.6898 - val_loss: 230.4510\n",
      "Epoch 879/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 185.5454 - val_loss: 230.3365\n",
      "Epoch 880/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 185.6936 - val_loss: 232.2123\n",
      "Epoch 881/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 180.7110 - val_loss: 230.6401\n",
      "Epoch 882/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 182.0078 - val_loss: 231.2493\n",
      "Epoch 883/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 187.1548 - val_loss: 231.7678\n",
      "Epoch 884/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 187.4746 - val_loss: 229.2888\n",
      "Epoch 885/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 191.3265 - val_loss: 231.1815\n",
      "Epoch 886/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 187.6327 - val_loss: 231.6800\n",
      "Epoch 887/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 185.1213 - val_loss: 229.2676\n",
      "Epoch 888/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 185.4560 - val_loss: 229.7164\n",
      "Epoch 889/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 188.1047 - val_loss: 229.0554\n",
      "Epoch 890/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 187.3177 - val_loss: 229.2830\n",
      "Epoch 891/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 187.7108 - val_loss: 228.9766\n",
      "Epoch 892/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 185.8070 - val_loss: 230.7430\n",
      "Epoch 893/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 182.8676 - val_loss: 229.8676\n",
      "Epoch 894/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 189.4548 - val_loss: 229.4984\n",
      "Epoch 895/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 182.0749 - val_loss: 230.6230\n",
      "Epoch 896/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 181.4224 - val_loss: 230.3127\n",
      "Epoch 897/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 184.0035 - val_loss: 229.3840\n",
      "Epoch 898/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 186.6406 - val_loss: 230.5356\n",
      "Epoch 899/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 186.8246 - val_loss: 229.6984\n",
      "Epoch 900/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 186.7376 - val_loss: 232.8768\n",
      "Epoch 901/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 183.7542 - val_loss: 232.3815\n",
      "Epoch 902/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 185.8893 - val_loss: 233.5932\n",
      "Epoch 903/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 183.9967 - val_loss: 230.6302\n",
      "Epoch 904/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 185.8181 - val_loss: 230.3314\n",
      "Epoch 905/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 188.5509 - val_loss: 228.3906\n",
      "Epoch 906/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 184.3603 - val_loss: 230.1887\n",
      "Epoch 907/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 186.8892 - val_loss: 227.9993\n",
      "Epoch 908/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 187.4282 - val_loss: 227.6778\n",
      "Epoch 909/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 184.9397 - val_loss: 228.0566\n",
      "Epoch 910/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 181.3533 - val_loss: 230.0105\n",
      "Epoch 911/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 183.7875 - val_loss: 230.7538\n",
      "Epoch 912/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 186.5393 - val_loss: 231.8641\n",
      "Epoch 913/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 182.1954 - val_loss: 230.9862\n",
      "Epoch 914/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 183.4785 - val_loss: 230.2266\n",
      "Epoch 915/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 182.8186 - val_loss: 230.1550\n",
      "Epoch 916/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 182.0665 - val_loss: 231.8865\n",
      "Epoch 917/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 184.7946 - val_loss: 230.2601\n",
      "Epoch 918/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 182.2191 - val_loss: 230.5078\n",
      "Epoch 919/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 184.1352 - val_loss: 232.7054\n",
      "Epoch 920/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 184.7218 - val_loss: 231.1682\n",
      "Epoch 921/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 184.9538 - val_loss: 232.3784\n",
      "Epoch 922/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 180.0401 - val_loss: 233.0831\n",
      "Epoch 923/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 180.3180 - val_loss: 231.8933\n",
      "Epoch 924/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 181.7226 - val_loss: 231.6826\n",
      "Epoch 925/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 181.3690 - val_loss: 231.6997\n",
      "Epoch 926/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 181.5478 - val_loss: 235.4088\n",
      "Epoch 927/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 184.6765 - val_loss: 235.0554\n",
      "Epoch 928/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 191.5344 - val_loss: 236.7086\n",
      "Epoch 929/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 178.7013 - val_loss: 234.5881\n",
      "Epoch 930/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 182.6688 - val_loss: 232.9979\n",
      "Epoch 931/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 183.7344 - val_loss: 231.7216\n",
      "Epoch 932/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 181.4552 - val_loss: 231.6991\n",
      "Epoch 933/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 179.7467 - val_loss: 236.9044\n",
      "Epoch 934/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 184.0105 - val_loss: 232.2923\n",
      "Epoch 935/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 182.6996 - val_loss: 232.0392\n",
      "Epoch 936/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 179.2792 - val_loss: 229.8508\n",
      "Epoch 937/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 184.1221 - val_loss: 231.2382\n",
      "Epoch 938/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 178.9756 - val_loss: 231.6547\n",
      "Epoch 939/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 189.0298 - val_loss: 230.0800\n",
      "Epoch 940/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 176.6282 - val_loss: 229.5861\n",
      "Epoch 941/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 185.9734 - val_loss: 228.3919\n",
      "Epoch 942/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 180.6698 - val_loss: 232.8572\n",
      "Epoch 943/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 179.7355 - val_loss: 231.0039\n",
      "Epoch 944/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 181.2197 - val_loss: 231.8303\n",
      "Epoch 945/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 185.0669 - val_loss: 233.5481\n",
      "Epoch 946/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 184.1000 - val_loss: 232.8678\n",
      "Epoch 947/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 183.1071 - val_loss: 229.5609\n",
      "Epoch 948/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 182.7311 - val_loss: 231.5991\n",
      "Epoch 949/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 181.2017 - val_loss: 230.2573\n",
      "Epoch 950/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 182.4791 - val_loss: 228.9757\n",
      "Epoch 951/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 184.1416 - val_loss: 229.4279\n",
      "Epoch 952/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 186.4343 - val_loss: 231.2937\n",
      "Epoch 953/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 181.2590 - val_loss: 231.4297\n",
      "Epoch 954/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 183.0775 - val_loss: 230.2630\n",
      "Epoch 955/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 180.3667 - val_loss: 230.8199\n",
      "Epoch 956/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 177.6820 - val_loss: 234.0048\n",
      "Epoch 957/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 180.3656 - val_loss: 233.4340\n",
      "Epoch 958/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 181.9218 - val_loss: 231.8663\n",
      "Epoch 959/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 182.7510 - val_loss: 230.9089\n",
      "Epoch 960/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 176.2601 - val_loss: 230.8560\n",
      "Epoch 961/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 180.0348 - val_loss: 230.4668\n",
      "Epoch 962/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 184.3641 - val_loss: 229.8085\n",
      "Epoch 963/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 182.9859 - val_loss: 230.9966\n",
      "Epoch 964/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 177.2144 - val_loss: 229.9410\n",
      "Epoch 965/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 180.2177 - val_loss: 230.7141\n",
      "Epoch 966/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 181.5190 - val_loss: 232.5850\n",
      "Epoch 967/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 181.1980 - val_loss: 229.9672\n",
      "Epoch 968/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 178.1758 - val_loss: 228.6324\n",
      "Epoch 969/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 176.3698 - val_loss: 231.8738\n",
      "Epoch 970/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 183.9298 - val_loss: 227.7507\n",
      "Epoch 971/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 184.3040 - val_loss: 228.6649\n",
      "Epoch 972/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 185.0863 - val_loss: 227.8869\n",
      "Epoch 973/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 177.7449 - val_loss: 228.7752\n",
      "Epoch 974/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 180.1219 - val_loss: 231.4810\n",
      "Epoch 975/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 185.7125 - val_loss: 230.6561\n",
      "Epoch 976/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 181.5984 - val_loss: 230.0155\n",
      "Epoch 977/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 177.3054 - val_loss: 233.5630\n",
      "Epoch 978/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 181.2677 - val_loss: 235.3706\n",
      "Epoch 979/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 179.0547 - val_loss: 230.7779\n",
      "Epoch 980/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 181.5687 - val_loss: 234.1015\n",
      "Epoch 981/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 183.2095 - val_loss: 233.1773\n",
      "Epoch 982/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 180.9219 - val_loss: 232.2677\n",
      "Epoch 983/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 175.8956 - val_loss: 231.4057\n",
      "Epoch 984/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 183.9581 - val_loss: 230.2811\n",
      "Epoch 985/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 180.9658 - val_loss: 232.6415\n",
      "Epoch 986/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 180.1829 - val_loss: 231.8962\n",
      "Epoch 987/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 181.3612 - val_loss: 230.7865\n",
      "Epoch 988/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 180.0175 - val_loss: 230.6618\n",
      "Epoch 989/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 178.4045 - val_loss: 231.6091\n",
      "Epoch 990/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 177.3601 - val_loss: 231.7042\n",
      "Epoch 991/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 175.9188 - val_loss: 231.3086\n",
      "Epoch 992/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 178.2735 - val_loss: 233.3656\n",
      "Epoch 993/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 179.6218 - val_loss: 233.2018\n",
      "Epoch 994/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 179.1647 - val_loss: 232.3732\n",
      "Epoch 995/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 174.9235 - val_loss: 232.4229\n",
      "Epoch 996/2000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 180.7740 - val_loss: 234.9657\n",
      "Epoch 997/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 177.4579 - val_loss: 231.0428\n",
      "Epoch 998/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 182.6631 - val_loss: 229.3802\n",
      "Epoch 999/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 177.2559 - val_loss: 231.5761\n",
      "Epoch 1000/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 178.8650 - val_loss: 230.3474\n",
      "Epoch 1001/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 177.6567 - val_loss: 231.5707\n",
      "Epoch 1002/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 181.6220 - val_loss: 228.6610\n",
      "Epoch 1003/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 177.4017 - val_loss: 229.5200\n",
      "Epoch 1004/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 178.9742 - val_loss: 229.9161\n",
      "Epoch 1005/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 176.1355 - val_loss: 229.0809\n",
      "Epoch 1006/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 176.3875 - val_loss: 227.5505\n",
      "Epoch 1007/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 181.6308 - val_loss: 230.4395\n",
      "Epoch 1008/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 176.9884 - val_loss: 227.9101\n",
      "Epoch 1009/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 176.7765 - val_loss: 231.2083\n",
      "Epoch 1010/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 179.8598 - val_loss: 229.4734\n",
      "Epoch 1011/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 181.2918 - val_loss: 232.4672\n",
      "Epoch 1012/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 177.7354 - val_loss: 230.1924\n",
      "Epoch 1013/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 182.0447 - val_loss: 229.3701\n",
      "Epoch 1014/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 178.6619 - val_loss: 229.9500\n",
      "Epoch 1015/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 178.7414 - val_loss: 230.3900\n",
      "Epoch 1016/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 176.9090 - val_loss: 229.7576\n",
      "Epoch 1017/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 176.3729 - val_loss: 229.6640\n",
      "Epoch 1018/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 178.4164 - val_loss: 229.7291\n",
      "Epoch 1019/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 181.7567 - val_loss: 228.8421\n",
      "Epoch 1020/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 180.6302 - val_loss: 229.1727\n",
      "Epoch 1021/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 174.8285 - val_loss: 229.5007\n",
      "Epoch 1022/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 178.8562 - val_loss: 230.1596\n",
      "Epoch 1023/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 173.8682 - val_loss: 231.1680\n",
      "Epoch 1024/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 173.0795 - val_loss: 232.0840\n",
      "Epoch 1025/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 178.5964 - val_loss: 230.4433\n",
      "Epoch 1026/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 179.7526 - val_loss: 229.2147\n",
      "Epoch 1027/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 174.5087 - val_loss: 229.2503\n",
      "Epoch 1028/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 177.9053 - val_loss: 229.8321\n",
      "Epoch 1029/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 178.3879 - val_loss: 228.1393\n",
      "Epoch 1030/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 181.7366 - val_loss: 230.0074\n",
      "Epoch 1031/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 178.6619 - val_loss: 232.9465\n",
      "Epoch 1032/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 176.6836 - val_loss: 230.4148\n",
      "Epoch 1033/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 174.2578 - val_loss: 230.3334\n",
      "Epoch 1034/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 179.0053 - val_loss: 230.0168\n",
      "Epoch 1035/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 176.0216 - val_loss: 230.4983\n",
      "Epoch 1036/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 179.3506 - val_loss: 229.5432\n",
      "Epoch 1037/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 172.8936 - val_loss: 229.3617\n",
      "Epoch 1038/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 179.2351 - val_loss: 230.2124\n",
      "Epoch 1039/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 178.4070 - val_loss: 230.6078\n",
      "Epoch 1040/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 175.8176 - val_loss: 228.8982\n",
      "Epoch 1041/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 175.2769 - val_loss: 229.1307\n",
      "Epoch 1042/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 177.1968 - val_loss: 229.3242\n",
      "Epoch 1043/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 173.8407 - val_loss: 232.2545\n",
      "Epoch 1044/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 175.1355 - val_loss: 230.7184\n",
      "Epoch 1045/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 176.9702 - val_loss: 231.5837\n",
      "Epoch 1046/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 171.9237 - val_loss: 233.2162\n",
      "Epoch 1047/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 175.4291 - val_loss: 229.6135\n",
      "Epoch 1048/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 173.8572 - val_loss: 231.4800\n",
      "Epoch 1049/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.9889 - val_loss: 228.9715\n",
      "Epoch 1050/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 176.4131 - val_loss: 231.2526\n",
      "Epoch 1051/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 175.9790 - val_loss: 228.0130\n",
      "Epoch 1052/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 174.4546 - val_loss: 229.5849\n",
      "Epoch 1053/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 174.0747 - val_loss: 231.2594\n",
      "Epoch 1054/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 176.4806 - val_loss: 233.1808\n",
      "Epoch 1055/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 174.9703 - val_loss: 231.1212\n",
      "Epoch 1056/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 168.2601 - val_loss: 229.3774\n",
      "Epoch 1057/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 174.6443 - val_loss: 229.0482\n",
      "Epoch 1058/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 176.2755 - val_loss: 228.7093\n",
      "Epoch 1059/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 175.7104 - val_loss: 228.1467\n",
      "Epoch 1060/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 173.7722 - val_loss: 228.4761\n",
      "Epoch 1061/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 176.7881 - val_loss: 226.6058\n",
      "Epoch 1062/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.7850 - val_loss: 229.5996\n",
      "Epoch 1063/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 174.4785 - val_loss: 231.4633\n",
      "Epoch 1064/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 177.9889 - val_loss: 229.7740\n",
      "Epoch 1065/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.5311 - val_loss: 228.3737\n",
      "Epoch 1066/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.3882 - val_loss: 229.8941\n",
      "Epoch 1067/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 176.4338 - val_loss: 229.8199\n",
      "Epoch 1068/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 173.2746 - val_loss: 228.3906\n",
      "Epoch 1069/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 174.5079 - val_loss: 226.9438\n",
      "Epoch 1070/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 172.9309 - val_loss: 227.8427\n",
      "Epoch 1071/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 170.9678 - val_loss: 229.2909\n",
      "Epoch 1072/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 172.3047 - val_loss: 228.6924\n",
      "Epoch 1073/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 178.2311 - val_loss: 229.6469\n",
      "Epoch 1074/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 172.6200 - val_loss: 229.0657\n",
      "Epoch 1075/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 174.8898 - val_loss: 230.8908\n",
      "Epoch 1076/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 171.8009 - val_loss: 229.3124\n",
      "Epoch 1077/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 174.7517 - val_loss: 228.7274\n",
      "Epoch 1078/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 171.0729 - val_loss: 228.3816\n",
      "Epoch 1079/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 170.5799 - val_loss: 229.6782\n",
      "Epoch 1080/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 174.9082 - val_loss: 227.7987\n",
      "Epoch 1081/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 174.3637 - val_loss: 232.0855\n",
      "Epoch 1082/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 169.4827 - val_loss: 237.3334\n",
      "Epoch 1083/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 170.3120 - val_loss: 233.3053\n",
      "Epoch 1084/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 170.8618 - val_loss: 230.0582\n",
      "Epoch 1085/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 178.9886 - val_loss: 231.5090\n",
      "Epoch 1086/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 175.0388 - val_loss: 230.2936\n",
      "Epoch 1087/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 169.9025 - val_loss: 226.1285\n",
      "Epoch 1088/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.0330 - val_loss: 228.5930\n",
      "Epoch 1089/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 173.2312 - val_loss: 229.7511\n",
      "Epoch 1090/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 171.0777 - val_loss: 228.3924\n",
      "Epoch 1091/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 166.1388 - val_loss: 229.5554\n",
      "Epoch 1092/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 168.1442 - val_loss: 228.4425\n",
      "Epoch 1093/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 174.8183 - val_loss: 231.0871\n",
      "Epoch 1094/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 175.2716 - val_loss: 230.6094\n",
      "Epoch 1095/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 177.1264 - val_loss: 228.6426\n",
      "Epoch 1096/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.7856 - val_loss: 230.7790\n",
      "Epoch 1097/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 170.8030 - val_loss: 230.4444\n",
      "Epoch 1098/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 174.0518 - val_loss: 231.5123\n",
      "Epoch 1099/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 170.6218 - val_loss: 232.0065\n",
      "Epoch 1100/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 171.9648 - val_loss: 231.8999\n",
      "Epoch 1101/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 173.6252 - val_loss: 231.8629\n",
      "Epoch 1102/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 173.5872 - val_loss: 231.5088\n",
      "Epoch 1103/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 173.0239 - val_loss: 229.8706\n",
      "Epoch 1104/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 176.7190 - val_loss: 231.6071\n",
      "Epoch 1105/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 173.5726 - val_loss: 232.8007\n",
      "Epoch 1106/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 172.6902 - val_loss: 230.2021\n",
      "Epoch 1107/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 174.6632 - val_loss: 229.9977\n",
      "Epoch 1108/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 169.4283 - val_loss: 231.6817\n",
      "Epoch 1109/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 171.8395 - val_loss: 229.1653\n",
      "Epoch 1110/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 173.7037 - val_loss: 229.6518\n",
      "Epoch 1111/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 172.6626 - val_loss: 231.4830\n",
      "Epoch 1112/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 173.8627 - val_loss: 230.7081\n",
      "Epoch 1113/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 171.6989 - val_loss: 231.0534\n",
      "Epoch 1114/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 169.7927 - val_loss: 230.5577\n",
      "Epoch 1115/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 168.2871 - val_loss: 229.3711\n",
      "Epoch 1116/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 169.4717 - val_loss: 229.8617\n",
      "Epoch 1117/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 172.0909 - val_loss: 230.1212\n",
      "Epoch 1118/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 170.5227 - val_loss: 229.7439\n",
      "Epoch 1119/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 170.3080 - val_loss: 231.0088\n",
      "Epoch 1120/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 173.2357 - val_loss: 236.9072\n",
      "Epoch 1121/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 172.9748 - val_loss: 228.5568\n",
      "Epoch 1122/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 171.6120 - val_loss: 229.2453\n",
      "Epoch 1123/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 173.5814 - val_loss: 232.2000\n",
      "Epoch 1124/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 170.6247 - val_loss: 232.3247\n",
      "Epoch 1125/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 171.6528 - val_loss: 229.1548\n",
      "Epoch 1126/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 172.0620 - val_loss: 231.4911\n",
      "Epoch 1127/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 172.4299 - val_loss: 233.8502\n",
      "Epoch 1128/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 173.4728 - val_loss: 227.8612\n",
      "Epoch 1129/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 171.4620 - val_loss: 226.7319\n",
      "Epoch 1130/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 173.4109 - val_loss: 227.3668\n",
      "Epoch 1131/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 173.3839 - val_loss: 227.3697\n",
      "Epoch 1132/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 174.8718 - val_loss: 228.8698\n",
      "Epoch 1133/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 169.2269 - val_loss: 231.0828\n",
      "Epoch 1134/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 172.0879 - val_loss: 231.1229\n",
      "Epoch 1135/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 176.4295 - val_loss: 231.9572\n",
      "Epoch 1136/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.2997 - val_loss: 230.0543\n",
      "Epoch 1137/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 170.6010 - val_loss: 231.6542\n",
      "Epoch 1138/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 174.9850 - val_loss: 230.9047\n",
      "Epoch 1139/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 174.0879 - val_loss: 231.4788\n",
      "Epoch 1140/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 171.9977 - val_loss: 233.4992\n",
      "Epoch 1141/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 172.3451 - val_loss: 232.4286\n",
      "Epoch 1142/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 174.2348 - val_loss: 230.8945\n",
      "Epoch 1143/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 167.5364 - val_loss: 236.4622\n",
      "Epoch 1144/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 171.6315 - val_loss: 231.7105\n",
      "Epoch 1145/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 168.8456 - val_loss: 228.9570\n",
      "Epoch 1146/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 167.2963 - val_loss: 230.6474\n",
      "Epoch 1147/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 172.1469 - val_loss: 231.2374\n",
      "Epoch 1148/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 172.9522 - val_loss: 229.3670\n",
      "Epoch 1149/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 171.0086 - val_loss: 231.8160\n",
      "Epoch 1150/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 165.6326 - val_loss: 231.9770\n",
      "Epoch 1151/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 169.6497 - val_loss: 229.6574\n",
      "Epoch 1152/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 167.7202 - val_loss: 230.7810\n",
      "Epoch 1153/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 176.6887 - val_loss: 229.1228\n",
      "Epoch 1154/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 174.5476 - val_loss: 229.5802\n",
      "Epoch 1155/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 167.8271 - val_loss: 229.2778\n",
      "Epoch 1156/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.6620 - val_loss: 231.6427\n",
      "Epoch 1157/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 172.4936 - val_loss: 230.9218\n",
      "Epoch 1158/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 169.9412 - val_loss: 233.0813\n",
      "Epoch 1159/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 169.7010 - val_loss: 228.1011\n",
      "Epoch 1160/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 166.6875 - val_loss: 227.2588\n",
      "Epoch 1161/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 165.7463 - val_loss: 230.1734\n",
      "Epoch 1162/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.8446 - val_loss: 232.7561\n",
      "Epoch 1163/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 165.4918 - val_loss: 229.2001\n",
      "Epoch 1164/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 168.3552 - val_loss: 231.8157\n",
      "Epoch 1165/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 170.4860 - val_loss: 232.5578\n",
      "Epoch 1166/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 173.2613 - val_loss: 232.7202\n",
      "Epoch 1167/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 175.3595 - val_loss: 229.4045\n",
      "Epoch 1168/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 171.9294 - val_loss: 232.0544\n",
      "Epoch 1169/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 175.3074 - val_loss: 232.1600\n",
      "Epoch 1170/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 175.3547 - val_loss: 230.7924\n",
      "Epoch 1171/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 172.7681 - val_loss: 230.0943\n",
      "Epoch 1172/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 168.9617 - val_loss: 232.0334\n",
      "Epoch 1173/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.4831 - val_loss: 229.3470\n",
      "Epoch 1174/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.4773 - val_loss: 229.8714\n",
      "Epoch 1175/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.3107 - val_loss: 227.9931\n",
      "Epoch 1176/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 167.0176 - val_loss: 228.9300\n",
      "Epoch 1177/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 172.3693 - val_loss: 227.6261\n",
      "Epoch 1178/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 170.2623 - val_loss: 224.3272\n",
      "Epoch 1179/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 166.1348 - val_loss: 231.1416\n",
      "Epoch 1180/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 169.2618 - val_loss: 229.8650\n",
      "Epoch 1181/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 168.1598 - val_loss: 233.4984\n",
      "Epoch 1182/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 170.0591 - val_loss: 229.8816\n",
      "Epoch 1183/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 171.8449 - val_loss: 227.7650\n",
      "Epoch 1184/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 167.3296 - val_loss: 228.4272\n",
      "Epoch 1185/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 167.6927 - val_loss: 228.9067\n",
      "Epoch 1186/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 162.2766 - val_loss: 228.1311\n",
      "Epoch 1187/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 169.0818 - val_loss: 229.6238\n",
      "Epoch 1188/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 164.8451 - val_loss: 229.8079\n",
      "Epoch 1189/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 171.2716 - val_loss: 230.5056\n",
      "Epoch 1190/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.1888 - val_loss: 232.9640\n",
      "Epoch 1191/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 166.8339 - val_loss: 232.7991\n",
      "Epoch 1192/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 171.3626 - val_loss: 232.8672\n",
      "Epoch 1193/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 167.5673 - val_loss: 232.7239\n",
      "Epoch 1194/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 171.0886 - val_loss: 228.7040\n",
      "Epoch 1195/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 173.7890 - val_loss: 232.1944\n",
      "Epoch 1196/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 172.2402 - val_loss: 234.3385\n",
      "Epoch 1197/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 168.7708 - val_loss: 232.3805\n",
      "Epoch 1198/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 164.1253 - val_loss: 229.4008\n",
      "Epoch 1199/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 166.7238 - val_loss: 232.1620\n",
      "Epoch 1200/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 167.1243 - val_loss: 231.6836\n",
      "Epoch 1201/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 169.8904 - val_loss: 230.6606\n",
      "Epoch 1202/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 169.3224 - val_loss: 233.2000\n",
      "Epoch 1203/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 171.0196 - val_loss: 232.3991\n",
      "Epoch 1204/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 172.9088 - val_loss: 232.0154\n",
      "Epoch 1205/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 164.7825 - val_loss: 230.5641\n",
      "Epoch 1206/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 165.6073 - val_loss: 229.0222\n",
      "Epoch 1207/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 169.0949 - val_loss: 230.2265\n",
      "Epoch 1208/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 165.4443 - val_loss: 232.0067\n",
      "Epoch 1209/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 165.4991 - val_loss: 232.4000\n",
      "Epoch 1210/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 168.6601 - val_loss: 231.2389\n",
      "Epoch 1211/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 168.9850 - val_loss: 232.2287\n",
      "Epoch 1212/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.1566 - val_loss: 233.3481\n",
      "Epoch 1213/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.1007 - val_loss: 233.2216\n",
      "Epoch 1214/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 166.9745 - val_loss: 231.8831\n",
      "Epoch 1215/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 164.0486 - val_loss: 236.4844\n",
      "Epoch 1216/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 164.7863 - val_loss: 232.6347\n",
      "Epoch 1217/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 167.3497 - val_loss: 237.6290\n",
      "Epoch 1218/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 167.3190 - val_loss: 235.6676\n",
      "Epoch 1219/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 169.4124 - val_loss: 231.9694\n",
      "Epoch 1220/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 167.8753 - val_loss: 235.0432\n",
      "Epoch 1221/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 168.2184 - val_loss: 233.3956\n",
      "Epoch 1222/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 172.1401 - val_loss: 237.8317\n",
      "Epoch 1223/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 163.6475 - val_loss: 232.0098\n",
      "Epoch 1224/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 168.5010 - val_loss: 234.6035\n",
      "Epoch 1225/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 167.1940 - val_loss: 233.3896\n",
      "Epoch 1226/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.3811 - val_loss: 229.4792\n",
      "Epoch 1227/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 166.6062 - val_loss: 234.4512\n",
      "Epoch 1228/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 168.3834 - val_loss: 232.1204\n",
      "Epoch 1229/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 169.2659 - val_loss: 232.2392\n",
      "Epoch 1230/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.0337 - val_loss: 231.6985\n",
      "Epoch 1231/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 162.8023 - val_loss: 232.1691\n",
      "Epoch 1232/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 169.1969 - val_loss: 233.7173\n",
      "Epoch 1233/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.4103 - val_loss: 230.9529\n",
      "Epoch 1234/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 166.2558 - val_loss: 231.0074\n",
      "Epoch 1235/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 170.5502 - val_loss: 232.1606\n",
      "Epoch 1236/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 170.0054 - val_loss: 230.9486\n",
      "Epoch 1237/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 163.0725 - val_loss: 231.8446\n",
      "Epoch 1238/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 166.6369 - val_loss: 228.8920\n",
      "Epoch 1239/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 166.1819 - val_loss: 231.4601\n",
      "Epoch 1240/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 169.2658 - val_loss: 231.9330\n",
      "Epoch 1241/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 165.9070 - val_loss: 230.4141\n",
      "Epoch 1242/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 167.1023 - val_loss: 229.3480\n",
      "Epoch 1243/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 168.6258 - val_loss: 231.5974\n",
      "Epoch 1244/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 168.0093 - val_loss: 230.7813\n",
      "Epoch 1245/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 168.4401 - val_loss: 229.2800\n",
      "Epoch 1246/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 165.8900 - val_loss: 229.3216\n",
      "Epoch 1247/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 164.2026 - val_loss: 230.2958\n",
      "Epoch 1248/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 167.1826 - val_loss: 229.1328\n",
      "Epoch 1249/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 162.6468 - val_loss: 233.4639\n",
      "Epoch 1250/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 168.9925 - val_loss: 233.5496\n",
      "Epoch 1251/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 166.1706 - val_loss: 235.0219\n",
      "Epoch 1252/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 166.8739 - val_loss: 230.8850\n",
      "Epoch 1253/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 170.1427 - val_loss: 232.2679\n",
      "Epoch 1254/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 162.1668 - val_loss: 230.1902\n",
      "Epoch 1255/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 168.3948 - val_loss: 230.7800\n",
      "Epoch 1256/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 160.6858 - val_loss: 235.2818\n",
      "Epoch 1257/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 166.1205 - val_loss: 233.1355\n",
      "Epoch 1258/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 166.9363 - val_loss: 234.8918\n",
      "Epoch 1259/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.9606 - val_loss: 233.3738\n",
      "Epoch 1260/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 168.8635 - val_loss: 236.3820\n",
      "Epoch 1261/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 159.9332 - val_loss: 231.5481\n",
      "Epoch 1262/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 160.9548 - val_loss: 233.3758\n",
      "Epoch 1263/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 158.2147 - val_loss: 233.0143\n",
      "Epoch 1264/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 167.0009 - val_loss: 231.4662\n",
      "Epoch 1265/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 165.5778 - val_loss: 229.7696\n",
      "Epoch 1266/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 163.6605 - val_loss: 231.4950\n",
      "Epoch 1267/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 165.3749 - val_loss: 231.9164\n",
      "Epoch 1268/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 164.4820 - val_loss: 231.2874\n",
      "Epoch 1269/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 170.1004 - val_loss: 232.2791\n",
      "Epoch 1270/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 166.7023 - val_loss: 232.1431\n",
      "Epoch 1271/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 164.6342 - val_loss: 233.6483\n",
      "Epoch 1272/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 162.7690 - val_loss: 234.8081\n",
      "Epoch 1273/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 166.1424 - val_loss: 232.4124\n",
      "Epoch 1274/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 171.4311 - val_loss: 232.6421\n",
      "Epoch 1275/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 162.5378 - val_loss: 227.9119\n",
      "Epoch 1276/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 159.1923 - val_loss: 234.5155\n",
      "Epoch 1277/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 168.2166 - val_loss: 232.1832\n",
      "Epoch 1278/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 165.5034 - val_loss: 233.7450\n",
      "Epoch 1279/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.1886 - val_loss: 234.2108\n",
      "Epoch 1280/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 171.3335 - val_loss: 230.9597\n",
      "Epoch 1281/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 164.2271 - val_loss: 231.9524\n",
      "Epoch 1282/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 164.6176 - val_loss: 231.9620\n",
      "Epoch 1283/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 168.9843 - val_loss: 232.8724\n",
      "Epoch 1284/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 166.4998 - val_loss: 232.8841\n",
      "Epoch 1285/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 168.6754 - val_loss: 234.3553\n",
      "Epoch 1286/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 164.2573 - val_loss: 233.2566\n",
      "Epoch 1287/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 160.4336 - val_loss: 233.4958\n",
      "Epoch 1288/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.9488 - val_loss: 233.3053\n",
      "Epoch 1289/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 162.9089 - val_loss: 231.3115\n",
      "Epoch 1290/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 161.4544 - val_loss: 237.8517\n",
      "Epoch 1291/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 164.4032 - val_loss: 235.4004\n",
      "Epoch 1292/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 160.5216 - val_loss: 231.6671\n",
      "Epoch 1293/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 166.8482 - val_loss: 233.0836\n",
      "Epoch 1294/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 166.7625 - val_loss: 232.2604\n",
      "Epoch 1295/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 164.8196 - val_loss: 229.4193\n",
      "Epoch 1296/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 162.2912 - val_loss: 229.5201\n",
      "Epoch 1297/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 160.9193 - val_loss: 230.3417\n",
      "Epoch 1298/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 163.7935 - val_loss: 230.7274\n",
      "Epoch 1299/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 160.0480 - val_loss: 232.4759\n",
      "Epoch 1300/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 162.5135 - val_loss: 230.6238\n",
      "Epoch 1301/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 166.1969 - val_loss: 231.3898\n",
      "Epoch 1302/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 167.2164 - val_loss: 232.0804\n",
      "Epoch 1303/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 164.6900 - val_loss: 234.3468\n",
      "Epoch 1304/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 161.9115 - val_loss: 234.3990\n",
      "Epoch 1305/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 166.9838 - val_loss: 232.8690\n",
      "Epoch 1306/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.2971 - val_loss: 233.7182\n",
      "Epoch 1307/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 167.8077 - val_loss: 232.5562\n",
      "Epoch 1308/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 166.3373 - val_loss: 232.9206\n",
      "Epoch 1309/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.0667 - val_loss: 235.3493\n",
      "Epoch 1310/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 159.0449 - val_loss: 231.7826\n",
      "Epoch 1311/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 164.6589 - val_loss: 229.7280\n",
      "Epoch 1312/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 161.0076 - val_loss: 232.7231\n",
      "Epoch 1313/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 160.6643 - val_loss: 230.5397\n",
      "Epoch 1314/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 167.3906 - val_loss: 232.4001\n",
      "Epoch 1315/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 166.0573 - val_loss: 232.1448\n",
      "Epoch 1316/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 168.4119 - val_loss: 235.3749\n",
      "Epoch 1317/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 163.9500 - val_loss: 232.8372\n",
      "Epoch 1318/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 161.0672 - val_loss: 232.5616\n",
      "Epoch 1319/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 163.9921 - val_loss: 232.1501\n",
      "Epoch 1320/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 162.8339 - val_loss: 234.1988\n",
      "Epoch 1321/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 162.1550 - val_loss: 230.6339\n",
      "Epoch 1322/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 164.4234 - val_loss: 228.8117\n",
      "Epoch 1323/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 159.9524 - val_loss: 231.5891\n",
      "Epoch 1324/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 162.2179 - val_loss: 230.6191\n",
      "Epoch 1325/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 166.0758 - val_loss: 227.7512\n",
      "Epoch 1326/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 166.5909 - val_loss: 229.7640\n",
      "Epoch 1327/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 159.2146 - val_loss: 231.5875\n",
      "Epoch 1328/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 162.5691 - val_loss: 231.7827\n",
      "Epoch 1329/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 164.7442 - val_loss: 232.5628\n",
      "Epoch 1330/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 164.6038 - val_loss: 229.3143\n",
      "Epoch 1331/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 164.6898 - val_loss: 228.3330\n",
      "Epoch 1332/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 164.2484 - val_loss: 229.2412\n",
      "Epoch 1333/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 165.3358 - val_loss: 230.5287\n",
      "Epoch 1334/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.8344 - val_loss: 230.4621\n",
      "Epoch 1335/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.4218 - val_loss: 229.8611\n",
      "Epoch 1336/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 164.2950 - val_loss: 230.5995\n",
      "Epoch 1337/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 163.0946 - val_loss: 233.8102\n",
      "Epoch 1338/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 163.2461 - val_loss: 232.5989\n",
      "Epoch 1339/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 162.0713 - val_loss: 231.0630\n",
      "Epoch 1340/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 159.6271 - val_loss: 232.8285\n",
      "Epoch 1341/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 162.3228 - val_loss: 231.5566\n",
      "Epoch 1342/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 162.6152 - val_loss: 230.5836\n",
      "Epoch 1343/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 165.7449 - val_loss: 232.2450\n",
      "Epoch 1344/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 157.5386 - val_loss: 231.6786\n",
      "Epoch 1345/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 163.4877 - val_loss: 232.9536\n",
      "Epoch 1346/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 165.5197 - val_loss: 227.8712\n",
      "Epoch 1347/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 158.1598 - val_loss: 231.5438\n",
      "Epoch 1348/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 161.8807 - val_loss: 231.6812\n",
      "Epoch 1349/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 164.2982 - val_loss: 233.9598\n",
      "Epoch 1350/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 164.4843 - val_loss: 231.9255\n",
      "Epoch 1351/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 158.6511 - val_loss: 230.3719\n",
      "Epoch 1352/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 164.8997 - val_loss: 229.8602\n",
      "Epoch 1353/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 162.1443 - val_loss: 230.4341\n",
      "Epoch 1354/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 158.0314 - val_loss: 231.2610\n",
      "Epoch 1355/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 163.7503 - val_loss: 231.8171\n",
      "Epoch 1356/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 166.8293 - val_loss: 230.8213\n",
      "Epoch 1357/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.3888 - val_loss: 233.6788\n",
      "Epoch 1358/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.3557 - val_loss: 232.5334\n",
      "Epoch 1359/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 162.5541 - val_loss: 231.1186\n",
      "Epoch 1360/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 166.2023 - val_loss: 232.3187\n",
      "Epoch 1361/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 160.9073 - val_loss: 232.0080\n",
      "Epoch 1362/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 165.1477 - val_loss: 233.5561\n",
      "Epoch 1363/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 160.3643 - val_loss: 232.8321\n",
      "Epoch 1364/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 161.1548 - val_loss: 232.2296\n",
      "Epoch 1365/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 163.7596 - val_loss: 231.2764\n",
      "Epoch 1366/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 162.2596 - val_loss: 230.0490\n",
      "Epoch 1367/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 168.1318 - val_loss: 229.2045\n",
      "Epoch 1368/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 161.7112 - val_loss: 232.2801\n",
      "Epoch 1369/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 169.4323 - val_loss: 231.3980\n",
      "Epoch 1370/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 161.6064 - val_loss: 231.2849\n",
      "Epoch 1371/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 159.5372 - val_loss: 231.2692\n",
      "Epoch 1372/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 163.5305 - val_loss: 232.6982\n",
      "Epoch 1373/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 159.2714 - val_loss: 232.7943\n",
      "Epoch 1374/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 164.4901 - val_loss: 234.4058\n",
      "Epoch 1375/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 161.0936 - val_loss: 234.0257\n",
      "Epoch 1376/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 156.5987 - val_loss: 229.7829\n",
      "Epoch 1377/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 160.1943 - val_loss: 231.8617\n",
      "Epoch 1378/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 161.2523 - val_loss: 237.5696\n",
      "Epoch 1379/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 163.5598 - val_loss: 232.5391\n",
      "Epoch 1380/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 164.1005 - val_loss: 229.7058\n",
      "Epoch 1381/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 162.8266 - val_loss: 232.5595\n",
      "Epoch 1382/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 160.6497 - val_loss: 234.9777\n",
      "Epoch 1383/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 160.2008 - val_loss: 233.1845\n",
      "Epoch 1384/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 164.2130 - val_loss: 230.7801\n",
      "Epoch 1385/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 156.3104 - val_loss: 230.3185\n",
      "Epoch 1386/2000\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 160.8272 - val_loss: 229.5235\n",
      "Epoch 1387/2000\n",
      "14/14 [==============================] - 1s 69ms/step - loss: 159.9799 - val_loss: 229.2038\n",
      "Epoch 1388/2000\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 155.8009 - val_loss: 228.9610\n",
      "Epoch 1389/2000\n",
      "14/14 [==============================] - 1s 52ms/step - loss: 160.7731 - val_loss: 229.8361\n",
      "Epoch 1390/2000\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 156.0004 - val_loss: 231.0254\n",
      "Epoch 1391/2000\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 161.6251 - val_loss: 232.9538\n",
      "Epoch 1392/2000\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 160.8981 - val_loss: 230.2243\n",
      "Epoch 1393/2000\n",
      "14/14 [==============================] - 1s 77ms/step - loss: 157.9848 - val_loss: 229.1173\n",
      "Epoch 1394/2000\n",
      "14/14 [==============================] - 1s 67ms/step - loss: 163.6569 - val_loss: 230.9065\n",
      "Epoch 1395/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 158.3831 - val_loss: 230.6021\n",
      "Epoch 1396/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 162.3619 - val_loss: 229.7135\n",
      "Epoch 1397/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 159.2611 - val_loss: 230.5375\n",
      "Epoch 1398/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 159.3870 - val_loss: 232.4583\n",
      "Epoch 1399/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 160.4659 - val_loss: 228.3916\n",
      "Epoch 1400/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 161.6473 - val_loss: 230.8063\n",
      "Epoch 1401/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 161.5054 - val_loss: 230.9254\n",
      "Epoch 1402/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.0200 - val_loss: 230.6073\n",
      "Epoch 1403/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 158.7828 - val_loss: 230.3597\n",
      "Epoch 1404/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 159.0842 - val_loss: 229.2354\n",
      "Epoch 1405/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 162.0113 - val_loss: 233.4638\n",
      "Epoch 1406/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 160.6294 - val_loss: 232.1297\n",
      "Epoch 1407/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 160.2184 - val_loss: 228.2806\n",
      "Epoch 1408/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 158.8834 - val_loss: 229.8853\n",
      "Epoch 1409/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 160.9445 - val_loss: 231.4361\n",
      "Epoch 1410/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 162.5480 - val_loss: 234.1325\n",
      "Epoch 1411/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 161.4515 - val_loss: 231.9554\n",
      "Epoch 1412/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 158.5443 - val_loss: 231.9632\n",
      "Epoch 1413/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 163.0251 - val_loss: 232.0500\n",
      "Epoch 1414/2000\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 158.8122 - val_loss: 231.8588\n",
      "Epoch 1415/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 159.1998 - val_loss: 233.5491\n",
      "Epoch 1416/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 167.2688 - val_loss: 234.0370\n",
      "Epoch 1417/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 158.5738 - val_loss: 232.3484\n",
      "Epoch 1418/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 156.7507 - val_loss: 233.3916\n",
      "Epoch 1419/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 161.8464 - val_loss: 230.1216\n",
      "Epoch 1420/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 164.8112 - val_loss: 230.6243\n",
      "Epoch 1421/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 157.9662 - val_loss: 233.4409\n",
      "Epoch 1422/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 164.7496 - val_loss: 233.4669\n",
      "Epoch 1423/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 158.7781 - val_loss: 229.5059\n",
      "Epoch 1424/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.7993 - val_loss: 234.9246\n",
      "Epoch 1425/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 160.7912 - val_loss: 233.8769\n",
      "Epoch 1426/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 156.9850 - val_loss: 233.0852\n",
      "Epoch 1427/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 161.6032 - val_loss: 232.4516\n",
      "Epoch 1428/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 156.1179 - val_loss: 234.8494\n",
      "Epoch 1429/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 161.4675 - val_loss: 229.8958\n",
      "Epoch 1430/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 159.7155 - val_loss: 235.9561\n",
      "Epoch 1431/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 160.2932 - val_loss: 230.9284\n",
      "Epoch 1432/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 164.4585 - val_loss: 232.0743\n",
      "Epoch 1433/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 160.9332 - val_loss: 233.1394\n",
      "Epoch 1434/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 158.8742 - val_loss: 231.8225\n",
      "Epoch 1435/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 162.0100 - val_loss: 233.4765\n",
      "Epoch 1436/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 156.5238 - val_loss: 230.2750\n",
      "Epoch 1437/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 159.2266 - val_loss: 233.3255\n",
      "Epoch 1438/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 159.2369 - val_loss: 230.2888\n",
      "Epoch 1439/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 159.4762 - val_loss: 229.9246\n",
      "Epoch 1440/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 160.0703 - val_loss: 230.3057\n",
      "Epoch 1441/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 159.1360 - val_loss: 230.4833\n",
      "Epoch 1442/2000\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 160.5920 - val_loss: 234.1037\n",
      "Epoch 1443/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 164.6321 - val_loss: 231.9642\n",
      "Epoch 1444/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 157.8461 - val_loss: 233.3445\n",
      "Epoch 1445/2000\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 159.8356 - val_loss: 233.2113\n",
      "Epoch 1446/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 160.3242 - val_loss: 231.6874\n",
      "Epoch 1447/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 158.0132 - val_loss: 231.4174\n",
      "Epoch 1448/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 161.6492 - val_loss: 232.4892\n",
      "Epoch 1449/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 158.7995 - val_loss: 233.5953\n",
      "Epoch 1450/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 161.6613 - val_loss: 233.0915\n",
      "Epoch 1451/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.0481 - val_loss: 232.7353\n",
      "Epoch 1452/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 157.9919 - val_loss: 230.6548\n",
      "Epoch 1453/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 163.0431 - val_loss: 233.3607\n",
      "Epoch 1454/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 160.6042 - val_loss: 232.9261\n",
      "Epoch 1455/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 161.0051 - val_loss: 236.4258\n",
      "Epoch 1456/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 161.3099 - val_loss: 235.1560\n",
      "Epoch 1457/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 161.2609 - val_loss: 232.0898\n",
      "Epoch 1458/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 155.5742 - val_loss: 232.6803\n",
      "Epoch 1459/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 160.7692 - val_loss: 231.1463\n",
      "Epoch 1460/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 157.1879 - val_loss: 230.7870\n",
      "Epoch 1461/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 156.5438 - val_loss: 230.1357\n",
      "Epoch 1462/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 155.7522 - val_loss: 230.8972\n",
      "Epoch 1463/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 160.1002 - val_loss: 230.1989\n",
      "Epoch 1464/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 163.0525 - val_loss: 229.5382\n",
      "Epoch 1465/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 154.9807 - val_loss: 229.9881\n",
      "Epoch 1466/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 158.9237 - val_loss: 229.3062\n",
      "Epoch 1467/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 158.8754 - val_loss: 230.9850\n",
      "Epoch 1468/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 160.0925 - val_loss: 230.3997\n",
      "Epoch 1469/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 160.4376 - val_loss: 233.2680\n",
      "Epoch 1470/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.1565 - val_loss: 231.7338\n",
      "Epoch 1471/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 160.3623 - val_loss: 233.1672\n",
      "Epoch 1472/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 159.5727 - val_loss: 236.7129\n",
      "Epoch 1473/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 162.1715 - val_loss: 236.1038\n",
      "Epoch 1474/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 160.3044 - val_loss: 235.1326\n",
      "Epoch 1475/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 163.1922 - val_loss: 229.7491\n",
      "Epoch 1476/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.6049 - val_loss: 234.2878\n",
      "Epoch 1477/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.5198 - val_loss: 234.9883\n",
      "Epoch 1478/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 158.1542 - val_loss: 237.4013\n",
      "Epoch 1479/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 157.4319 - val_loss: 234.0385\n",
      "Epoch 1480/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 158.7307 - val_loss: 230.2812\n",
      "Epoch 1481/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 157.2253 - val_loss: 231.4828\n",
      "Epoch 1482/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 158.6342 - val_loss: 234.2676\n",
      "Epoch 1483/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 157.8630 - val_loss: 232.1354\n",
      "Epoch 1484/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 158.6651 - val_loss: 232.4239\n",
      "Epoch 1485/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 157.1279 - val_loss: 232.8842\n",
      "Epoch 1486/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.7235 - val_loss: 229.2952\n",
      "Epoch 1487/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 158.5310 - val_loss: 228.6712\n",
      "Epoch 1488/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 159.3568 - val_loss: 232.1071\n",
      "Epoch 1489/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 163.7555 - val_loss: 232.2415\n",
      "Epoch 1490/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 159.1879 - val_loss: 234.4790\n",
      "Epoch 1491/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 159.4897 - val_loss: 231.1363\n",
      "Epoch 1492/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 153.5350 - val_loss: 231.8698\n",
      "Epoch 1493/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 160.5768 - val_loss: 230.4020\n",
      "Epoch 1494/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.7334 - val_loss: 231.2317\n",
      "Epoch 1495/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 162.6554 - val_loss: 232.5482\n",
      "Epoch 1496/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.7538 - val_loss: 231.6831\n",
      "Epoch 1497/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 156.7630 - val_loss: 232.8177\n",
      "Epoch 1498/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 159.2192 - val_loss: 234.6924\n",
      "Epoch 1499/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 159.1342 - val_loss: 232.4454\n",
      "Epoch 1500/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 156.7843 - val_loss: 233.7666\n",
      "Epoch 1501/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 156.7466 - val_loss: 233.8649\n",
      "Epoch 1502/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 159.9879 - val_loss: 233.2265\n",
      "Epoch 1503/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 162.7404 - val_loss: 232.8714\n",
      "Epoch 1504/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 159.8925 - val_loss: 229.7635\n",
      "Epoch 1505/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.1753 - val_loss: 232.3386\n",
      "Epoch 1506/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 159.0951 - val_loss: 233.0921\n",
      "Epoch 1507/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 157.8469 - val_loss: 235.9569\n",
      "Epoch 1508/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 157.2233 - val_loss: 236.5983\n",
      "Epoch 1509/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 157.3922 - val_loss: 235.2225\n",
      "Epoch 1510/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 159.0906 - val_loss: 231.4190\n",
      "Epoch 1511/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 160.3450 - val_loss: 231.1525\n",
      "Epoch 1512/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 159.4505 - val_loss: 229.3087\n",
      "Epoch 1513/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.4157 - val_loss: 231.4945\n",
      "Epoch 1514/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 155.9271 - val_loss: 236.7268\n",
      "Epoch 1515/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 156.6614 - val_loss: 236.5118\n",
      "Epoch 1516/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 156.2248 - val_loss: 232.7232\n",
      "Epoch 1517/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 160.4529 - val_loss: 229.5075\n",
      "Epoch 1518/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 157.0646 - val_loss: 235.5847\n",
      "Epoch 1519/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 155.2964 - val_loss: 232.1854\n",
      "Epoch 1520/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 156.7119 - val_loss: 231.8685\n",
      "Epoch 1521/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 163.1947 - val_loss: 228.7009\n",
      "Epoch 1522/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 163.1058 - val_loss: 230.4344\n",
      "Epoch 1523/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 158.2468 - val_loss: 230.7563\n",
      "Epoch 1524/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 154.1633 - val_loss: 232.5569\n",
      "Epoch 1525/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 151.9402 - val_loss: 230.7103\n",
      "Epoch 1526/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.1634 - val_loss: 233.8101\n",
      "Epoch 1527/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 159.3380 - val_loss: 234.5432\n",
      "Epoch 1528/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 156.4780 - val_loss: 230.9370\n",
      "Epoch 1529/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 162.0121 - val_loss: 228.9644\n",
      "Epoch 1530/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.5530 - val_loss: 229.1388\n",
      "Epoch 1531/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 159.7572 - val_loss: 232.0067\n",
      "Epoch 1532/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.6762 - val_loss: 233.7497\n",
      "Epoch 1533/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 156.0326 - val_loss: 230.6357\n",
      "Epoch 1534/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 157.1061 - val_loss: 232.6905\n",
      "Epoch 1535/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 156.9074 - val_loss: 230.0992\n",
      "Epoch 1536/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 158.3100 - val_loss: 229.9194\n",
      "Epoch 1537/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 156.4786 - val_loss: 226.2901\n",
      "Epoch 1538/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 156.3368 - val_loss: 231.9596\n",
      "Epoch 1539/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 162.2838 - val_loss: 234.7735\n",
      "Epoch 1540/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 154.9940 - val_loss: 231.9683\n",
      "Epoch 1541/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 152.9794 - val_loss: 232.8259\n",
      "Epoch 1542/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 157.4835 - val_loss: 233.5298\n",
      "Epoch 1543/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 155.7560 - val_loss: 232.6958\n",
      "Epoch 1544/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 155.4864 - val_loss: 230.9095\n",
      "Epoch 1545/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 161.2943 - val_loss: 230.1294\n",
      "Epoch 1546/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.2723 - val_loss: 232.2399\n",
      "Epoch 1547/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.1739 - val_loss: 233.5371\n",
      "Epoch 1548/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 156.3015 - val_loss: 232.3160\n",
      "Epoch 1549/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.9198 - val_loss: 232.8330\n",
      "Epoch 1550/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 157.5097 - val_loss: 231.8066\n",
      "Epoch 1551/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 156.7691 - val_loss: 233.0161\n",
      "Epoch 1552/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 158.8109 - val_loss: 233.6107\n",
      "Epoch 1553/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 159.4068 - val_loss: 232.5536\n",
      "Epoch 1554/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 156.1999 - val_loss: 233.8563\n",
      "Epoch 1555/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 152.2411 - val_loss: 232.9397\n",
      "Epoch 1556/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 155.2381 - val_loss: 234.1413\n",
      "Epoch 1557/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 156.9455 - val_loss: 229.9757\n",
      "Epoch 1558/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 157.6040 - val_loss: 234.1803\n",
      "Epoch 1559/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 155.4715 - val_loss: 232.4206\n",
      "Epoch 1560/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 154.9818 - val_loss: 232.1711\n",
      "Epoch 1561/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 155.3811 - val_loss: 230.4691\n",
      "Epoch 1562/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 153.9512 - val_loss: 229.6875\n",
      "Epoch 1563/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 160.6109 - val_loss: 230.7516\n",
      "Epoch 1564/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 156.1250 - val_loss: 232.7389\n",
      "Epoch 1565/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 154.4350 - val_loss: 230.7751\n",
      "Epoch 1566/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 157.1103 - val_loss: 233.0500\n",
      "Epoch 1567/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 155.5106 - val_loss: 231.9543\n",
      "Epoch 1568/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 157.0342 - val_loss: 233.9406\n",
      "Epoch 1569/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.8620 - val_loss: 230.7774\n",
      "Epoch 1570/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 153.1309 - val_loss: 228.3252\n",
      "Epoch 1571/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.5055 - val_loss: 229.9094\n",
      "Epoch 1572/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 159.0577 - val_loss: 228.4858\n",
      "Epoch 1573/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 156.3179 - val_loss: 226.0309\n",
      "Epoch 1574/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 155.0661 - val_loss: 227.4692\n",
      "Epoch 1575/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.2808 - val_loss: 229.9175\n",
      "Epoch 1576/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.7312 - val_loss: 231.7924\n",
      "Epoch 1577/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 155.3385 - val_loss: 230.5177\n",
      "Epoch 1578/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.9908 - val_loss: 231.1635\n",
      "Epoch 1579/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 158.7541 - val_loss: 233.4771\n",
      "Epoch 1580/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.9008 - val_loss: 227.3787\n",
      "Epoch 1581/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 155.2383 - val_loss: 227.0828\n",
      "Epoch 1582/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 154.8616 - val_loss: 232.4843\n",
      "Epoch 1583/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 156.0913 - val_loss: 234.4339\n",
      "Epoch 1584/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 156.4863 - val_loss: 232.0145\n",
      "Epoch 1585/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 157.5859 - val_loss: 234.5699\n",
      "Epoch 1586/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 157.0833 - val_loss: 230.5112\n",
      "Epoch 1587/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 156.7110 - val_loss: 232.0690\n",
      "Epoch 1588/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 155.1348 - val_loss: 232.0194\n",
      "Epoch 1589/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 153.4203 - val_loss: 233.1304\n",
      "Epoch 1590/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.6586 - val_loss: 232.6505\n",
      "Epoch 1591/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 155.4119 - val_loss: 234.7425\n",
      "Epoch 1592/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 155.6352 - val_loss: 228.9609\n",
      "Epoch 1593/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.4264 - val_loss: 229.0764\n",
      "Epoch 1594/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 158.0772 - val_loss: 229.6893\n",
      "Epoch 1595/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.7030 - val_loss: 227.2428\n",
      "Epoch 1596/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 154.1396 - val_loss: 225.0060\n",
      "Epoch 1597/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 158.2102 - val_loss: 233.3585\n",
      "Epoch 1598/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.4480 - val_loss: 228.8169\n",
      "Epoch 1599/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 153.7515 - val_loss: 230.8075\n",
      "Epoch 1600/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 156.6568 - val_loss: 231.8305\n",
      "Epoch 1601/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 155.1066 - val_loss: 230.9247\n",
      "Epoch 1602/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 148.4654 - val_loss: 229.0718\n",
      "Epoch 1603/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 156.6734 - val_loss: 227.7346\n",
      "Epoch 1604/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 159.8264 - val_loss: 225.0864\n",
      "Epoch 1605/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 156.9455 - val_loss: 226.9197\n",
      "Epoch 1606/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.1365 - val_loss: 227.8246\n",
      "Epoch 1607/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 155.5412 - val_loss: 232.7498\n",
      "Epoch 1608/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.2300 - val_loss: 232.0101\n",
      "Epoch 1609/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 158.9191 - val_loss: 230.5201\n",
      "Epoch 1610/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 153.0631 - val_loss: 232.1366\n",
      "Epoch 1611/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 158.6305 - val_loss: 229.9861\n",
      "Epoch 1612/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 154.7402 - val_loss: 227.9192\n",
      "Epoch 1613/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 157.1693 - val_loss: 230.2045\n",
      "Epoch 1614/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 155.7343 - val_loss: 228.8333\n",
      "Epoch 1615/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 154.8192 - val_loss: 230.4078\n",
      "Epoch 1616/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 157.6774 - val_loss: 232.4619\n",
      "Epoch 1617/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 155.7225 - val_loss: 231.5216\n",
      "Epoch 1618/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 155.4131 - val_loss: 235.4897\n",
      "Epoch 1619/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 158.2290 - val_loss: 234.7147\n",
      "Epoch 1620/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 155.1206 - val_loss: 235.9301\n",
      "Epoch 1621/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 154.5841 - val_loss: 225.6821\n",
      "Epoch 1622/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 156.2013 - val_loss: 229.1939\n",
      "Epoch 1623/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 157.4751 - val_loss: 232.7488\n",
      "Epoch 1624/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 157.6910 - val_loss: 230.6721\n",
      "Epoch 1625/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 157.5461 - val_loss: 234.8183\n",
      "Epoch 1626/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.1634 - val_loss: 229.5764\n",
      "Epoch 1627/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 156.7854 - val_loss: 227.5531\n",
      "Epoch 1628/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 155.9236 - val_loss: 231.5417\n",
      "Epoch 1629/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 154.3414 - val_loss: 232.3362\n",
      "Epoch 1630/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 158.7323 - val_loss: 232.7728\n",
      "Epoch 1631/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 152.4030 - val_loss: 229.9442\n",
      "Epoch 1632/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 158.6051 - val_loss: 229.5924\n",
      "Epoch 1633/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 154.2686 - val_loss: 229.0120\n",
      "Epoch 1634/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 154.9008 - val_loss: 226.7954\n",
      "Epoch 1635/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 155.4901 - val_loss: 231.2407\n",
      "Epoch 1636/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 155.2456 - val_loss: 230.2103\n",
      "Epoch 1637/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 152.1786 - val_loss: 232.6694\n",
      "Epoch 1638/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 151.2947 - val_loss: 232.1065\n",
      "Epoch 1639/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 152.8049 - val_loss: 227.6231\n",
      "Epoch 1640/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 155.0786 - val_loss: 230.9855\n",
      "Epoch 1641/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 154.7067 - val_loss: 230.4030\n",
      "Epoch 1642/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 151.9178 - val_loss: 234.7147\n",
      "Epoch 1643/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.1496 - val_loss: 231.8914\n",
      "Epoch 1644/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 156.5983 - val_loss: 232.2161\n",
      "Epoch 1645/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 154.7338 - val_loss: 232.9565\n",
      "Epoch 1646/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 158.3973 - val_loss: 232.1985\n",
      "Epoch 1647/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 158.3785 - val_loss: 230.9081\n",
      "Epoch 1648/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 157.5818 - val_loss: 228.7926\n",
      "Epoch 1649/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 155.9240 - val_loss: 227.1397\n",
      "Epoch 1650/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 155.2000 - val_loss: 229.1741\n",
      "Epoch 1651/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.4825 - val_loss: 233.9231\n",
      "Epoch 1652/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 157.1321 - val_loss: 234.7357\n",
      "Epoch 1653/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.6439 - val_loss: 229.7543\n",
      "Epoch 1654/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 155.4602 - val_loss: 233.1979\n",
      "Epoch 1655/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 153.3880 - val_loss: 234.7512\n",
      "Epoch 1656/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 157.2763 - val_loss: 233.4364\n",
      "Epoch 1657/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.3962 - val_loss: 236.0084\n",
      "Epoch 1658/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 159.6594 - val_loss: 233.8003\n",
      "Epoch 1659/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 157.3839 - val_loss: 233.4541\n",
      "Epoch 1660/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 156.0432 - val_loss: 234.0182\n",
      "Epoch 1661/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 156.8698 - val_loss: 234.0913\n",
      "Epoch 1662/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 153.5041 - val_loss: 236.6698\n",
      "Epoch 1663/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 158.7341 - val_loss: 231.7538\n",
      "Epoch 1664/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 154.6579 - val_loss: 233.5615\n",
      "Epoch 1665/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 157.8873 - val_loss: 233.2350\n",
      "Epoch 1666/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 156.6669 - val_loss: 235.4914\n",
      "Epoch 1667/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.1149 - val_loss: 232.8583\n",
      "Epoch 1668/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 155.5662 - val_loss: 235.6859\n",
      "Epoch 1669/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 156.0634 - val_loss: 232.9901\n",
      "Epoch 1670/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 156.0808 - val_loss: 230.0994\n",
      "Epoch 1671/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 156.2740 - val_loss: 235.0019\n",
      "Epoch 1672/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 158.7360 - val_loss: 232.3974\n",
      "Epoch 1673/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 155.4417 - val_loss: 232.0058\n",
      "Epoch 1674/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 155.1738 - val_loss: 231.3774\n",
      "Epoch 1675/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 156.8009 - val_loss: 231.2288\n",
      "Epoch 1676/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 152.5192 - val_loss: 231.8571\n",
      "Epoch 1677/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 159.3314 - val_loss: 230.8951\n",
      "Epoch 1678/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 154.8903 - val_loss: 235.1542\n",
      "Epoch 1679/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 156.8192 - val_loss: 230.6788\n",
      "Epoch 1680/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.7761 - val_loss: 228.7742\n",
      "Epoch 1681/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 155.5422 - val_loss: 229.6245\n",
      "Epoch 1682/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 153.6288 - val_loss: 231.8433\n",
      "Epoch 1683/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 149.0417 - val_loss: 227.8360\n",
      "Epoch 1684/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 155.8755 - val_loss: 227.6090\n",
      "Epoch 1685/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 157.4528 - val_loss: 231.2245\n",
      "Epoch 1686/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 152.4688 - val_loss: 229.1500\n",
      "Epoch 1687/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 152.6015 - val_loss: 233.6154\n",
      "Epoch 1688/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 154.2728 - val_loss: 231.1605\n",
      "Epoch 1689/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 156.3431 - val_loss: 231.4855\n",
      "Epoch 1690/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 151.3024 - val_loss: 233.4431\n",
      "Epoch 1691/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 151.3663 - val_loss: 230.1832\n",
      "Epoch 1692/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 152.7565 - val_loss: 232.4696\n",
      "Epoch 1693/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 155.8536 - val_loss: 229.8294\n",
      "Epoch 1694/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.0892 - val_loss: 230.9357\n",
      "Epoch 1695/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 151.4659 - val_loss: 229.5011\n",
      "Epoch 1696/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 154.8876 - val_loss: 231.8691\n",
      "Epoch 1697/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.5692 - val_loss: 235.2999\n",
      "Epoch 1698/2000\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 152.8807 - val_loss: 235.4235\n",
      "Epoch 1699/2000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 155.3596 - val_loss: 233.6971\n",
      "Epoch 1700/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 154.4281 - val_loss: 232.8145\n",
      "Epoch 1701/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.6876 - val_loss: 232.2399\n",
      "Epoch 1702/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 156.0327 - val_loss: 229.8055\n",
      "Epoch 1703/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.4896 - val_loss: 230.8861\n",
      "Epoch 1704/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 151.5115 - val_loss: 230.2968\n",
      "Epoch 1705/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 154.6016 - val_loss: 231.9725\n",
      "Epoch 1706/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 154.7152 - val_loss: 236.9891\n",
      "Epoch 1707/2000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 156.2122 - val_loss: 234.2908\n",
      "Epoch 1708/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.8282 - val_loss: 234.6361\n",
      "Epoch 1709/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 151.3806 - val_loss: 231.0224\n",
      "Epoch 1710/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.3149 - val_loss: 230.0481\n",
      "Epoch 1711/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 153.6346 - val_loss: 229.7988\n",
      "Epoch 1712/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 149.8586 - val_loss: 234.8198\n",
      "Epoch 1713/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 155.2095 - val_loss: 235.7592\n",
      "Epoch 1714/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 152.5945 - val_loss: 233.4379\n",
      "Epoch 1715/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 149.1431 - val_loss: 236.4148\n",
      "Epoch 1716/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 153.1868 - val_loss: 233.9842\n",
      "Epoch 1717/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 153.6126 - val_loss: 234.3179\n",
      "Epoch 1718/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 152.8649 - val_loss: 234.7026\n",
      "Epoch 1719/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.6275 - val_loss: 233.1503\n",
      "Epoch 1720/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 148.6019 - val_loss: 234.3597\n",
      "Epoch 1721/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 152.1623 - val_loss: 232.8890\n",
      "Epoch 1722/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.2589 - val_loss: 231.1644\n",
      "Epoch 1723/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 150.9478 - val_loss: 230.1623\n",
      "Epoch 1724/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.1200 - val_loss: 232.2076\n",
      "Epoch 1725/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.0079 - val_loss: 233.2568\n",
      "Epoch 1726/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 156.5821 - val_loss: 234.4438\n",
      "Epoch 1727/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 152.0704 - val_loss: 229.3648\n",
      "Epoch 1728/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 153.7868 - val_loss: 234.0936\n",
      "Epoch 1729/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 154.6066 - val_loss: 233.1179\n",
      "Epoch 1730/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 154.6054 - val_loss: 234.2152\n",
      "Epoch 1731/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 150.0966 - val_loss: 233.1464\n",
      "Epoch 1732/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 153.1703 - val_loss: 228.6453\n",
      "Epoch 1733/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 153.2744 - val_loss: 233.7912\n",
      "Epoch 1734/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.5780 - val_loss: 228.0581\n",
      "Epoch 1735/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 155.2613 - val_loss: 230.2938\n",
      "Epoch 1736/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 152.6839 - val_loss: 236.2344\n",
      "Epoch 1737/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 153.0101 - val_loss: 231.9391\n",
      "Epoch 1738/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 153.6067 - val_loss: 233.4554\n",
      "Epoch 1739/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 153.0704 - val_loss: 233.4574\n",
      "Epoch 1740/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 150.7307 - val_loss: 231.1339\n",
      "Epoch 1741/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.2911 - val_loss: 236.1147\n",
      "Epoch 1742/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 153.8537 - val_loss: 234.6938\n",
      "Epoch 1743/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.2428 - val_loss: 234.8353\n",
      "Epoch 1744/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 153.1690 - val_loss: 235.1722\n",
      "Epoch 1745/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 153.3394 - val_loss: 232.6751\n",
      "Epoch 1746/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 153.7730 - val_loss: 232.4028\n",
      "Epoch 1747/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 151.3779 - val_loss: 232.6456\n",
      "Epoch 1748/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 155.3995 - val_loss: 233.0258\n",
      "Epoch 1749/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 154.8687 - val_loss: 229.1893\n",
      "Epoch 1750/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 152.2190 - val_loss: 228.2426\n",
      "Epoch 1751/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 153.2883 - val_loss: 227.6073\n",
      "Epoch 1752/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 156.1092 - val_loss: 232.4223\n",
      "Epoch 1753/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.7097 - val_loss: 231.0311\n",
      "Epoch 1754/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 153.8692 - val_loss: 231.8671\n",
      "Epoch 1755/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.3153 - val_loss: 233.8717\n",
      "Epoch 1756/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 154.3240 - val_loss: 230.9773\n",
      "Epoch 1757/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 148.1744 - val_loss: 234.6703\n",
      "Epoch 1758/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 155.1514 - val_loss: 229.0969\n",
      "Epoch 1759/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.9919 - val_loss: 230.5567\n",
      "Epoch 1760/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 149.3894 - val_loss: 234.1094\n",
      "Epoch 1761/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 152.9917 - val_loss: 231.6910\n",
      "Epoch 1762/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 155.0068 - val_loss: 233.1597\n",
      "Epoch 1763/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 157.8780 - val_loss: 231.1485\n",
      "Epoch 1764/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 155.8710 - val_loss: 232.6816\n",
      "Epoch 1765/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 154.0400 - val_loss: 230.7424\n",
      "Epoch 1766/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 151.8881 - val_loss: 233.4773\n",
      "Epoch 1767/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.1622 - val_loss: 235.0362\n",
      "Epoch 1768/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 153.7374 - val_loss: 230.9973\n",
      "Epoch 1769/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 152.0296 - val_loss: 231.3948\n",
      "Epoch 1770/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.5622 - val_loss: 229.5052\n",
      "Epoch 1771/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 154.3701 - val_loss: 231.9311\n",
      "Epoch 1772/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.8654 - val_loss: 235.6215\n",
      "Epoch 1773/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 147.6615 - val_loss: 235.3789\n",
      "Epoch 1774/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 151.5872 - val_loss: 235.3942\n",
      "Epoch 1775/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 155.2887 - val_loss: 233.4283\n",
      "Epoch 1776/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 150.3326 - val_loss: 230.0227\n",
      "Epoch 1777/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 151.1135 - val_loss: 234.2021\n",
      "Epoch 1778/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 153.7480 - val_loss: 232.3745\n",
      "Epoch 1779/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 154.6192 - val_loss: 232.0735\n",
      "Epoch 1780/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.2102 - val_loss: 229.6438\n",
      "Epoch 1781/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 148.5649 - val_loss: 229.5932\n",
      "Epoch 1782/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 154.6923 - val_loss: 234.4437\n",
      "Epoch 1783/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 153.6573 - val_loss: 234.1323\n",
      "Epoch 1784/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 154.1507 - val_loss: 232.7102\n",
      "Epoch 1785/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.8891 - val_loss: 231.0684\n",
      "Epoch 1786/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 154.3464 - val_loss: 232.3434\n",
      "Epoch 1787/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 151.2051 - val_loss: 231.3279\n",
      "Epoch 1788/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.0441 - val_loss: 232.3680\n",
      "Epoch 1789/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 151.7801 - val_loss: 240.8306\n",
      "Epoch 1790/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 153.7765 - val_loss: 238.2634\n",
      "Epoch 1791/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 151.3724 - val_loss: 232.7307\n",
      "Epoch 1792/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.6616 - val_loss: 234.4832\n",
      "Epoch 1793/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.7262 - val_loss: 233.8187\n",
      "Epoch 1794/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 156.1089 - val_loss: 228.0386\n",
      "Epoch 1795/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 153.8526 - val_loss: 230.0652\n",
      "Epoch 1796/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 153.1527 - val_loss: 233.1875\n",
      "Epoch 1797/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 151.6425 - val_loss: 230.8082\n",
      "Epoch 1798/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 153.2631 - val_loss: 228.5954\n",
      "Epoch 1799/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.6938 - val_loss: 228.7609\n",
      "Epoch 1800/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 154.1435 - val_loss: 231.9506\n",
      "Epoch 1801/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 155.4109 - val_loss: 229.8849\n",
      "Epoch 1802/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.2790 - val_loss: 231.4642\n",
      "Epoch 1803/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.2656 - val_loss: 233.7319\n",
      "Epoch 1804/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 149.1733 - val_loss: 233.0431\n",
      "Epoch 1805/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 152.2790 - val_loss: 231.8095\n",
      "Epoch 1806/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 153.6720 - val_loss: 230.6304\n",
      "Epoch 1807/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 155.8519 - val_loss: 233.0258\n",
      "Epoch 1808/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 158.6769 - val_loss: 231.1854\n",
      "Epoch 1809/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.7182 - val_loss: 231.1813\n",
      "Epoch 1810/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.1181 - val_loss: 236.8325\n",
      "Epoch 1811/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.2197 - val_loss: 235.3713\n",
      "Epoch 1812/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 151.2061 - val_loss: 232.6606\n",
      "Epoch 1813/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.0609 - val_loss: 234.6082\n",
      "Epoch 1814/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 150.5855 - val_loss: 233.5771\n",
      "Epoch 1815/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.5488 - val_loss: 235.4761\n",
      "Epoch 1816/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.7567 - val_loss: 233.2027\n",
      "Epoch 1817/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.0801 - val_loss: 230.1841\n",
      "Epoch 1818/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.4153 - val_loss: 234.6283\n",
      "Epoch 1819/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.0459 - val_loss: 231.0761\n",
      "Epoch 1820/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.0101 - val_loss: 228.3405\n",
      "Epoch 1821/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 152.5793 - val_loss: 237.1891\n",
      "Epoch 1822/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 151.2597 - val_loss: 228.7788\n",
      "Epoch 1823/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 153.3531 - val_loss: 232.1189\n",
      "Epoch 1824/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.9371 - val_loss: 235.8215\n",
      "Epoch 1825/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 151.5830 - val_loss: 234.3360\n",
      "Epoch 1826/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 151.2773 - val_loss: 232.2682\n",
      "Epoch 1827/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 147.8933 - val_loss: 229.1019\n",
      "Epoch 1828/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.0661 - val_loss: 229.9322\n",
      "Epoch 1829/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.3294 - val_loss: 231.9929\n",
      "Epoch 1830/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 147.7569 - val_loss: 229.9825\n",
      "Epoch 1831/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 155.6253 - val_loss: 234.7056\n",
      "Epoch 1832/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.0145 - val_loss: 233.8791\n",
      "Epoch 1833/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 148.8710 - val_loss: 233.1223\n",
      "Epoch 1834/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 148.5630 - val_loss: 234.1002\n",
      "Epoch 1835/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 155.8116 - val_loss: 231.5790\n",
      "Epoch 1836/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 150.5555 - val_loss: 231.8221\n",
      "Epoch 1837/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 150.4272 - val_loss: 233.5343\n",
      "Epoch 1838/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.8251 - val_loss: 231.8170\n",
      "Epoch 1839/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 151.7111 - val_loss: 230.8472\n",
      "Epoch 1840/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 147.9882 - val_loss: 231.7512\n",
      "Epoch 1841/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 153.7137 - val_loss: 233.5501\n",
      "Epoch 1842/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.9258 - val_loss: 233.1953\n",
      "Epoch 1843/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.8092 - val_loss: 232.0343\n",
      "Epoch 1844/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 158.3397 - val_loss: 233.0981\n",
      "Epoch 1845/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 150.6145 - val_loss: 239.3192\n",
      "Epoch 1846/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 154.2170 - val_loss: 234.1732\n",
      "Epoch 1847/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 150.9256 - val_loss: 234.5964\n",
      "Epoch 1848/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.6458 - val_loss: 234.2655\n",
      "Epoch 1849/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 151.5451 - val_loss: 228.6335\n",
      "Epoch 1850/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 152.1146 - val_loss: 227.8885\n",
      "Epoch 1851/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.6553 - val_loss: 228.5940\n",
      "Epoch 1852/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.5513 - val_loss: 234.8911\n",
      "Epoch 1853/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 152.2371 - val_loss: 235.5392\n",
      "Epoch 1854/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.5970 - val_loss: 233.2624\n",
      "Epoch 1855/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 154.4666 - val_loss: 233.0433\n",
      "Epoch 1856/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 149.0118 - val_loss: 232.0607\n",
      "Epoch 1857/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 149.3708 - val_loss: 236.0750\n",
      "Epoch 1858/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 152.4429 - val_loss: 233.5364\n",
      "Epoch 1859/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 150.2767 - val_loss: 232.9349\n",
      "Epoch 1860/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 151.7801 - val_loss: 233.6748\n",
      "Epoch 1861/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 152.9074 - val_loss: 234.2999\n",
      "Epoch 1862/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 152.6057 - val_loss: 226.4325\n",
      "Epoch 1863/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 152.0692 - val_loss: 230.9966\n",
      "Epoch 1864/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.3584 - val_loss: 234.2200\n",
      "Epoch 1865/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 146.1987 - val_loss: 235.1191\n",
      "Epoch 1866/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 153.4789 - val_loss: 236.1701\n",
      "Epoch 1867/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.1629 - val_loss: 241.8576\n",
      "Epoch 1868/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 146.7554 - val_loss: 237.4297\n",
      "Epoch 1869/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 150.5226 - val_loss: 233.4861\n",
      "Epoch 1870/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 147.0022 - val_loss: 232.8666\n",
      "Epoch 1871/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.2966 - val_loss: 234.2416\n",
      "Epoch 1872/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.9040 - val_loss: 230.7394\n",
      "Epoch 1873/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.2384 - val_loss: 234.6915\n",
      "Epoch 1874/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 151.5624 - val_loss: 233.5375\n",
      "Epoch 1875/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 146.9050 - val_loss: 230.1944\n",
      "Epoch 1876/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 154.1120 - val_loss: 231.5745\n",
      "Epoch 1877/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.8006 - val_loss: 232.2509\n",
      "Epoch 1878/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 152.2497 - val_loss: 234.9310\n",
      "Epoch 1879/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 151.7796 - val_loss: 232.4893\n",
      "Epoch 1880/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 149.3713 - val_loss: 235.1189\n",
      "Epoch 1881/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 156.7171 - val_loss: 239.5997\n",
      "Epoch 1882/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 153.9046 - val_loss: 231.1949\n",
      "Epoch 1883/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 150.4294 - val_loss: 230.6247\n",
      "Epoch 1884/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 151.2635 - val_loss: 236.4848\n",
      "Epoch 1885/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 151.5094 - val_loss: 233.1893\n",
      "Epoch 1886/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 152.5843 - val_loss: 231.4541\n",
      "Epoch 1887/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 148.5989 - val_loss: 233.6651\n",
      "Epoch 1888/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 151.8788 - val_loss: 233.3519\n",
      "Epoch 1889/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 153.2351 - val_loss: 237.3500\n",
      "Epoch 1890/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 153.3346 - val_loss: 236.4615\n",
      "Epoch 1891/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 153.5791 - val_loss: 230.1919\n",
      "Epoch 1892/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.7491 - val_loss: 226.1568\n",
      "Epoch 1893/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 152.1171 - val_loss: 229.7733\n",
      "Epoch 1894/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.4353 - val_loss: 233.3806\n",
      "Epoch 1895/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 152.8429 - val_loss: 232.8434\n",
      "Epoch 1896/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 150.7027 - val_loss: 233.7086\n",
      "Epoch 1897/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 151.8595 - val_loss: 233.5640\n",
      "Epoch 1898/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 153.6828 - val_loss: 232.0243\n",
      "Epoch 1899/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.3509 - val_loss: 231.6019\n",
      "Epoch 1900/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 144.5588 - val_loss: 238.2134\n",
      "Epoch 1901/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 145.8833 - val_loss: 235.2025\n",
      "Epoch 1902/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 156.4765 - val_loss: 230.2254\n",
      "Epoch 1903/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 153.0992 - val_loss: 233.6514\n",
      "Epoch 1904/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.3770 - val_loss: 232.9136\n",
      "Epoch 1905/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.6105 - val_loss: 236.8314\n",
      "Epoch 1906/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 150.0418 - val_loss: 231.6585\n",
      "Epoch 1907/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 147.3618 - val_loss: 234.7158\n",
      "Epoch 1908/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 151.5246 - val_loss: 234.2658\n",
      "Epoch 1909/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.4540 - val_loss: 235.5338\n",
      "Epoch 1910/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 152.8532 - val_loss: 230.5441\n",
      "Epoch 1911/2000\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 153.6232 - val_loss: 234.9293\n",
      "Epoch 1912/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 148.3525 - val_loss: 233.1110\n",
      "Epoch 1913/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 153.4668 - val_loss: 237.0418\n",
      "Epoch 1914/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 151.9322 - val_loss: 235.0227\n",
      "Epoch 1915/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 147.8410 - val_loss: 233.6361\n",
      "Epoch 1916/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 149.0238 - val_loss: 230.9543\n",
      "Epoch 1917/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 154.1364 - val_loss: 237.8961\n",
      "Epoch 1918/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 152.7864 - val_loss: 228.7984\n",
      "Epoch 1919/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 152.0071 - val_loss: 237.9516\n",
      "Epoch 1920/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 153.7426 - val_loss: 227.8584\n",
      "Epoch 1921/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 152.3509 - val_loss: 233.3079\n",
      "Epoch 1922/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 150.9194 - val_loss: 229.4199\n",
      "Epoch 1923/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 146.8209 - val_loss: 231.7132\n",
      "Epoch 1924/2000\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 151.9809 - val_loss: 235.9153\n",
      "Epoch 1925/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 148.1658 - val_loss: 232.3639\n",
      "Epoch 1926/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 152.0872 - val_loss: 231.6167\n",
      "Epoch 1927/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 147.3967 - val_loss: 231.3460\n",
      "Epoch 1928/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 148.7298 - val_loss: 234.5132\n",
      "Epoch 1929/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 149.6703 - val_loss: 232.7802\n",
      "Epoch 1930/2000\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 151.1119 - val_loss: 234.9938\n",
      "Epoch 1931/2000\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 153.5081 - val_loss: 227.3456\n",
      "Epoch 1932/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 148.6193 - val_loss: 235.6927\n",
      "Epoch 1933/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 152.0074 - val_loss: 230.2105\n",
      "Epoch 1934/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 148.0659 - val_loss: 230.6553\n",
      "Epoch 1935/2000\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 151.7409 - val_loss: 233.6208\n",
      "Epoch 1936/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 149.5251 - val_loss: 228.3551\n",
      "Epoch 1937/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 153.3925 - val_loss: 224.3779\n",
      "Epoch 1938/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 149.1681 - val_loss: 227.9950\n",
      "Epoch 1939/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 150.9644 - val_loss: 230.4316\n",
      "Epoch 1940/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.6239 - val_loss: 228.1273\n",
      "Epoch 1941/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 147.5480 - val_loss: 238.0093\n",
      "Epoch 1942/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 147.7182 - val_loss: 231.1971\n",
      "Epoch 1943/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 147.9337 - val_loss: 238.7348\n",
      "Epoch 1944/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 151.3963 - val_loss: 235.7519\n",
      "Epoch 1945/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 146.0750 - val_loss: 233.4733\n",
      "Epoch 1946/2000\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 150.8055 - val_loss: 231.5240\n",
      "Epoch 1947/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 150.1581 - val_loss: 237.1610\n",
      "Epoch 1948/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 154.3143 - val_loss: 233.2240\n",
      "Epoch 1949/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 148.7046 - val_loss: 231.4110\n",
      "Epoch 1950/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 149.9932 - val_loss: 234.4696\n",
      "Epoch 1951/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 144.6350 - val_loss: 232.9884\n",
      "Epoch 1952/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 146.1865 - val_loss: 235.1903\n",
      "Epoch 1953/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 148.2710 - val_loss: 232.4613\n",
      "Epoch 1954/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.0918 - val_loss: 233.4614\n",
      "Epoch 1955/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 148.6682 - val_loss: 234.4745\n",
      "Epoch 1956/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 147.7471 - val_loss: 236.4860\n",
      "Epoch 1957/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 148.9528 - val_loss: 234.7312\n",
      "Epoch 1958/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 151.0742 - val_loss: 240.0775\n",
      "Epoch 1959/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 145.9458 - val_loss: 230.8762\n",
      "Epoch 1960/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 145.4038 - val_loss: 231.8002\n",
      "Epoch 1961/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 152.7483 - val_loss: 236.6556\n",
      "Epoch 1962/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 151.7186 - val_loss: 231.8934\n",
      "Epoch 1963/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 148.4755 - val_loss: 231.5997\n",
      "Epoch 1964/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 152.2896 - val_loss: 237.7958\n",
      "Epoch 1965/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 147.2761 - val_loss: 239.8512\n",
      "Epoch 1966/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 148.0339 - val_loss: 233.0170\n",
      "Epoch 1967/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 148.1739 - val_loss: 230.5676\n",
      "Epoch 1968/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 152.1063 - val_loss: 229.1261\n",
      "Epoch 1969/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 152.4319 - val_loss: 234.2325\n",
      "Epoch 1970/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.7228 - val_loss: 232.5407\n",
      "Epoch 1971/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 149.2300 - val_loss: 233.9497\n",
      "Epoch 1972/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 151.7209 - val_loss: 230.4113\n",
      "Epoch 1973/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 150.7793 - val_loss: 244.0202\n",
      "Epoch 1974/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 148.7940 - val_loss: 228.6051\n",
      "Epoch 1975/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 148.5812 - val_loss: 228.9474\n",
      "Epoch 1976/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 154.1007 - val_loss: 227.6129\n",
      "Epoch 1977/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 145.1950 - val_loss: 242.7954\n",
      "Epoch 1978/2000\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 147.8792 - val_loss: 231.2643\n",
      "Epoch 1979/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 148.2087 - val_loss: 234.2484\n",
      "Epoch 1980/2000\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 147.7510 - val_loss: 234.6987\n",
      "Epoch 1981/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 147.1826 - val_loss: 240.3808\n",
      "Epoch 1982/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 147.6449 - val_loss: 233.8765\n",
      "Epoch 1983/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 146.2946 - val_loss: 234.2848\n",
      "Epoch 1984/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 153.8062 - val_loss: 233.1925\n",
      "Epoch 1985/2000\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 148.7090 - val_loss: 229.9358\n",
      "Epoch 1986/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 147.5692 - val_loss: 236.1163\n",
      "Epoch 1987/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 144.4588 - val_loss: 237.9358\n",
      "Epoch 1988/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 148.4935 - val_loss: 232.0908\n",
      "Epoch 1989/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 151.1298 - val_loss: 232.9299\n",
      "Epoch 1990/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 151.4200 - val_loss: 227.8541\n",
      "Epoch 1991/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 152.5396 - val_loss: 233.9643\n",
      "Epoch 1992/2000\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 149.4171 - val_loss: 226.3578\n",
      "Epoch 1993/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 148.9644 - val_loss: 234.7169\n",
      "Epoch 1994/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 146.1295 - val_loss: 229.9248\n",
      "Epoch 1995/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 153.5991 - val_loss: 236.1024\n",
      "Epoch 1996/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 147.5501 - val_loss: 229.7327\n",
      "Epoch 1997/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 152.5257 - val_loss: 232.3262\n",
      "Epoch 1998/2000\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 148.7958 - val_loss: 235.4084\n",
      "Epoch 1999/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 147.6101 - val_loss: 228.9569\n",
      "Epoch 2000/2000\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 148.5013 - val_loss: 232.8051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220.33407592773438"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelling\n",
    "# Create initial model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add just one layer for now\n",
    "model.add(Masking(mask_value=placeholder_value))\n",
    "model.add(Masking(mask_value=placeholder_value1))\n",
    "model.add(layers.LSTM(256, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(layers.LSTM(128, activation='tanh', return_sequences=False, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(layers.Dense(100, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(layers.Dense(80, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(layers.Dense(120, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(len(i_s) - 3, activation='linear')) # output layer has ~160 linear nodes\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.00001), \n",
    "              loss=keras.losses.MeanAbsoluteError())\n",
    "\n",
    "# Define an Early Stop callback\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience = 5, \n",
    "                                           verbose=1, restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    validation_split=0.3,\n",
    "                    batch_size=64, epochs=2000, verbose=1)\n",
    "\n",
    "model.evaluate(X_test, Y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -30\n",
    "player = players[i]\n",
    "print(player)\n",
    "hist, act = X_test[i], Y_test[i]\n",
    "hist = hist.reshape(1, 6, 163)\n",
    "pred = model.predict(hist)\n",
    "print(hist.shape)\n",
    "for feat_i in range(X_train.shape[2]):\n",
    "    print(f'  feature: {df.columns[i_s[feat_i + 3]]}')\n",
    "    print(f'    {hist[0, :, feat_i+3]}, -> {np.round(pred, 2)[0, feat_i]} vs {np.round(act[feat_i], 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(X_train)\n",
    "\n",
    "def sim(origin:list, other:list) -> float:\n",
    "    \"\"\"\n",
    "    A function that takes in the stats of two players and returns how similar other is to origin\n",
    "    origin and other should have length 163\n",
    "    \"\"\"\n",
    "    O1 = ss.transform(origin)\n",
    "    O2 = ss.transform(other)\n",
    "\n",
    "    return cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_player(p_id: str, szn: str) -> str:\n",
    "    \"\"\"\n",
    "    a function That, given a players id, will return the player whose next season more resembles that player\n",
    "    pid: the player we most want to replicate\n",
    "    sn: the season of that player we most want to replicate. \n",
    "    \"\"\"\n",
    "    szn_hist = df.loc[df['p_id'] == p_id & df['Season'] == szn]\n",
    "\n",
    "    total.sort_values(['p_id', 'Season'])\n",
    "    pids = total['p_id'].unique()  # Get a list of all the unique player ids\n",
    "\n",
    "    coss = []\n",
    "    mses = []\n",
    "\n",
    "    for player in pids:\n",
    "        p_df = total.loc[total['p_id'] == player]  # Get the historical data for this player. \n",
    "        # We need to pad the data\n",
    "        player_data = full_careers[pid] \n",
    "        placeholder_value1 = -500\n",
    "        for season in seasons:\n",
    "            if season not in list(player_data['Season']):\n",
    "                # Add the season row with blanks\n",
    "                # print(f'{pid} needs {season}')\n",
    "                player_name = player_data['Player'].mode()[0]\n",
    "                nation = player_data['Nation'].mode()[0]\n",
    "                born = player_data['Born'].mode()[0]\n",
    "                position = player_data['Pos'].mode()[0]\n",
    "                squad = player_data['Squad'].mode()[0]\n",
    "                comp = player_data['Comp'].mode()[0]\n",
    "\n",
    "                age = int(season.split('-')[0]) - born              \n",
    "\n",
    "                new_row = [pid, season, player_name, nation, position, squad, comp, age, born] + [placeholder_value1] * 160\n",
    "                new_row_df = pd.DataFrame([new_row], columns=df.columns)\n",
    "                player_data = pd.concat([player_data, new_row_df], ignore_index=True)\n",
    "        # now we take the 6 most recent seasons as our model expects 6 seasons\n",
    "        placeholder_value = -999.0\n",
    "        player_data = player_data.fillna(placeholder_value)\n",
    "\n",
    "        data = player_data.to_numpy().reshape(1, 7, 169)\n",
    "        p_hist = player_data[:, -6:, ]\n",
    "\n",
    "        p_pred = model.predict(p_hist)\n",
    "        print(f'prediction shape: {p_pred.shape}, player shape: {szn_hist.shape}')\n",
    "        coss = cosine_similarity(p_pred, szn_hist)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bebo_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
